name: Performance Regression Check

"on":
  pull_request:
    paths:
      - '**/*.py'
      - 'tests/benchmarks/**'
      - '.github/workflows/performance.yml'
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday at 2 AM UTC

jobs:
  # Check if benchmarks exist
  benchmark-check:
    runs-on: ubuntu-latest
    outputs:
      has_benchmarks: ${{ steps.check-benchmarks.outputs.has_benchmarks }}
      has_baselines: ${{ steps.check-benchmarks.outputs.has_baselines }}
    steps:
    - uses: actions/checkout@v4
    
    - name: Check for benchmark tests and baselines
      id: check-benchmarks
      run: |
        if [ -d "tests/benchmarks" ] && find tests/benchmarks -name "test_*.py" -type f | head -1 >/dev/null 2>&1; then
          echo "has_benchmarks=true" >> $GITHUB_OUTPUT
          echo "Benchmark tests found"
        else
          echo "has_benchmarks=false" >> $GITHUB_OUTPUT
          echo "No benchmark tests found"
        fi
        
        if [ -f "tests/benchmarks/baselines.json" ]; then
          echo "has_baselines=true" >> $GITHUB_OUTPUT
          echo "Baselines file found"
        else
          echo "has_baselines=false" >> $GITHUB_OUTPUT
          echo "No baselines file found"
        fi

  # Run performance benchmarks
  benchmark:
    runs-on: ubuntu-latest
    needs: benchmark-check
    if: needs.benchmark-check.outputs.has_benchmarks == 'true'
    timeout-minutes: 20
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-benchmark-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        
        # Install project dependencies
        if [ -f "requirements.txt" ]; then
          pip install -r requirements.txt || echo "Some requirements failed"
        fi
        
        # Install benchmark dependencies
        pip install pytest pytest-benchmark pytest-timeout

    - name: Run benchmark tests
      run: |
        echo "=== Running Performance Benchmarks ==="
        
        # Run fast benchmarks only (exclude slow ones in CI)
        pytest tests/benchmarks/ \
          -v \
          -m "benchmark and not slow_benchmark" \
          --tb=short \
          --timeout=60 \
          --maxfail=5 \
          || echo "Some benchmarks failed but continuing"

    - name: Check for performance regressions
      if: needs.benchmark-check.outputs.has_baselines == 'true'
      run: |
        echo "=== Checking for Performance Regressions ==="
        
        # Generate performance report
        if [ -f "monitoring/performance_dashboard.py" ]; then
          python monitoring/performance_dashboard.py --output performance_report.md
          
          # Check if report indicates regressions
          if grep -q "üî¥" performance_report.md; then
            echo "‚ö†Ô∏è Performance regressions detected!"
            echo "Check the performance report for details."
            
            # Extract regression info
            echo "## Performance Regression Summary" >> $GITHUB_STEP_SUMMARY
            grep -A 10 "Performance Regressions" performance_report.md >> $GITHUB_STEP_SUMMARY || true
          else
            echo "‚úÖ No performance regressions detected"
          fi
        else
          echo "Performance dashboard not available"
        fi

    - name: Generate baseline data (main branch only)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "=== Updating Performance Baselines ==="
        
        if [ -f "scripts/collect_baselines.py" ]; then
          # Run baseline collection with fewer iterations for CI
          python scripts/collect_baselines.py --iterations 3 --compare
        else
          echo "Baseline collection script not found"
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: |
          tests/benchmarks/benchmark_results.json
          tests/benchmarks/baselines.json
          performance_report.md
        retention-days: 30

    - name: Comment PR with performance results
      uses: actions/github-script@v6
      if: github.event_name == 'pull_request' && hashFiles('performance_report.md') != ''
      with:
        script: |
          const fs = require('fs');
          
          let comment = "## üöÄ Performance Benchmark Results\n\n";
          
          try {
            const report = fs.readFileSync('performance_report.md', 'utf8');
            
            // Extract key sections
            const regressionMatch = report.match(/## üö® Performance Regressions([\s\S]*?)##/);
            const overviewMatch = report.match(/## üìä Overview([\s\S]*?)##/);
            
            if (overviewMatch) {
              comment += "### Overview\n" + overviewMatch[1].trim() + "\n\n";
            }
            
            if (regressionMatch && regressionMatch[1].includes('üî¥')) {
              comment += "### ‚ö†Ô∏è Regressions Detected\n" + regressionMatch[1].trim() + "\n\n";
              comment += "**Action Required**: Performance regressions detected. Please investigate.\n\n";
            } else {
              comment += "### ‚úÖ Performance Status\nNo significant regressions detected.\n\n";
            }
            
          } catch (error) {
            comment += "‚ùå Could not read performance report.\n\n";
          }
          
          comment += "üìä **Full Report**: Check the 'benchmark-results' artifact for detailed analysis.";
          
          // Find existing comment to update
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const botComment = comments.find(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('üöÄ Performance Benchmark Results')
          );
          
          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }

  # Collect baselines (scheduled runs)
  collect-baselines:
    runs-on: ubuntu-latest
    needs: benchmark-check
    if: github.event_name == 'schedule' && needs.benchmark-check.outputs.has_benchmarks == 'true'
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f "requirements.txt" ]; then
          pip install -r requirements.txt || echo "Some requirements failed"
        fi
        pip install pytest pytest-benchmark pytest-timeout

    - name: Collect performance baselines
      run: |
        echo "=== Collecting Performance Baselines ==="
        
        if [ -f "scripts/collect_baselines.py" ]; then
          python scripts/collect_baselines.py --iterations 5 --compare
        else
          echo "Baseline collection script not found"
        fi

    - name: Generate performance dashboard
      run: |
        echo "=== Generating Performance Dashboard ==="
        
        if [ -f "monitoring/performance_dashboard.py" ]; then
          python monitoring/performance_dashboard.py
        fi

    - name: Commit updated baselines
      run: |
        # Configure git
        git config user.name 'github-actions[bot]'
        git config user.email '41898282+github-actions[bot]@users.noreply.github.com'
        
        # Add files if they exist and have changes
        git add tests/benchmarks/baselines.json || true
        git add performance_dashboard.md || true
        
        # Check if there are changes to commit
        if git diff --cached --quiet; then
          echo "No changes to commit"
        else
          git commit -m "perf: update performance baselines [automated]

- Collected new performance baselines
- Updated performance dashboard
- Scheduled baseline collection run

Auto-generated by GitHub Actions" || echo "Commit failed"
          
          git push || echo "Push failed"
        fi

  # Performance summary
  performance-summary:
    runs-on: ubuntu-latest
    needs: [benchmark-check, benchmark]
    if: always()
    
    steps:
    - name: Performance check summary
      run: |
        echo "=== Performance Check Summary ==="
        echo "Has benchmarks: ${{ needs.benchmark-check.outputs.has_benchmarks }}"
        echo "Has baselines: ${{ needs.benchmark-check.outputs.has_baselines }}"
        echo "Benchmark job result: ${{ needs.benchmark.result }}"
        
        if [ "${{ needs.benchmark-check.outputs.has_benchmarks }}" = "false" ]; then
          echo "‚ÑπÔ∏è No benchmark tests found. Add tests in tests/benchmarks/ to enable performance monitoring."
        elif [ "${{ needs.benchmark-check.outputs.has_baselines }}" = "false" ]; then
          echo "‚ÑπÔ∏è No baselines found. Run 'python scripts/collect_baselines.py' to establish baselines."
        elif [ "${{ needs.benchmark.result }}" = "success" ]; then
          echo "‚úÖ Performance benchmarks completed successfully"
        else
          echo "‚ö†Ô∏è Performance benchmarks had issues"
        fi