name: CI

"on":
  pull_request:
    paths-ignore:
      - 'docs/**'
      - '*.md'
  push:
    branches: [ "main" ]
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # Simple health check that always runs
  health-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Repository Health Check
      run: |
        echo "=== Repository Structure ==="
        echo "Python files: $(find . -name '*.py' -not -path './.*' | wc -l)"
        echo "Test files: $(find . -name 'test_*.py' -o -name '*_test.py' | wc -l)"
        echo "Workflow files: $(find .github/workflows -name '*.yml' | wc -l)"
        echo "Requirements files: $(ls requirements*.txt 2>/dev/null | wc -l)"
        echo ""
        echo "=== Dependencies Check ==="
        ls -la requirements*.txt 2>/dev/null || echo "No requirements files"
        ls -la pyproject.toml 2>/dev/null || echo "No pyproject.toml"
        echo ""
        echo "=== Directory Structure ==="
        find . -maxdepth 2 -type d -not -path './.*' | head -20

  # Basic test runner
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Check for wheel cache dependencies
      id: check-wheels
      run: |
        if [ -f "scripts/fetch_wheels.py" ] && [ -f "docs/build_artifacts/wheel-manifest.txt" ]; then
          echo "has_wheel_cache=true" >> $GITHUB_OUTPUT
        else
          echo "has_wheel_cache=false" >> $GITHUB_OUTPUT
        fi

    - name: Cache PyPI wheels (if supported)
      if: steps.check-wheels.outputs.has_wheel_cache == 'true'
      id: cache-pip
      uses: actions/cache@v4
      with:
        path: vendor/wheels
        key: pip-v2-${{ hashFiles('docs/build_artifacts/wheel-manifest.txt') }}

    - name: Populate wheel cache (if supported)
      if: steps.check-wheels.outputs.has_wheel_cache == 'true' && steps.cache-pip.outputs.cache-hit != 'true'
      run: python scripts/fetch_wheels.py

    - name: Install dependencies (wheel cache)
      if: steps.check-wheels.outputs.has_wheel_cache == 'true'
      run: |
        set -o pipefail
        pip install --no-index --find-links vendor/wheels -r requirements-dev.txt 2>&1 | tee /tmp/pip.log
        if grep -q "Found link requiring enable-hashes" /tmp/pip.log; then
          echo "::warning ::Wheel cache changed; attempting fallback installation"
          pip install -r requirements-dev.txt
        fi

    - name: Install dependencies (fallback)
      if: steps.check-wheels.outputs.has_wheel_cache == 'false'
      run: |
        python -m pip install --upgrade pip

        # Try multiple installation strategies with better error handling
        if [ -f "requirements-dev.txt" ]; then
          echo "Installing from requirements-dev.txt"
          pip install -r requirements-dev.txt || echo "requirements-dev.txt installation failed, trying basic deps"
        fi
        
        # Fallback to basic dependencies if requirements-dev.txt fails
        echo "Installing core dependencies"
        pip install pytest pytest-cov pytest-xdist numpy scikit-learn torch transformers || echo "Some core dependencies failed"
        
        # Install specific packages for Agent Forge
        pip install wandb pydantic fastapi aiohttp tiktoken || echo "Some Agent Forge dependencies failed"

    - name: Run tests
      run: |
        echo "=== Available test runners ==="
        which pytest 2>/dev/null && echo "pytest: available" || echo "pytest: not found"

        echo ""
        echo "=== Running tests ==="

        # Try pytest with coverage
        if which pytest >/dev/null 2>&1; then
          if [ -f "pyproject.toml" ] || [ -f "setup.cfg" ] || [ -f "pytest.ini" ]; then
            echo "Running pytest with project configuration"
            pytest --cov=. --cov-report=xml --cov-report=term || echo "Tests completed with issues"
          else
            echo "Running pytest with basic configuration"
            pytest -v --tb=short || echo "Tests completed with issues"
          fi
        else
          echo "Pytest not available, trying unittest discovery"
          python -m unittest discover -v || echo "No tests found or unittest failed"
        fi

    - name: Upload coverage (if available)
      if: hashFiles('coverage.xml') != ''
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: basic-coverage

  # Simplified build job that doesn't fail the entire CI
  build-check:
    needs: test
    runs-on: ubuntu-latest
    if: success() || failure()  # Run even if tests fail
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Check Docker setup
      id: docker-check
      run: |
        if [ -f "docker-compose.yml" ] || [ -f "docker-compose.yaml" ]; then
          echo "has_docker=true" >> $GITHUB_OUTPUT
          echo "Docker compose file found"
        else
          echo "has_docker=false" >> $GITHUB_OUTPUT
          echo "No Docker compose file found"
        fi

    - name: Validate Docker setup (if available)
      if: steps.docker-check.outputs.has_docker == 'true'
      run: |
        echo "=== Docker Compose Validation ==="
        docker compose config || echo "Docker compose validation failed"

        echo ""
        echo "=== Available Services ==="
        docker compose config --services 2>/dev/null || echo "No services defined"

    - name: Build status summary
      run: |
        echo "=== Build Summary ==="
        echo "Docker available: ${{ steps.docker-check.outputs.has_docker }}"
        echo "Test job status: ${{ needs.test.result }}"
        echo "Build check completed successfully"

  # Security and quality checks
  security-check:
    runs-on: ubuntu-latest
    if: always()  # Always run security checks
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Basic security scan
        run: |
          pip install safety bandit || echo "Security tools installation failed"

          echo "=== Safety Check ==="
          if which safety >/dev/null 2>&1; then
            safety check --continue-on-error || echo "Safety check completed with warnings"
          else
            echo "Safety not available, skipping"
          fi

          echo ""
          echo "=== Bandit Security Scan ==="
          if which bandit >/dev/null 2>&1; then
            bandit -r . -f json -o bandit-report.json || echo "Bandit scan completed"
            bandit -r . || echo "Bandit scan completed with issues"
          else
            echo "Bandit not available, skipping"
          fi

      - name: Upload security report
        if: hashFiles('bandit-report.json') != ''
        uses: actions/upload-artifact@v3
        with:
          name: security-report
          path: bandit-report.json

  # Full Agent Forge pipeline test (nightly and on-demand)
  full-pipeline-test:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[full-pipeline]')
    env:
      FRONTIER_API_KEY: ${{ secrets.FRONTIER_API_KEY }}
      WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
      WANDB_PROJECT: "agent-forge-ci"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || echo "requirements.txt not found, installing basic deps"
          pip install wandb torch transformers datasets huggingface_hub
          pip install pytest pytest-cov  # For any post-pipeline tests

      - name: Validate pipeline configuration
        run: |
          echo "=== Pipeline Configuration Validation ==="
          python run_full_agent_forge.py --dry-run --device cpu
          echo "Configuration validation completed"

      - name: Run full Agent Forge pipeline
        run: |
          echo "=== Starting Full Agent Forge Pipeline ==="
          python run_full_agent_forge.py \
            --no-deploy \
            --device cpu \
            --quick \
            --timeout 5400 \
            --benchmark-timeout 1800
          echo "Pipeline execution completed"

      - name: Run smoke test validation
        run: |
          echo "=== Running Agent Forge Smoke Test ==="
          python run_smoke_test.py \
            --no-deploy \
            --timeout 3600 \
            --quick
          echo "Smoke test completed"

      - name: Upload pipeline results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: agent-forge-pipeline-results
          path: |
            benchmark_results/
            agent_forge_outputs/
            wandb/
            agent_forge_pipeline_summary.json
            *.log

      - name: Check pipeline success
        run: |
          if [ -f "agent_forge_pipeline_summary.json" ]; then
            echo "=== Pipeline Summary ==="
            python -c "
import json
with open('agent_forge_pipeline_summary.json', 'r') as f:
    summary = json.load(f)
print(f'Pipeline Status: {summary.get(\"pipeline_execution\", {}).get(\"status\", \"unknown\")}')
if 'benchmark_results' in summary:
    model_averages = summary['benchmark_results'].get('model_averages', {})
    if model_averages:
        best_model = max(model_averages, key=model_averages.get)
        best_score = model_averages[best_model]
        print(f'Best Model: {best_model} (Score: {best_score:.3f})')
"
          else
            echo "ERROR: Pipeline summary not found"
            exit 1
          fi

      - name: Performance regression check
        if: success()
        run: |
          echo "=== Performance Regression Check ==="
          python -c "
import json
import os

# Load results
try:
    with open('benchmark_results/agent_forge_model_comparison.json', 'r') as f:
        results = json.load(f)

    model_averages = results.get('model_averages', {})
    if not model_averages:
        print('No model averages found')
        exit(0)

    # Check against minimum thresholds
    thresholds = {'MMLU': 0.40, 'GSM8K': 0.25, 'HumanEval': 0.15}
    benchmark_data = results.get('benchmark_comparison', [])

    if model_averages:
        best_model = max(model_averages, key=model_averages.get)
        print(f'Best model: {best_model} with average {model_averages[best_model]:.3f}')

        # Check individual benchmarks
        regressions = []
        for benchmark_entry in benchmark_data:
            benchmark = benchmark_entry.get('Benchmark', '')
            if benchmark in thresholds and best_model in benchmark_entry:
                score = benchmark_entry[best_model]
                threshold = thresholds[benchmark]
                if score < threshold:
                    regressions.append(f'{benchmark}: {score:.3f} < {threshold:.3f}')

        if regressions:
            print('PERFORMANCE REGRESSIONS DETECTED:')
            for regression in regressions:
                print(f'  - {regression}')
            # Don't fail CI for regressions, just warn
            print('Regressions detected but not failing CI (alerts sent to W&B)')
        else:
            print('No performance regressions detected')

except Exception as e:
    print(f'Regression check failed: {e}')
    # Don't fail CI for regression check issues
"

      - name: Cleanup
        if: always()
        run: |
          echo "=== Cleanup ==="
          # Clean up large files to save space
          find . -name "*.pt" -size +100M -delete 2>/dev/null || true
          find . -name "*.bin" -size +100M -delete 2>/dev/null || true
          echo "Cleanup completed"
