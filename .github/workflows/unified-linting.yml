name: "Unified Code Quality Pipeline"

on:
  push:
    branches: [main, develop, feature/*, fix/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      run_full_scan:
        description: "Run comprehensive quality scan with all tools"
        required: false
        default: "false"
        type: boolean
      target_languages:
        description: "Comma-separated list of languages to lint (python,frontend,security)"
        required: false
        default: "python,frontend,security"
        type: string
      skip_cache:
        description: "Skip performance cache and run fresh scans"
        required: false
        default: "false"
        type: boolean
      fail_on_warnings:
        description: "Fail build on warning-level issues"
        required: false
        default: "false"
        type: boolean

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"
  CARGO_TERM_COLOR: always
  PIP_DISABLE_PIP_VERSION_CHECK: 1

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ============================================
  # STAGE 1: MCP Server Initialization
  # ============================================
  mcp-init:
    name: "Initialize MCP Servers"
    runs-on: ubuntu-latest
    outputs:
      mcp-session-id: ${{ steps.mcp-setup.outputs.session-id }}
      github-mcp-status: ${{ steps.mcp-setup.outputs.github-status }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: "Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: "Install MCP Dependencies"
        run: |
          pip install asyncio aiohttp pydantic
          npm install -g @ruvnet/claude-flow@alpha
          
      - name: "Initialize MCP Server Coordination"
        id: mcp-setup
        run: |
          echo "Starting MCP server coordination for unified linting..."
          
          # Generate session ID
          SESSION_ID="linting-$(date +%Y%m%d-%H%M%S)-${{ github.run_id }}"
          echo "session-id=$SESSION_ID" >> $GITHUB_OUTPUT
          
          # Initialize MCP servers with error handling
          npx claude-flow@alpha mcp start --auto-orchestrator --enable-neural --daemon || echo "MCP server start with fallback"
          
          # Test GitHub MCP integration
          if npx claude-flow@alpha mcp tools --category=github --quiet; then
            echo "github-status=enabled" >> $GITHUB_OUTPUT
            echo "✅ GitHub MCP integration enabled"
          else
            echo "github-status=fallback" >> $GITHUB_OUTPUT
            echo "⚠️ Using fallback GitHub integration"
          fi
          
          # Initialize coordination hooks
          npx claude-flow@alpha hooks pre-task \
            --description "unified-linting-pipeline" \
            --session-id "$SESSION_ID" \
            --github-automation "full" \
            --performance-caching "enabled" || echo "Hook initialization with fallback"
          
          echo "MCP coordination initialized successfully"

  # ============================================
  # STAGE 2: Parallel Linting Execution
  # ============================================
  python-linting:
    name: "Python Code Quality"
    runs-on: ubuntu-latest
    needs: mcp-init
    outputs:
      python-results: ${{ steps.python-lint.outputs.results-file }}
      quality-score: ${{ steps.python-lint.outputs.quality-score }}
      critical-issues: ${{ steps.python-lint.outputs.critical-issues }}
    steps:
      - uses: actions/checkout@v4
        
      - name: "Setup Python Environment"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: "Install Python Linting Tools"
        run: |
          pip install ruff black mypy bandit safety semgrep detect-secrets pip-audit
          pip install -r config/requirements/requirements.txt || echo "No requirements found"
          
      - name: "Cache Linting Results"
        uses: actions/cache@v4
        with:
          path: |
            .ruff_cache
            .mypy_cache
            .bandit_cache
          key: python-lint-${{ hashFiles('**/*.py', 'config/linting/*.yml') }}-${{ github.sha }}
          restore-keys: |
            python-lint-${{ hashFiles('**/*.py', 'config/linting/*.yml') }}-
            python-lint-
            
      - name: "Run Unified Python Linting"
        id: python-lint
        run: |
          echo "Running unified Python linting pipeline..."
          mkdir -p artifacts/python-linting
          
          # Run the unified linting manager
          python config/linting/run_unified_linting.py \
            --language=python \
            --output=artifacts/python-linting/results.json \
            --format=json \
            --github-integration \
            ${{ github.event.inputs.skip_cache == 'true' && '--skip-cache' || '' }} \
            ${{ github.event.inputs.fail_on_warnings == 'true' && '--fail-on-critical' || '' }}
          
          # Extract key metrics for downstream jobs
          if [ -f "artifacts/python-linting/results.json" ]; then
            QUALITY_SCORE=$(jq -r '.quality_metrics.overall_score // 0' artifacts/python-linting/results.json)
            CRITICAL_ISSUES=$(jq -r '.pipeline_summary.critical_issues // 0' artifacts/python-linting/results.json)
            
            echo "results-file=artifacts/python-linting/results.json" >> $GITHUB_OUTPUT
            echo "quality-score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
            echo "critical-issues=$CRITICAL_ISSUES" >> $GITHUB_OUTPUT
            
            echo "✅ Python linting completed - Quality Score: $QUALITY_SCORE/100"
            echo "Critical Issues: $CRITICAL_ISSUES"
          else
            echo "❌ Python linting failed - no results file generated"
            exit 1
          fi
          
      - name: "MCP Post-Processing"
        run: |
          # Notify MCP about Python linting completion
          npx claude-flow@alpha hooks post-edit \
            --file "artifacts/python-linting/results.json" \
            --memory-key "swarm/python-linting/${{ github.run_id }}" \
            --session-id "${{ needs.mcp-init.outputs.mcp-session-id }}" || echo "MCP notification failed"
            
      - name: "Upload Python Results"
        uses: actions/upload-artifact@v4
        with:
          name: python-linting-results
          path: artifacts/python-linting/
          retention-days: 30

  frontend-linting:
    name: "Frontend Code Quality"
    runs-on: ubuntu-latest
    needs: mcp-init
    outputs:
      frontend-results: ${{ steps.frontend-lint.outputs.results-file }}
      quality-score: ${{ steps.frontend-lint.outputs.quality-score }}
      critical-issues: ${{ steps.frontend-lint.outputs.critical-issues }}
    steps:
      - uses: actions/checkout@v4
        
      - name: "Setup Node.js Environment"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: |
            apps/web/package-lock.json
            package-lock.json
            
      - name: "Setup Python for Unified Manager"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: "Install Frontend Linting Tools"
        run: |
          # Global installation of linting tools
          npm install -g eslint prettier typescript
          npm install -g @typescript-eslint/parser @typescript-eslint/eslint-plugin
          npm install -g eslint-plugin-react eslint-plugin-react-hooks
          npm install -g eslint-plugin-security eslint-plugin-import
          npm install -g stylelint stylelint-config-standard
          
          # Install project dependencies if they exist
          if [ -f "apps/web/package.json" ]; then
            cd apps/web && npm install
          fi
          
      - name: "Cache Frontend Dependencies"
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
            apps/web/node_modules
            .eslintcache
          key: frontend-${{ hashFiles('**/package-lock.json', 'config/linting/*.yml') }}-${{ github.sha }}
          restore-keys: |
            frontend-${{ hashFiles('**/package-lock.json', 'config/linting/*.yml') }}-
            frontend-
            
      - name: "Run Unified Frontend Linting"
        id: frontend-lint
        run: |
          echo "Running unified frontend linting pipeline..."
          mkdir -p artifacts/frontend-linting
          
          # Check if frontend files exist
          if find . -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.css" -o -name "*.scss" | head -1 | grep -q .; then
            python config/linting/run_unified_linting.py \
              --language=frontend \
              --output=artifacts/frontend-linting/results.json \
              --format=json \
              --github-integration \
              ${{ github.event.inputs.skip_cache == 'true' && '--skip-cache' || '' }}
          else
            echo "No frontend files found, skipping frontend linting"
            echo '{"status": "skipped", "reason": "no_frontend_files"}' > artifacts/frontend-linting/results.json
          fi
          
          # Extract metrics
          if [ -f "artifacts/frontend-linting/results.json" ]; then
            QUALITY_SCORE=$(jq -r '.quality_metrics.overall_score // 100' artifacts/frontend-linting/results.json)
            CRITICAL_ISSUES=$(jq -r '.pipeline_summary.critical_issues // 0' artifacts/frontend-linting/results.json)
            
            echo "results-file=artifacts/frontend-linting/results.json" >> $GITHUB_OUTPUT
            echo "quality-score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
            echo "critical-issues=$CRITICAL_ISSUES" >> $GITHUB_OUTPUT
            
            echo "✅ Frontend linting completed - Quality Score: $QUALITY_SCORE/100"
          fi
          
      - name: "Upload Frontend Results"
        uses: actions/upload-artifact@v4
        with:
          name: frontend-linting-results
          path: artifacts/frontend-linting/
          retention-days: 30

  security-linting:
    name: "Security Analysis"
    runs-on: ubuntu-latest
    needs: mcp-init
    outputs:
      security-results: ${{ steps.security-lint.outputs.results-file }}
      security-score: ${{ steps.security-lint.outputs.security-score }}
      critical-vulnerabilities: ${{ steps.security-lint.outputs.critical-vulns }}
      block-deployment: ${{ steps.security-lint.outputs.block-deployment }}
    steps:
      - uses: actions/checkout@v4
        
      - name: "Setup Python Environment"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: "Install Security Tools"
        run: |
          pip install bandit safety semgrep detect-secrets pip-audit
          # Install additional security validation dependencies
          pip install cryptography requests urllib3
          
      - name: "Cache Security Scan Data"
        uses: actions/cache@v4
        with:
          path: |
            .pip-audit-cache
            .semgrep-cache
            .safety-cache
          key: security-${{ hashFiles('**/requirements*.txt', '**/*.py') }}-${{ github.sha }}
          restore-keys: |
            security-${{ hashFiles('**/requirements*.txt', '**/*.py') }}-
            security-
            
      - name: "Initialize Security Baseline"
        run: |
          # Create or update secrets baseline
          if [ ! -f ".secrets.baseline" ]; then
            echo "Creating initial secrets baseline..."
            detect-secrets scan --baseline .secrets.baseline || echo "Baseline creation completed with findings"
          fi
          
      - name: "Run Unified Security Linting"
        id: security-lint
        run: |
          echo "Running unified security linting pipeline..."
          mkdir -p artifacts/security-linting
          
          python config/linting/run_unified_linting.py \
            --language=security \
            --output=artifacts/security-linting/results.json \
            --format=json \
            --fail-on-critical \
            --github-integration \
            ${{ github.event.inputs.skip_cache == 'true' && '--skip-cache' || '' }}
          
          # Extract security metrics
          if [ -f "artifacts/security-linting/results.json" ]; then
            SECURITY_SCORE=$(jq -r '.quality_metrics.security_score // 0' artifacts/security-linting/results.json)
            CRITICAL_VULNS=$(jq -r '.pipeline_summary.critical_issues // 0' artifacts/security-linting/results.json)
            TOTAL_SECURITY_ISSUES=$(jq -r '.pipeline_summary.security_issues // 0' artifacts/security-linting/results.json)
            
            echo "results-file=artifacts/security-linting/results.json" >> $GITHUB_OUTPUT
            echo "security-score=$SECURITY_SCORE" >> $GITHUB_OUTPUT
            echo "critical-vulns=$CRITICAL_VULNS" >> $GITHUB_OUTPUT
            
            # Determine if deployment should be blocked
            if [ "$CRITICAL_VULNS" -gt 0 ]; then
              echo "block-deployment=true" >> $GITHUB_OUTPUT
              echo "🚨 CRITICAL: $CRITICAL_VULNS critical security vulnerabilities found - BLOCKING DEPLOYMENT"
            elif [ "$TOTAL_SECURITY_ISSUES" -gt 10 ]; then
              echo "block-deployment=warning" >> $GITHUB_OUTPUT
              echo "⚠️ WARNING: $TOTAL_SECURITY_ISSUES security issues found"
            else
              echo "block-deployment=false" >> $GITHUB_OUTPUT
              echo "✅ Security scan passed - Security Score: $SECURITY_SCORE/100"
            fi
          else
            echo "❌ Security linting failed"
            exit 1
          fi
          
      - name: "Security Gate Enforcement"
        if: steps.security-lint.outputs.block-deployment == 'true'
        run: |
          echo "🛑 SECURITY GATE FAILED - Critical vulnerabilities detected"
          echo "This build is blocked until security issues are resolved"
          exit 1
          
      - name: "Upload Security Results"
        uses: actions/upload-artifact@v4
        with:
          name: security-linting-results
          path: artifacts/security-linting/
          retention-days: 90

  # ============================================
  # STAGE 3: Quality Aggregation & Reporting
  # ============================================
  quality-aggregation:
    name: "Quality Metrics & Reporting"
    runs-on: ubuntu-latest
    needs: [mcp-init, python-linting, frontend-linting, security-linting]
    if: always()
    outputs:
      overall-quality-score: ${{ steps.aggregate.outputs.overall-score }}
      quality-gate-status: ${{ steps.aggregate.outputs.gate-status }}
      deployment-ready: ${{ steps.aggregate.outputs.deployment-ready }}
    steps:
      - uses: actions/checkout@v4
        
      - name: "Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: "Download All Linting Results"
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: "Aggregate Quality Metrics"
        id: aggregate
        run: |
          echo "Aggregating quality metrics from all linting stages..."
          mkdir -p artifacts/quality-report
          
          # Create aggregation script
          cat > aggregate_quality_metrics.py << 'EOF'
          import json
          import os
          from pathlib import Path
          from typing import Dict, Any
          
          def load_results(artifact_dir: str) -> Dict[str, Any]:
              results = {}
              for subdir in Path(artifact_dir).iterdir():
                  if subdir.is_dir():
                      results_file = subdir / "results.json"
                      if results_file.exists():
                          with open(results_file) as f:
                              results[subdir.name.replace('-results', '')] = json.load(f)
              return results
          
          def calculate_overall_quality(results: Dict[str, Any]) -> Dict[str, Any]:
              python_score = 0
              frontend_score = 100  # Default if no frontend
              security_score = 0
              
              total_issues = 0
              critical_issues = 0
              security_issues = 0
              
              for stage, data in results.items():
                  if isinstance(data, dict):
                      if "quality_metrics" in data:
                          metrics = data["quality_metrics"]
                          if stage == "python-linting":
                              python_score = metrics.get("overall_score", 0)
                              security_score = max(security_score, metrics.get("security_score", 0))
                          elif stage == "frontend-linting":
                              frontend_score = metrics.get("overall_score", 100)
                          elif stage == "security-linting":
                              security_score = metrics.get("security_score", 0)
                      
                      if "pipeline_summary" in data:
                          summary = data["pipeline_summary"]
                          total_issues += summary.get("total_issues_found", 0)
                          critical_issues += summary.get("critical_issues", 0)
                          security_issues += summary.get("security_issues", 0)
              
              # Calculate weighted overall score
              overall_score = (python_score * 0.4 + frontend_score * 0.3 + security_score * 0.3)
              
              # Determine quality gate status
              if critical_issues > 0:
                  gate_status = "failed_critical"
                  deployment_ready = "false"
              elif security_score < 70:
                  gate_status = "failed_security"  
                  deployment_ready = "false"
              elif overall_score < 60:
                  gate_status = "failed_overall"
                  deployment_ready = "false"
              elif security_issues > 10:
                  gate_status = "warning_security"
                  deployment_ready = "warning"
              elif total_issues > 100:
                  gate_status = "warning_quality"
                  deployment_ready = "warning"
              else:
                  gate_status = "passed"
                  deployment_ready = "true"
              
              return {
                  "overall_score": round(overall_score, 2),
                  "python_score": python_score,
                  "frontend_score": frontend_score, 
                  "security_score": security_score,
                  "total_issues": total_issues,
                  "critical_issues": critical_issues,
                  "security_issues": security_issues,
                  "quality_gate_status": gate_status,
                  "deployment_ready": deployment_ready,
                  "timestamp": "$(date -Iseconds)",
                  "git_sha": "${{ github.sha }}",
                  "workflow_run": "${{ github.run_id }}"
              }
          
          # Main execution
          results = load_results("artifacts")
          quality_report = calculate_overall_quality(results)
          
          # Save comprehensive report
          comprehensive_report = {
              "quality_summary": quality_report,
              "individual_results": results,
              "github_context": {
                  "ref": "${{ github.ref }}",
                  "sha": "${{ github.sha }}",
                  "actor": "${{ github.actor }}",
                  "event": "${{ github.event_name }}"
              }
          }
          
          with open("artifacts/quality-report/comprehensive_report.json", "w") as f:
              json.dump(comprehensive_report, f, indent=2)
          
          # Output for GitHub Actions
          print(f"overall-score={quality_report['overall_score']}")
          print(f"gate-status={quality_report['quality_gate_status']}")
          print(f"deployment-ready={quality_report['deployment_ready']}")
          EOF
          
          # Run aggregation
          python aggregate_quality_metrics.py >> $GITHUB_OUTPUT
          
          # Load and display summary
          OVERALL_SCORE=$(jq -r '.quality_summary.overall_score' artifacts/quality-report/comprehensive_report.json)
          GATE_STATUS=$(jq -r '.quality_summary.quality_gate_status' artifacts/quality-report/comprehensive_report.json)
          
          echo "📊 Overall Quality Score: $OVERALL_SCORE/100"
          echo "🚪 Quality Gate Status: $GATE_STATUS"
          
      - name: "Generate Quality Dashboard"
        run: |
          echo "Generating quality dashboard..."
          
          cat > artifacts/quality-report/dashboard.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>AIVillage Code Quality Report</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  .header { background: #2c3e50; color: white; padding: 20px; border-radius: 8px; }
                  .metrics { display: flex; gap: 20px; margin: 20px 0; }
                  .metric-card { background: #f8f9fa; padding: 20px; border-radius: 8px; flex: 1; }
                  .score { font-size: 2em; font-weight: bold; }
                  .passed { color: #27ae60; }
                  .warning { color: #f39c12; }
                  .failed { color: #e74c3c; }
                  .issues-list { background: #fff; border: 1px solid #ddd; padding: 15px; border-radius: 5px; }
              </style>
          </head>
          <body>
              <div class="header">
                  <h1>🔍 AIVillage Unified Code Quality Report</h1>
                  <p>Generated: $(date) | Commit: ${{ github.sha }} | Run: ${{ github.run_id }}</p>
              </div>
              
              <div class="metrics">
                  <div class="metric-card">
                      <h3>Overall Score</h3>
                      <div class="score">$(jq -r '.quality_summary.overall_score' artifacts/quality-report/comprehensive_report.json)/100</div>
                  </div>
                  <div class="metric-card">
                      <h3>Security Score</h3>
                      <div class="score">$(jq -r '.quality_summary.security_score' artifacts/quality-report/comprehensive_report.json)/100</div>
                  </div>
                  <div class="metric-card">
                      <h3>Quality Gate</h3>
                      <div class="score">$(jq -r '.quality_summary.quality_gate_status' artifacts/quality-report/comprehensive_report.json)</div>
                  </div>
              </div>
              
              <div class="issues-list">
                  <h3>📋 Summary</h3>
                  <p><strong>Total Issues:</strong> $(jq -r '.quality_summary.total_issues' artifacts/quality-report/comprehensive_report.json)</p>
                  <p><strong>Critical Issues:</strong> $(jq -r '.quality_summary.critical_issues' artifacts/quality-report/comprehensive_report.json)</p>
                  <p><strong>Security Issues:</strong> $(jq -r '.quality_summary.security_issues' artifacts/quality-report/comprehensive_report.json)</p>
              </div>
          </body>
          </html>
          EOF
          
      - name: "MCP Quality Pattern Learning"
        run: |
          # Store quality patterns in MCP memory for learning
          npx claude-flow@alpha hooks post-task \
            --task-id "quality-aggregation" \
            --session-id "${{ needs.mcp-init.outputs.mcp-session-id }}" \
            --memory-store "quality-patterns" \
            --data-file "artifacts/quality-report/comprehensive_report.json" || echo "MCP learning failed"
          
      - name: "Upload Quality Report"
        uses: actions/upload-artifact@v4
        with:
          name: quality-report
          path: artifacts/quality-report/
          retention-days: 90

  # ============================================
  # STAGE 4: GitHub Integration & PR Updates
  # ============================================
  github-integration:
    name: "GitHub PR Integration"
    runs-on: ubuntu-latest
    needs: [mcp-init, quality-aggregation]
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
        
      - name: "Download Quality Report"
        uses: actions/download-artifact@v4
        with:
          name: quality-report
          path: quality-report/
          
      - name: "Generate PR Comment"
        run: |
          # Load quality data
          OVERALL_SCORE=$(jq -r '.quality_summary.overall_score' quality-report/comprehensive_report.json)
          SECURITY_SCORE=$(jq -r '.quality_summary.security_score' quality-report/comprehensive_report.json)
          GATE_STATUS=$(jq -r '.quality_summary.quality_gate_status' quality-report/comprehensive_report.json)
          TOTAL_ISSUES=$(jq -r '.quality_summary.total_issues' quality-report/comprehensive_report.json)
          CRITICAL_ISSUES=$(jq -r '.quality_summary.critical_issues' quality-report/comprehensive_report.json)
          SECURITY_ISSUES=$(jq -r '.quality_summary.security_issues' quality-report/comprehensive_report.json)
          
          # Generate status emoji
          if [[ "$GATE_STATUS" == "passed" ]]; then
            STATUS_EMOJI="✅"
            STATUS_TEXT="PASSED"
          elif [[ "$GATE_STATUS" == *"warning"* ]]; then
            STATUS_EMOJI="⚠️"
            STATUS_TEXT="WARNING"
          else
            STATUS_EMOJI="❌" 
            STATUS_TEXT="FAILED"
          fi
          
          # Create PR comment
          cat > pr-comment.md << EOF
          ## 🔍 Unified Code Quality Report $STATUS_EMOJI
          
          **Overall Quality Score**: $OVERALL_SCORE/100  
          **Security Score**: $SECURITY_SCORE/100  
          **Quality Gate Status**: $STATUS_TEXT
          
          ### 📊 Issues Summary
          - **Total Issues**: $TOTAL_ISSUES
          - **Critical Issues**: $CRITICAL_ISSUES  
          - **Security Issues**: $SECURITY_ISSUES
          
          ### 🔧 Tools Executed
          - **Python**: ruff, black, mypy, bandit
          - **Frontend**: eslint, prettier, typescript  
          - **Security**: detect-secrets, semgrep, pip-audit
          
          ### 📈 Quality Breakdown
          - **Python Score**: $(jq -r '.quality_summary.python_score' quality-report/comprehensive_report.json)/100
          - **Frontend Score**: $(jq -r '.quality_summary.frontend_score' quality-report/comprehensive_report.json)/100
          - **Security Score**: $SECURITY_SCORE/100
          
          ### 🎯 Next Steps
          EOF
          
          # Add context-specific recommendations
          if [[ "$CRITICAL_ISSUES" -gt 0 ]]; then
            echo "- 🚨 **CRITICAL**: Address $CRITICAL_ISSUES critical issues before merge" >> pr-comment.md
          fi
          
          if [[ "$SECURITY_ISSUES" -gt 0 ]]; then
            echo "- 🔒 Review $SECURITY_ISSUES security findings" >> pr-comment.md
          fi
          
          if [[ "$GATE_STATUS" == "passed" ]]; then
            echo "- ✅ All quality gates passed - ready for merge" >> pr-comment.md
            echo "- 🎉 Great work maintaining code quality!" >> pr-comment.md
          fi
          
          echo "" >> pr-comment.md
          echo "---" >> pr-comment.md
          echo "*Generated by AIVillage Unified Linting Pipeline with MCP Integration*" >> pr-comment.md
          echo "*Workflow Run: [\`${{ github.run_id }}\`](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})*" >> pr-comment.md
          
      - name: "Post PR Comment"
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('pr-comment.md', 'utf8');
            
            // Check if we already commented on this PR
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
            });
            
            const botComment = comments.data.find(comment => 
              comment.body.includes('Unified Code Quality Report') && 
              comment.user.type === 'Bot'
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body: comment
              });
            }
            
      - name: "Set PR Status Checks"
        uses: actions/github-script@v7
        with:
          script: |
            const qualityData = JSON.parse(require('fs').readFileSync('quality-report/comprehensive_report.json', 'utf8'));
            const gateStatus = qualityData.quality_summary.quality_gate_status;
            
            // Map internal status to GitHub check status
            let state, description;
            if (gateStatus === 'passed') {
              state = 'success';
              description = 'All quality gates passed';
            } else if (gateStatus.includes('warning')) {
              state = 'success'; // Allow merge but with warnings
              description = 'Quality checks passed with warnings';
            } else {
              state = 'failure';
              description = 'Quality gates failed - review required';
            }
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              target_url: `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`,
              description: description,
              context: 'unified-linting/quality-gate'
            });

  # ============================================
  # STAGE 5: MCP Session Cleanup
  # ============================================
  mcp-cleanup:
    name: "MCP Session Cleanup"
    runs-on: ubuntu-latest
    needs: [mcp-init, quality-aggregation, github-integration]
    if: always()
    steps:
      - name: "MCP Session Finalization"
        run: |
          echo "Finalizing MCP session and exporting metrics..."
          
          # Export session metrics and learnings
          npx claude-flow@alpha hooks session-end \
            --session-id "${{ needs.mcp-init.outputs.mcp-session-id }}" \
            --export-metrics true \
            --save-patterns true || echo "MCP cleanup completed with warnings"
          
          echo "✅ MCP coordination session completed successfully"

  # ============================================
  # STAGE 6: Quality Gate Final Decision
  # ============================================
  quality-gate:
    name: "Quality Gate Enforcement"
    runs-on: ubuntu-latest
    needs: [quality-aggregation]
    if: always()
    steps:
      - name: "Quality Gate Decision"
        run: |
          GATE_STATUS="${{ needs.quality-aggregation.outputs.quality-gate-status }}"
          DEPLOYMENT_READY="${{ needs.quality-aggregation.outputs.deployment-ready }}"
          OVERALL_SCORE="${{ needs.quality-aggregation.outputs.overall-quality-score }}"
          
          echo "🎯 Quality Gate Enforcement"
          echo "Gate Status: $GATE_STATUS"
          echo "Deployment Ready: $DEPLOYMENT_READY" 
          echo "Overall Score: $OVERALL_SCORE/100"
          
          case "$GATE_STATUS" in
            "passed")
              echo "✅ All quality gates PASSED - Ready for deployment"
              ;;
            "warning"*)
              echo "⚠️ Quality gates passed with WARNINGS - Review recommended"
              ;;
            "failed_critical")
              echo "🚨 CRITICAL issues found - BLOCKING deployment"
              exit 1
              ;;
            "failed_security")
              echo "🔒 SECURITY gate failed - BLOCKING deployment"  
              exit 1
              ;;
            "failed_overall")
              echo "📊 OVERALL quality gate failed - Review required"
              exit 1
              ;;
            *)
              echo "❓ Unknown gate status - Manual review required"
              exit 1
              ;;
          esac