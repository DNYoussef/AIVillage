name: Operational Artifacts Collection

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      parallel_collection:
        description: 'Enable parallel artifact collection'
        required: false
        default: 'true'
        type: boolean
      verbose_output:
        description: 'Enable verbose logging'
        required: false
        default: 'false'
        type: boolean

jobs:
  collect-artifacts:
    name: Collect Operational Artifacts
    runs-on: ubuntu-latest
    timeout-minutes: 30

    permissions:
      contents: read
      security-events: write  # For uploading security artifacts to GitHub Security tab

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch full history for hotspot analysis
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          git \
          curl \
          jq \
          build-essential

    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install -r config/requirements/requirements.txt
        pip install -r config/requirements/requirements-dev.txt

        # Install additional tools for artifact collection
        pip install \
          coverage[toml] \
          pytest-cov \
          pytest-benchmark \
          bandit[toml] \
          safety \
          pip-audit \
          cyclonedx-py \
          memory-profiler \
          radon \
          mypy

    - name: Install security scanning tools
      run: |
        # Install Semgrep
        python -m pip install semgrep

        # Install Trivy
        sudo apt-get install wget apt-transport-https gnupg lsb-release
        wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
        echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
        sudo apt-get update
        sudo apt-get install trivy

        # Install Grype
        curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin

    - name: Create artifacts directories
      run: |
        mkdir -p artifacts/{coverage,security,sbom,performance,quality,containers,compliance,reports}

    - name: Run test suite with coverage
      run: |
        # Run tests with coverage collection
        pytest tests/ \
          --cov=packages \
          --cov-report=xml:artifacts/coverage/coverage.xml \
          --cov-report=html:artifacts/coverage/htmlcov \
          --cov-report=json:artifacts/coverage/coverage.json \
          --cov-report=term-missing \
          --benchmark-skip || true

    - name: Run performance benchmarks
      run: |
        # Run performance benchmarks if they exist
        if [ -d "tests/benchmarks" ]; then
          pytest tests/benchmarks/ \
            --benchmark-json=artifacts/performance/benchmark-results.json \
            --benchmark-only || true
        fi

    - name: Collect operational artifacts
      run: |
        python scripts/operational/collect_artifacts.py \
          --output-dir artifacts \
          ${{ github.event.inputs.parallel_collection == 'true' && '--parallel' || '' }} \
          ${{ github.event.inputs.verbose_output == 'true' && '--verbose' || '' }} || true

    - name: Upload coverage to Codecov
      if: always()
      uses: codecov/codecov-action@v3
      with:
        files: ./artifacts/coverage/coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload security results to GitHub Security tab
      if: always()
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: artifacts/security/
      continue-on-error: true

    - name: Archive artifact collection
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: operational-artifacts-${{ github.run_id }}
        path: |
          artifacts/
          !artifacts/**/*.html
        retention-days: 30

    - name: Archive HTML reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: html-reports-${{ github.run_id }}
        path: |
          artifacts/coverage/htmlcov/
          artifacts/**/*.html
        retention-days: 7

    - name: Generate artifacts summary
      if: always()
      run: |
        echo "## ðŸ”§ Operational Artifacts Collection Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "artifacts/reports/collection_report_*.json" ]; then
          REPORT_FILE=$(ls artifacts/reports/collection_report_*.json | head -1)

          echo "### ðŸ“Š Collection Statistics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Category | Artifacts | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-----------|--------|" >> $GITHUB_STEP_SUMMARY

          # Parse JSON report and create summary table
          cat > /tmp/parse_report.py << 'EOF'
import json
import sys
import os

report_file = os.environ.get('REPORT_FILE', '')
try:
    with open(report_file, "r") as f:
        report = json.load(f)

    categories = [
        ("Coverage", len(report.get("coverage_artifacts", []))),
        ("Security", len(report.get("security_artifacts", []))),
        ("SBOM", len(report.get("sbom_artifacts", []))),
        ("Performance", len(report.get("performance_artifacts", []))),
        ("Quality", len(report.get("quality_artifacts", []))),
        ("Container", len(report.get("container_artifacts", []))),
        ("Compliance", len(report.get("compliance_artifacts", [])))
    ]

    for category, count in categories:
        status = "[OK]" if count > 0 else "[FAIL]"
        print(f"| {category} | {count} | {status} |")

    print()
    total = report.get("total_artifacts", 0)
    successful = report.get("successful_artifacts", 0)
    size_mb = report.get("total_size_mb", 0)
    duration = report.get("end_time", 0) - report.get("start_time", 0)

    print(f"**Total Artifacts:** {total}")
    print(f"**Success Rate:** {successful}/{total} ({successful/max(1,total)*100:.1f}%)")
    print(f"**Total Size:** {size_mb:.2f} MB")
    print(f"**Collection Duration:** {duration:.1f}s")

except Exception as e:
    print("Error parsing collection report:", e)
EOF
          REPORT_FILE="$REPORT_FILE" python3 /tmp/parse_report.py >> $GITHUB_STEP_SUMMARY
        else
          echo "[ERROR] Collection report not found" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Check artifact collection thresholds
      if: always()
      run: |
        # Parse collection report and check thresholds
        REPORT_FILE=$(ls artifacts/reports/collection_report_*.json | head -1 || echo "")

        if [ -n "$REPORT_FILE" ] && [ -f "$REPORT_FILE" ]; then
          cat > /tmp/check_threshold.py << 'EOF'
import json
import sys
import os

report_file = os.environ.get('REPORT_FILE', '')
with open(report_file, "r") as f:
    report = json.load(f)

total = report.get("total_artifacts", 0)
successful = report.get("successful_artifacts", 0)

if total == 0:
    print("Warning: No artifacts collected")
    sys.exit(1)

success_rate = successful / total
if success_rate < 0.8:  # 80% threshold
    print(f"Error: Artifact collection success rate too low: {success_rate:.1%}")
    sys.exit(1)

print(f"[SUCCESS] Artifact collection successful: {success_rate:.1%} ({successful}/{total})")
EOF
          REPORT_FILE="$REPORT_FILE" python3 /tmp/check_threshold.py
        else
          echo "[ERROR] Could not find collection report for threshold checking"
          exit 1
        fi

  # Security analysis job
  security-analysis:
    name: Security Analysis
    runs-on: ubuntu-latest
    needs: collect-artifacts
    if: always()

    steps:
    - name: Download artifacts
      uses: actions/download-artifact@v4
      with:
        name: operational-artifacts-${{ github.run_id }}
        path: artifacts/

    - name: Analyze security findings
      run: |
        echo "## ðŸ”’ Security Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Count security findings
        BANDIT_FILE="artifacts/security/bandit-report.json"
        SAFETY_FILE="artifacts/security/safety-report.json"

        if [ -f "$BANDIT_FILE" ]; then
          HIGH_ISSUES=$(jq -r '.results | map(select(.issue_severity == "HIGH")) | length' "$BANDIT_FILE" 2>/dev/null || echo "0")
          MEDIUM_ISSUES=$(jq -r '.results | map(select(.issue_severity == "MEDIUM")) | length' "$BANDIT_FILE" 2>/dev/null || echo "0")
          echo "**Bandit SAST:** $HIGH_ISSUES high, $MEDIUM_ISSUES medium issues" >> $GITHUB_STEP_SUMMARY
        fi

        if [ -f "$SAFETY_FILE" ]; then
          VULNS=$(jq -r '.vulnerabilities | length' "$SAFETY_FILE" 2>/dev/null || echo "0")
          echo "**Safety Check:** $VULNS dependency vulnerabilities" >> $GITHUB_STEP_SUMMARY
        fi

  # Performance analysis job
  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: collect-artifacts
    if: always()

    steps:
    - name: Download artifacts
      uses: actions/download-artifact@v4
      with:
        name: operational-artifacts-${{ github.run_id }}
        path: artifacts/

    - name: Analyze performance results
      run: |
        echo "## âš¡ Performance Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        BENCHMARK_FILE="artifacts/performance/benchmark-results.json"

        if [ -f "$BENCHMARK_FILE" ]; then
          cat > /tmp/analyze_bench.py << 'EOF'
import json
import os
try:
    benchmark_file = os.environ.get('BENCHMARK_FILE', '')
    with open(benchmark_file, "r") as f:
        data = json.load(f)

    benchmarks = data.get("benchmarks", [])
    print(f"**Total Benchmarks:** {len(benchmarks)}")

    if benchmarks:
        avg_time = sum(b.get("stats", {}).get("mean", 0) for b in benchmarks) / len(benchmarks)
        print(f"**Average Execution Time:** {avg_time:.4f}s")

        slowest = max(benchmarks, key=lambda b: b.get("stats", {}).get("mean", 0))
        print(f"**Slowest Benchmark:** {slowest.get("name", "unknown")} ({slowest.get("stats", {}).get("mean", 0):.4f}s)")

except Exception as e:
    print("Error analyzing benchmarks:", e)
EOF
          BENCHMARK_FILE="$BENCHMARK_FILE" python3 /tmp/analyze_bench.py >> $GITHUB_STEP_SUMMARY
        else
          echo "[ERROR] No benchmark results found" >> $GITHUB_STEP_SUMMARY
        fi
