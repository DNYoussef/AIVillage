name: Frontier Curriculum Engine Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/agent_forge/curriculum/**'
      - 'tests/curriculum/**'
      - '.github/workflows/curriculum-tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/agent_forge/curriculum/**'
      - 'tests/curriculum/**'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_live_tests:
        description: 'Run live API tests (requires OPENROUTER_API_KEY)'
        type: boolean
        default: false
      performance_benchmarks:
        description: 'Run performance benchmarks'
        type: boolean
        default: true

env:
  PYTHON_VERSION: '3.11'
  CURRICULUM_TEST_MODE: 'ci'

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    name: Unit Tests

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-curriculum-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-curriculum-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio pytest-cov
        pip install -r src/agent_forge/curriculum/requirements.txt
        pip install -e .

    - name: Run unit tests
      run: |
        pytest tests/curriculum/ \
          -v \
          --cov=src/agent_forge/curriculum \
          --cov-report=xml \
          --cov-report=term-missing \
          --ignore=tests/curriculum/test_live_api_integration.py \
          --ignore=tests/curriculum/performance/ \
          --ignore=tests/curriculum/validation/ \
          --asyncio-mode=auto

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: curriculum-unit-tests
        name: curriculum-coverage

  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: unit-tests

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio psutil
        pip install -r src/agent_forge/curriculum/requirements.txt
        pip install -e .

    - name: Run integration tests
      run: |
        pytest tests/curriculum/test_integration_comprehensive.py \
          tests/curriculum/integration/ \
          -v \
          --asyncio-mode=auto \
          --tb=short

    - name: Upload test artifacts
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-logs
        path: |
          pytest.log
          /tmp/curriculum_*.log

  live-api-tests:
    runs-on: ubuntu-latest
    name: Live API Tests
    needs: integration-tests
    if: |
      github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.run_live_tests == 'true') ||
      contains(github.event.head_commit.message, '[run-live-tests]')

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio
        pip install -r src/agent_forge/curriculum/requirements.txt
        pip install -e .

    - name: Run live API tests
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      run: |
        pytest tests/curriculum/test_live_api_integration.py \
          -v \
          --asyncio-mode=auto \
          --tb=short \
          -m "not slow"

    - name: Upload API test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: live-api-test-results
        path: |
          .forge/cache/
          *.jsonl

  performance-benchmarks:
    runs-on: ubuntu-latest
    name: Performance Benchmarks
    needs: integration-tests
    if: |
      github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.performance_benchmarks == 'true')

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio psutil numpy
        pip install -r src/agent_forge/curriculum/requirements.txt
        pip install -e .

    - name: Run performance benchmarks
      run: |
        pytest tests/curriculum/performance/ \
          -v \
          --asyncio-mode=auto \
          --tb=short \
          --durations=10

    - name: Generate performance report
      if: always()
      run: |
        python -c "
        import json
        import os
        from datetime import datetime

        # Create performance report
        report = {
          'timestamp': datetime.utcnow().isoformat(),
          'commit': os.getenv('GITHUB_SHA', 'unknown'),
          'branch': os.getenv('GITHUB_REF_NAME', 'unknown'),
          'runner': 'github-actions'
        }

        with open('performance_report.json', 'w') as f:
          json.dump(report, f, indent=2)
        "

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-benchmarks
        path: |
          performance_report.json
          test_performance_*.json

  effectiveness-validation:
    runs-on: ubuntu-latest
    name: Curriculum Effectiveness Validation
    needs: integration-tests
    if: |
      github.event_name == 'schedule' ||
      contains(github.event.head_commit.message, '[validate-effectiveness]')

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio numpy scipy
        pip install -r src/agent_forge/curriculum/requirements.txt
        pip install -e .

    - name: Run effectiveness validation
      run: |
        pytest tests/curriculum/validation/ \
          -v \
          --asyncio-mode=auto \
          --tb=short

    - name: Upload validation results
      uses: actions/upload-artifact@v3
      with:
        name: effectiveness-validation
        path: |
          curriculum_effectiveness_*.json
          validation_report_*.md

  security-scan:
    runs-on: ubuntu-latest
    name: Security Scan

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety

    - name: Run Bandit security scan
      run: |
        bandit -r src/agent_forge/curriculum/ \
          -f json \
          -o curriculum_security_scan.json \
          --confidence-level medium \
          --severity-level medium

    - name: Run Safety dependency check
      run: |
        safety check \
          --file src/agent_forge/curriculum/requirements.txt \
          --json \
          --output safety_report.json || true

    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: |
          curriculum_security_scan.json
          safety_report.json

  code-quality:
    runs-on: ubuntu-latest
    name: Code Quality Checks

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install quality tools
      run: |
        python -m pip install --upgrade pip
        pip install black isort ruff mypy
        pip install -r src/agent_forge/curriculum/requirements.txt

    - name: Check code formatting with Black
      run: |
        black --check --diff --line-length=120 src/agent_forge/curriculum/

    - name: Check import sorting with isort
      run: |
        isort --check-only --diff --line-length=120 src/agent_forge/curriculum/

    - name: Run Ruff linting
      run: |
        ruff check src/agent_forge/curriculum/ \
          --select E,W,F,I,UP,B,C4,S106,S107,S108 \
          --line-length=120

    - name: Run mypy type checking
      run: |
        mypy src/agent_forge/curriculum/ \
          --ignore-missing-imports \
          --no-strict-optional

    - name: Upload code quality reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: code-quality-reports
        path: |
          ruff_report.txt

  build-summary:
    runs-on: ubuntu-latest
    name: Build Summary
    needs: [unit-tests, integration-tests, security-scan, code-quality]
    if: always()

    steps:
    - name: Generate build summary
      run: |
        echo "## Frontier Curriculum Engine - Build Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Job Status" >> $GITHUB_STEP_SUMMARY
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Security Scan: ${{ needs.security-scan.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Code Quality: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ needs.live-api-tests.result }}" != "" ]; then
          echo "- Live API Tests: ${{ needs.live-api-tests.result }}" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.performance-benchmarks.result }}" != "" ]; then
          echo "- Performance Benchmarks: ${{ needs.performance-benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.effectiveness-validation.result }}" != "" ]; then
          echo "- Effectiveness Validation: ${{ needs.effectiveness-validation.result }}" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Build Information" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Timestamp**: $(date -u)" >> $GITHUB_STEP_SUMMARY
