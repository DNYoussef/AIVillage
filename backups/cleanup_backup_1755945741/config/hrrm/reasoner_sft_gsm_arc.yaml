# Reasoner SFT configuration for GSM8K and ARC
vocab_size: 32000
d_model: 512
n_layers: 16
n_head: 8
max_seq_len: 2048
rope_base: 10000

# HRM parameters
max_H: 4
inner_T: 4

# Reasoning tokens
start_thought_token: "<SoT>"
end_thought_token: "<EoT>"

# Self-consistency parameters
self_consistency_k: 5

# Training
dropout: 0.1
tie_embeddings: true

# Training hyperparameters
learning_rate: 1e-4  # Lower for SFT
weight_decay: 0.1
warmup_steps: 500
max_steps: 15000
batch_size: 16
gradient_accumulation_steps: 2

# Thought supervision weight
lambda_thought: 0.2

# Scheduler
scheduler_type: "linear"
min_lr_ratio: 0.0

# Logging
log_interval: 25
checkpoint_interval: 1000
