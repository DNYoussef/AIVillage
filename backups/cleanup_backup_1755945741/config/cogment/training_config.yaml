# Cogment Training Configuration
# 4-Stage curriculum with multi-optimizer setup and GrokFast integration

# 4-Stage Curriculum Configuration
curriculum:
  enabled: true
  auto_advance_stages: true

  stage_0:
    name: "sanity"
    description: "Sanity checks and basic functionality validation"
    max_steps: 500
    batch_size: 8
    sequence_length: 256
    learning_rate: 3e-4
    max_refinement_steps: 2
    act_threshold: 0.99
    ponder_cost_initial: 0.0
    ponder_cost_final: 0.0
    convergence_patience: 100
    min_accuracy: 0.8
    max_ponder_cost: 2.0

  stage_1:
    name: "arc_visual"
    description: "ARC-like visual reasoning with augmentations"
    max_steps: 4000
    batch_size: 8
    sequence_length: 512
    learning_rate: 2e-4
    max_refinement_steps: 4
    act_threshold: 0.99
    ponder_cost_initial: 0.005
    ponder_cost_final: 0.01
    convergence_patience: 500
    min_accuracy: 0.7
    max_ponder_cost: 4.0

    # Data augmentation for ARC
    augmentation:
      enabled: true
      rate: 0.8
      types: ["rotation", "reflection", "color_permutation"]
      rotation_count: 4

  stage_2:
    name: "algorithmic"
    description: "Algorithmic puzzles and structured reasoning"
    max_steps: 8000
    batch_size: 8
    sequence_length: 1024
    learning_rate: 1e-4
    max_refinement_steps: 6
    act_threshold: 0.99
    ponder_cost_initial: 0.01
    ponder_cost_final: 0.015
    convergence_patience: 800
    min_accuracy: 0.7
    max_ponder_cost: 6.0

  stage_3:
    name: "math_reasoning"
    description: "Mathematical reasoning and multi-hop text"
    max_steps: 16000
    batch_size: 4
    sequence_length: 1536
    learning_rate: 5e-5
    max_refinement_steps: 8
    act_threshold: 0.99
    ponder_cost_initial: 0.02
    ponder_cost_final: 0.02
    convergence_patience: 1000
    min_accuracy: 0.6
    max_ponder_cost: 8.0

  stage_4:
    name: "long_context"
    description: "Long-context understanding and generation"
    max_steps: 32000
    batch_size: 2
    sequence_length: 2048
    learning_rate: 2e-5
    max_refinement_steps: 8
    act_threshold: 0.99
    ponder_cost_initial: 0.02
    ponder_cost_final: 0.02
    convergence_patience: 1500
    min_accuracy: 0.5
    max_ponder_cost: 8.0

# Multi-Optimizer Setup (from Agent 4's TrainingConfig)
optimizers:
  refinement_core:
    type: "AdamW"
    lr: 3e-4
    weight_decay: 0.01
    betas: [0.9, 0.95]

  gated_ltm:
    type: "AdamW"
    lr: 1e-4
    weight_decay: 0.005
    betas: [0.9, 0.999]

  act_halting:
    type: "AdamW"
    lr: 5e-4
    weight_decay: 0.01
    betas: [0.9, 0.95]

  other_components:
    type: "AdamW"
    lr: 2e-4
    weight_decay: 0.01
    betas: [0.9, 0.95]

# Scheduler Configuration
schedulers:
  type: "cosine"                 # cosine, linear, constant
  warmup_steps: 1000
  min_lr_ratio: 0.1

# Training Parameters
training:
  max_epochs: 10
  gradient_clip_norm: 1.0
  accumulation_steps: 1

  # Evaluation
  eval_interval: 500
  save_interval: 2000
  log_interval: 100

  # Early stopping
  early_stopping_patience: 5000
  min_delta: 1e-4

  # Mixed precision
  use_amp: true
  amp_dtype: "float16"

  # Checkpointing
  save_dir: "./checkpoints/cogment"
  save_best_only: false

# Memory Management
memory:
  ltm_decay_interval: 100
  ltm_consolidation_interval: 5000
  ltm_write_frequency: "adaptive"  # "always", "adaptive", "stage_dependent"

  # Stage-specific memory settings
  stage_memory_settings:
    stage_0:
      ltm_read_only: true
      ltm_write_alpha: 0.0
      ltm_decay_rate: 0.0

    stage_1:
      ltm_read_only: false
      ltm_write_alpha: 0.05
      ltm_decay_rate: 1e-3

    stage_2:
      ltm_read_only: false
      ltm_write_alpha: 0.1
      ltm_decay_rate: 1e-3

    stage_3:
      ltm_read_only: false
      ltm_write_alpha: 0.15
      ltm_decay_rate: 5e-4

    stage_4:
      ltm_read_only: false
      ltm_write_alpha: 0.2
      ltm_decay_rate: 1e-4

# Loss Function Configuration
loss:
  # Loss component weights
  deep_supervision_weight: 1.0
  improvement_weight: 0.1
  consistency_weight: 0.1
  ponder_weight: 1.0

  # Stage-specific loss scaling
  stage_loss_scaling:
    stage_0: 1.0
    stage_1: 1.0
    stage_2: 1.2
    stage_3: 1.5
    stage_4: 2.0
