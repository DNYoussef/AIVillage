#!/usr/bin/env python3
"""HypeRAG Creativity Benchmark Suite.

Measures novelty and plausibility of creative connections generated by the HypeRAG system.

Metrics:
- Surprise@5: Mean path rarity of top 5 creative bridges
- Guardian Pass%: Percentage of creative bridges that pass Guardian validation
- User Usefulness: Manual evaluation scores imported from JSON
- Hidden-Link Precision: Precision of discovered connections after consolidation

Datasets:
- Remote Association Challenge: Synthetic sets requiring novel connections
- Cross-Domain Concept Pairs: Biology ↔ Finance, Physics ↔ Agriculture connections
"""

import argparse
import asyncio
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
import json
import logging
from pathlib import Path
import statistics

# Import HypeRAG components
import sys
from typing import Any

import numpy as np

sys.path.append(str(Path(__file__).parent.parent))

from mcp_servers.hyperag.guardian.gate import CreativeBridge, GuardianGate

logger = logging.getLogger(__name__)


@dataclass
class CreativityMetrics:
    """Creativity benchmark results."""

    surprise_at_5: float
    guardian_pass_rate: float
    user_usefulness_score: float
    hidden_link_precision: float
    total_bridges_evaluated: int
    dataset_name: str
    timestamp: str


@dataclass
class RemoteAssociationTask:
    """Remote Association Challenge task."""

    task_id: str
    cue_words: list[str]  # e.g., ["cottage", "swiss", "cake"]
    target_word: str  # e.g., "cheese"
    domain: str
    difficulty: str  # easy, medium, hard


@dataclass
class CrossDomainPair:
    """Cross-domain concept connection task."""

    pair_id: str
    source_concept: str
    source_domain: str
    target_concept: str
    target_domain: str
    expected_connections: list[str]  # Ground truth connections


class CreativityDatasetGenerator:
    """Generates creativity evaluation datasets."""

    def __init__(self) -> None:
        self.remote_association_tasks = self._create_remote_association_tasks()
        self.cross_domain_pairs = self._create_cross_domain_pairs()

    def _create_remote_association_tasks(self) -> list[RemoteAssociationTask]:
        """Create remote association challenge tasks."""
        tasks = [
            # Easy tasks
            RemoteAssociationTask("rat_001", ["cottage", "swiss", "cake"], "cheese", "food", "easy"),
            RemoteAssociationTask("rat_002", ["dream", "break", "light"], "day", "time", "easy"),
            RemoteAssociationTask("rat_003", ["fish", "mine", "rush"], "gold", "metal", "easy"),
            RemoteAssociationTask("rat_004", ["base", "snow", "dance"], "ball", "sports", "easy"),
            RemoteAssociationTask("rat_005", ["mouse", "sharp", "blue"], "cheese", "food", "easy"),
            # Medium tasks
            RemoteAssociationTask("rat_006", ["nuclear", "feud", "album"], "family", "social", "medium"),
            RemoteAssociationTask("rat_007", ["measure", "worm", "video"], "tape", "material", "medium"),
            RemoteAssociationTask("rat_008", ["flower", "friend", "scout"], "girl", "person", "medium"),
            RemoteAssociationTask("rat_009", ["painting", "bowl", "nail"], "finger", "body", "medium"),
            RemoteAssociationTask("rat_010", ["river", "note", "account"], "bank", "finance", "medium"),
            # Hard tasks
            RemoteAssociationTask("rat_011", ["phoenix", "york", "lawyer"], "new", "concept", "hard"),
            RemoteAssociationTask("rat_012", ["magic", "chocolate", "fortune"], "cookie", "food", "hard"),
            RemoteAssociationTask("rat_013", ["sleeping", "bean", "trash"], "bag", "container", "hard"),
            RemoteAssociationTask("rat_014", ["cross", "rain", "tie"], "bow", "object", "hard"),
            RemoteAssociationTask("rat_015", ["widget", "days", "person"], "sales", "business", "hard"),
        ]
        return tasks

    def _create_cross_domain_pairs(self) -> list[CrossDomainPair]:
        """Create cross-domain concept connection tasks."""
        pairs = [
            # Biology ↔ Finance
            CrossDomainPair(
                "cd_001",
                "ecosystem",
                "biology",
                "market",
                "finance",
                ["balance", "diversity", "stability", "competition", "adaptation"],
            ),
            CrossDomainPair(
                "cd_002",
                "cell_division",
                "biology",
                "stock_split",
                "finance",
                ["growth", "multiplication", "distribution", "value_preservation"],
            ),
            CrossDomainPair(
                "cd_003",
                "symbiosis",
                "biology",
                "partnership",
                "finance",
                [
                    "mutual_benefit",
                    "cooperation",
                    "interdependence",
                    "shared_resources",
                ],
            ),
            CrossDomainPair(
                "cd_004",
                "natural_selection",
                "biology",
                "market_selection",
                "finance",
                ["survival", "efficiency", "competitive_advantage", "elimination"],
            ),
            CrossDomainPair(
                "cd_005",
                "immune_system",
                "biology",
                "risk_management",
                "finance",
                ["protection", "detection", "response", "memory", "adaptation"],
            ),
            # Physics ↔ Agriculture
            CrossDomainPair(
                "cd_006",
                "thermodynamics",
                "physics",
                "crop_rotation",
                "agriculture",
                ["energy_conservation", "efficiency", "equilibrium", "cycle"],
            ),
            CrossDomainPair(
                "cd_007",
                "wave_propagation",
                "physics",
                "pest_spread",
                "agriculture",
                ["transmission", "amplitude", "frequency", "interference", "damping"],
            ),
            CrossDomainPair(
                "cd_008",
                "quantum_entanglement",
                "physics",
                "plant_communication",
                "agriculture",
                [
                    "non_local_connection",
                    "instant_response",
                    "correlation",
                    "information_transfer",
                ],
            ),
            CrossDomainPair(
                "cd_009",
                "phase_transitions",
                "physics",
                "seasonal_changes",
                "agriculture",
                [
                    "critical_points",
                    "state_change",
                    "temperature_dependence",
                    "reversibility",
                ],
            ),
            CrossDomainPair(
                "cd_010",
                "resonance",
                "physics",
                "companion_planting",
                "agriculture",
                [
                    "amplification",
                    "frequency_matching",
                    "constructive_interference",
                    "optimization",
                ],
            ),
            # Additional cross-domain pairs
            CrossDomainPair(
                "cd_011",
                "neural_networks",
                "neuroscience",
                "supply_chains",
                "logistics",
                [
                    "connectivity",
                    "information_flow",
                    "bottlenecks",
                    "optimization",
                    "learning",
                ],
            ),
            CrossDomainPair(
                "cd_012",
                "protein_folding",
                "biochemistry",
                "organizational_structure",
                "management",
                ["configuration", "stability", "function", "misfolding", "chaperones"],
            ),
        ]
        return pairs


class CreativityBenchmark:
    """Main creativity benchmark evaluation system."""

    def __init__(
        self,
        hyperag_client=None,
        guardian_gate: GuardianGate | None = None,
        output_dir: Path = Path("./benchmark_results"),
    ) -> None:
        self.hyperag_client = hyperag_client
        self.guardian_gate = guardian_gate or GuardianGate()
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize dataset generator
        self.dataset_generator = CreativityDatasetGenerator()

        # Path metrics tracking
        self.path_rarity_cache = {}

        # User evaluation scores (to be loaded from JSON)
        self.user_scores = {}

    def load_user_evaluation_scores(self, scores_path: Path) -> None:
        """Load manual user evaluation scores from JSON file."""
        try:
            with open(scores_path) as f:
                self.user_scores = json.load(f)
            logger.info(f"Loaded {len(self.user_scores)} user evaluation scores")
        except FileNotFoundError:
            logger.warning(f"User scores file not found: {scores_path}")
            self.user_scores = {}
        except Exception as e:
            logger.exception(f"Error loading user scores: {e}")
            self.user_scores = {}

    async def evaluate_remote_association_tasks(self) -> dict[str, Any]:
        """Evaluate creativity on Remote Association Challenge tasks."""
        logger.info("Evaluating Remote Association Challenge tasks...")

        results = {"task_results": [], "summary_metrics": {}, "creative_bridges": []}

        for task in self.dataset_generator.remote_association_tasks:
            try:
                # Generate creative bridges for the task
                bridges = await self._generate_creative_bridges_for_task(task)

                # Evaluate each bridge
                bridge_evaluations = []
                for bridge in bridges:
                    evaluation = await self._evaluate_creative_bridge(bridge, task)
                    bridge_evaluations.append(evaluation)

                # Calculate task-level metrics
                task_result = {
                    "task_id": task.task_id,
                    "cue_words": task.cue_words,
                    "target_word": task.target_word,
                    "difficulty": task.difficulty,
                    "bridges_generated": len(bridges),
                    "bridges_evaluated": bridge_evaluations,
                    "surprise_scores": [e["surprise_score"] for e in bridge_evaluations],
                    "guardian_passes": [e["guardian_passed"] for e in bridge_evaluations],
                    "user_usefulness": [e["user_usefulness"] for e in bridge_evaluations],
                }

                results["task_results"].append(task_result)
                results["creative_bridges"].extend(bridges)

            except Exception as e:
                logger.exception(f"Error evaluating task {task.task_id}: {e}")
                continue

        # Calculate summary metrics
        results["summary_metrics"] = self._calculate_summary_metrics(results["task_results"])

        return results

    async def evaluate_cross_domain_pairs(self) -> dict[str, Any]:
        """Evaluate creativity on cross-domain concept pairs."""
        logger.info("Evaluating Cross-Domain Concept Pairs...")

        results = {"pair_results": [], "summary_metrics": {}, "creative_bridges": []}

        for pair in self.dataset_generator.cross_domain_pairs:
            try:
                # Generate creative bridges between domains
                bridges = await self._generate_cross_domain_bridges(pair)

                # Evaluate each bridge
                bridge_evaluations = []
                for bridge in bridges:
                    evaluation = await self._evaluate_creative_bridge(bridge, pair)
                    bridge_evaluations.append(evaluation)

                # Calculate pair-level metrics
                pair_result = {
                    "pair_id": pair.pair_id,
                    "source_concept": pair.source_concept,
                    "source_domain": pair.source_domain,
                    "target_concept": pair.target_concept,
                    "target_domain": pair.target_domain,
                    "expected_connections": pair.expected_connections,
                    "bridges_generated": len(bridges),
                    "bridges_evaluated": bridge_evaluations,
                    "surprise_scores": [e["surprise_score"] for e in bridge_evaluations],
                    "guardian_passes": [e["guardian_passed"] for e in bridge_evaluations],
                    "precision_score": self._calculate_precision_score(bridge_evaluations, pair.expected_connections),
                }

                results["pair_results"].append(pair_result)
                results["creative_bridges"].extend(bridges)

            except Exception as e:
                logger.exception(f"Error evaluating pair {pair.pair_id}: {e}")
                continue

        # Calculate summary metrics
        results["summary_metrics"] = self._calculate_summary_metrics(results["pair_results"], is_cross_domain=True)

        return results

    async def _generate_creative_bridges_for_task(self, task: RemoteAssociationTask) -> list[CreativeBridge]:
        """Generate creative bridges for a remote association task."""
        bridges = []

        # Mock creative bridge generation - in reality this would use HypeRAG's creative system
        for i, cue_word in enumerate(task.cue_words):
            bridge = CreativeBridge(
                id=f"{task.task_id}_bridge_{i}",
                confidence=np.random.uniform(0.4, 0.9),
                bridge_type="semantic_association",
                source_nodes=[cue_word],
                target_nodes=[task.target_word],
            )
            bridges.append(bridge)

        # Generate additional creative connections
        for i in range(2):  # Generate 2 additional creative bridges
            bridge = CreativeBridge(
                id=f"{task.task_id}_creative_{i}",
                confidence=np.random.uniform(0.3, 0.8),
                bridge_type="creative_leap",
                source_nodes=task.cue_words,
                target_nodes=[task.target_word, f"creative_concept_{i}"],
            )
            bridges.append(bridge)

        return bridges

    async def _generate_cross_domain_bridges(self, pair: CrossDomainPair) -> list[CreativeBridge]:
        """Generate creative bridges between cross-domain concepts."""
        bridges = []

        # Generate bridges based on expected connections
        for i, connection in enumerate(pair.expected_connections[:3]):  # Top 3 expected
            bridge = CreativeBridge(
                id=f"{pair.pair_id}_bridge_{i}",
                confidence=np.random.uniform(0.5, 0.9),
                bridge_type="cross_domain_analogy",
                source_nodes=[pair.source_concept],
                target_nodes=[pair.target_concept, connection],
            )
            bridges.append(bridge)

        # Generate novel creative bridges
        novel_connections = [
            "emergent_property",
            "systematic_parallel",
            "functional_analogy",
        ]
        for i, novel_conn in enumerate(novel_connections):
            bridge = CreativeBridge(
                id=f"{pair.pair_id}_novel_{i}",
                confidence=np.random.uniform(0.2, 0.7),
                bridge_type="novel_discovery",
                source_nodes=[pair.source_concept, pair.source_domain],
                target_nodes=[pair.target_concept, pair.target_domain, novel_conn],
            )
            bridges.append(bridge)

        return bridges

    async def _evaluate_creative_bridge(self, bridge: CreativeBridge, context: Any) -> dict[str, Any]:
        """Evaluate a single creative bridge."""
        evaluation = {
            "bridge_id": bridge.id,
            "surprise_score": 0.0,
            "guardian_passed": False,
            "user_usefulness": 0.0,
            "path_rarity": 0.0,
            "guardian_confidence": 0.0,
        }

        try:
            # 1. Calculate surprise score (path rarity)
            evaluation["surprise_score"] = self._calculate_surprise_score(bridge)
            evaluation["path_rarity"] = evaluation["surprise_score"]  # Same metric

            # 2. Guardian validation
            guardian_result = await self.guardian_gate.evaluate_creative(bridge)
            evaluation["guardian_passed"] = guardian_result == "APPLY"
            evaluation["guardian_confidence"] = bridge.confidence

            # 3. User usefulness score (from loaded JSON)
            evaluation["user_usefulness"] = self.user_scores.get(bridge.id, 0.0)

        except Exception as e:
            logger.exception(f"Error evaluating bridge {bridge.id}: {e}")

        return evaluation

    def _calculate_surprise_score(self, bridge: CreativeBridge) -> float:
        """Calculate surprise score based on path rarity."""
        # Mock implementation - in reality this would analyze actual graph paths

        # Factors that increase surprise:
        # - Distance between source and target concepts
        # - Rarity of connection type
        # - Cross-domain nature
        # - Novelty of intermediate concepts

        base_surprise = 0.5

        # Bridge type factor
        type_multipliers = {
            "semantic_association": 1.0,
            "creative_leap": 1.5,
            "cross_domain_analogy": 2.0,
            "novel_discovery": 2.5,
        }
        type_factor = type_multipliers.get(bridge.bridge_type, 1.0)

        # Confidence factor (lower confidence can indicate more surprising connections)
        confidence_factor = 1.2 - bridge.confidence  # Inverse relationship

        # Number of nodes factor (more complex paths are more surprising)
        node_count = len(bridge.source_nodes) + len(bridge.target_nodes)
        complexity_factor = 1.0 + (node_count - 2) * 0.1

        surprise_score = base_surprise * type_factor * confidence_factor * complexity_factor

        # Add some randomness to simulate real-world variability
        surprise_score += np.random.normal(0, 0.1)

        return min(max(surprise_score, 0.0), 1.0)  # Clamp to [0, 1]

    def _calculate_precision_score(self, evaluations: list[dict], expected_connections: list[str]) -> float:
        """Calculate precision of discovered connections against ground truth."""
        if not evaluations or not expected_connections:
            return 0.0

        # Extract discovered connections from bridge evaluations
        discovered_connections = set()
        for eval_result in evaluations:
            # In a real implementation, this would extract actual discovered connections
            # For now, simulate based on evaluation scores
            if eval_result["guardian_passed"] and eval_result["surprise_score"] > 0.5:
                discovered_connections.add(f"connection_{eval_result['bridge_id']}")

        # Calculate precision against expected connections
        expected_set = set(expected_connections)

        if not discovered_connections:
            return 0.0

        # Mock precision calculation
        # In reality, this would use semantic similarity to match discovered vs expected
        overlap = len(discovered_connections.intersection(expected_set))
        precision = overlap / len(discovered_connections) if discovered_connections else 0.0

        # Add some realistic variation
        precision = min(max(precision + np.random.normal(0, 0.1), 0.0), 1.0)

        return precision

    def _calculate_summary_metrics(self, results: list[dict], is_cross_domain: bool = False) -> dict[str, float]:
        """Calculate summary metrics across all tasks/pairs."""
        if not results:
            return {}

        all_surprise_scores = []
        all_guardian_passes = []
        all_user_scores = []
        all_precision_scores = []

        for result in results:
            all_surprise_scores.extend(result.get("surprise_scores", []))
            all_guardian_passes.extend(result.get("guardian_passes", []))
            all_user_scores.extend(result.get("user_usefulness", []))

            if is_cross_domain and "precision_score" in result:
                all_precision_scores.append(result["precision_score"])

        # Calculate Surprise@5 (top 5 surprise scores)
        top_5_surprise = sorted(all_surprise_scores, reverse=True)[:5]
        surprise_at_5 = statistics.mean(top_5_surprise) if top_5_surprise else 0.0

        # Guardian Pass Rate
        guardian_pass_rate = sum(all_guardian_passes) / len(all_guardian_passes) if all_guardian_passes else 0.0

        # User Usefulness Score
        user_usefulness_avg = statistics.mean([s for s in all_user_scores if s > 0]) if all_user_scores else 0.0

        # Hidden-Link Precision (for cross-domain tasks)
        hidden_link_precision = statistics.mean(all_precision_scores) if all_precision_scores else 0.0

        return {
            "surprise_at_5": surprise_at_5,
            "guardian_pass_rate": guardian_pass_rate,
            "user_usefulness_score": user_usefulness_avg,
            "hidden_link_precision": hidden_link_precision,
            "total_bridges_evaluated": len(all_surprise_scores),
            "mean_surprise_score": (statistics.mean(all_surprise_scores) if all_surprise_scores else 0.0),
            "std_surprise_score": (statistics.stdev(all_surprise_scores) if len(all_surprise_scores) > 1 else 0.0),
        }

    async def run_full_benchmark(self, user_scores_path: Path | None = None) -> CreativityMetrics:
        """Run the complete creativity benchmark suite."""
        logger.info("Starting HypeRAG Creativity Benchmark Suite...")

        # Load user evaluation scores if provided
        if user_scores_path:
            self.load_user_evaluation_scores(user_scores_path)

        # Run Remote Association Challenge
        rat_results = await self.evaluate_remote_association_tasks()

        # Run Cross-Domain Concept Pairs
        cd_results = await self.evaluate_cross_domain_pairs()

        # Combine results
        combined_metrics = self._combine_results(rat_results, cd_results)

        # Create final metrics object
        creativity_metrics = CreativityMetrics(
            surprise_at_5=combined_metrics["surprise_at_5"],
            guardian_pass_rate=combined_metrics["guardian_pass_rate"],
            user_usefulness_score=combined_metrics["user_usefulness_score"],
            hidden_link_precision=combined_metrics["hidden_link_precision"],
            total_bridges_evaluated=combined_metrics["total_bridges_evaluated"],
            dataset_name="remote_association_and_cross_domain",
            timestamp=datetime.now(UTC).isoformat(),
        )

        # Save detailed results
        await self._save_results(rat_results, cd_results, creativity_metrics)

        logger.info("Creativity benchmark completed successfully")
        return creativity_metrics

    def _combine_results(self, rat_results: dict, cd_results: dict) -> dict[str, float]:
        """Combine results from both evaluation types."""
        rat_metrics = rat_results["summary_metrics"]
        cd_metrics = cd_results["summary_metrics"]

        # Weight the metrics (can be adjusted based on importance)
        rat_weight = 0.6
        cd_weight = 0.4

        combined = {
            "surprise_at_5": (
                rat_metrics.get("surprise_at_5", 0) * rat_weight + cd_metrics.get("surprise_at_5", 0) * cd_weight
            ),
            "guardian_pass_rate": (
                rat_metrics.get("guardian_pass_rate", 0) * rat_weight
                + cd_metrics.get("guardian_pass_rate", 0) * cd_weight
            ),
            "user_usefulness_score": (
                rat_metrics.get("user_usefulness_score", 0) * rat_weight
                + cd_metrics.get("user_usefulness_score", 0) * cd_weight
            ),
            "hidden_link_precision": cd_metrics.get("hidden_link_precision", 0),  # Only from cross-domain
            "total_bridges_evaluated": (
                rat_metrics.get("total_bridges_evaluated", 0) + cd_metrics.get("total_bridges_evaluated", 0)
            ),
        }

        return combined

    async def _save_results(self, rat_results: dict, cd_results: dict, metrics: CreativityMetrics) -> None:
        """Save benchmark results to files."""
        timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")

        # Save detailed results
        results_file = self.output_dir / f"creativity_results_{timestamp}.json"
        detailed_results = {
            "metadata": {
                "benchmark_version": "1.0",
                "timestamp": metrics.timestamp,
                "total_bridges_evaluated": metrics.total_bridges_evaluated,
            },
            "summary_metrics": asdict(metrics),
            "remote_association_results": rat_results,
            "cross_domain_results": cd_results,
        }

        with open(results_file, "w") as f:
            json.dump(detailed_results, f, indent=2)

        # Save metrics summary
        metrics_file = self.output_dir / f"creativity_metrics_{timestamp}.json"
        with open(metrics_file, "w") as f:
            json.dump(asdict(metrics), f, indent=2)

        logger.info(f"Results saved to {results_file}")
        logger.info(f"Metrics saved to {metrics_file}")


def create_sample_user_scores():
    """Create a sample user evaluation scores file."""
    sample_scores = {}

    # Generate sample scores for common bridge IDs
    for task_id in [f"rat_{i:03d}" for i in range(1, 16)]:
        for bridge_idx in range(5):  # 5 bridges per task
            bridge_id = f"{task_id}_bridge_{bridge_idx}"
            sample_scores[bridge_id] = np.random.uniform(0.0, 1.0)

            if bridge_idx < 2:  # Creative bridges
                creative_bridge_id = f"{task_id}_creative_{bridge_idx}"
                sample_scores[creative_bridge_id] = np.random.uniform(0.0, 1.0)

    # Cross-domain pairs
    for pair_id in [f"cd_{i:03d}" for i in range(1, 13)]:
        for bridge_idx in range(6):  # 6 bridges per pair
            if bridge_idx < 3:
                bridge_id = f"{pair_id}_bridge_{bridge_idx}"
            else:
                bridge_id = f"{pair_id}_novel_{bridge_idx - 3}"
            sample_scores[bridge_id] = np.random.uniform(0.0, 1.0)

    return sample_scores


async def main() -> None:
    parser = argparse.ArgumentParser(description="HypeRAG Creativity Benchmark Suite")
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("./benchmark_results"),
        help="Output directory for results",
    )
    parser.add_argument("--user-scores", type=Path, help="Path to user evaluation scores JSON file")
    parser.add_argument(
        "--generate-sample-scores",
        action="store_true",
        help="Generate sample user scores file",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # Generate sample scores if requested
    if args.generate_sample_scores:
        sample_scores = create_sample_user_scores()
        scores_file = args.output_dir / "sample_user_scores.json"
        args.output_dir.mkdir(parents=True, exist_ok=True)

        with open(scores_file, "w") as f:
            json.dump(sample_scores, f, indent=2)

        print(f"Generated sample user scores: {scores_file}")
        return

    # Run benchmark
    benchmark = CreativityBenchmark(output_dir=args.output_dir)

    try:
        metrics = await benchmark.run_full_benchmark(user_scores_path=args.user_scores)

        # Print results
        print("\n" + "=" * 60)
        print("HypeRAG CREATIVITY BENCHMARK RESULTS")
        print("=" * 60)
        print(f"Surprise@5:             {metrics.surprise_at_5:.3f}")
        print(f"Guardian Pass Rate:     {metrics.guardian_pass_rate:.1%}")
        print(f"User Usefulness Score:  {metrics.user_usefulness_score:.3f}")
        print(f"Hidden-Link Precision:  {metrics.hidden_link_precision:.3f}")
        print(f"Total Bridges Evaluated: {metrics.total_bridges_evaluated}")
        print(f"Timestamp:              {metrics.timestamp}")
        print("=" * 60)

    except Exception as e:
        logger.exception(f"Benchmark failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
