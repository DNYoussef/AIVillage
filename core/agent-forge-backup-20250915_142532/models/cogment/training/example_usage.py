"""
Example Usage of Cogment Training System.

Demonstrates how to use the complete training system with GrokFast integration,
4-stage curriculum, and multi-optimizer setup.
"""

import torch
from torch.utils.data import DataLoader, TensorDataset

# Import Cogment components
from ..core.config import CogmentConfig
from ..core.model import Cogment
from .curriculum import CurriculumStage, FourStageCurriculum
from .evaluator import StageEvaluator
from .grokfast_integration import GrokFastConfig
from .losses import CogmentLoss

# Import training system
from .trainer import CogmentTrainer, MultiOptimizerConfig, TrainingConfig


def create_example_training_setup():
    """
    Create a complete training setup for Cogment.

    This demonstrates the integration between all training components
    and how to use them with the existing Cogment model.
    """

    # 1. Model Configuration
    model_config = CogmentConfig(
        vocab_size=32000,
        d_model=320,  # Matches Agent 1 config
        n_layers=6,
        n_head=8,
        max_seq_len=2048,
        # ACT parameters
        max_refinement_steps=8,
        min_refinement_steps=1,
        act_threshold=0.99,
        halt_epsilon=0.01,
        ponder_cost_weight=0.01,
        # LTM parameters (matches Agent 2)
        ltm_capacity=1024,  # Memory slots
        ltm_dim=512,  # Memory dimension
        ltm_enabled=True,
        # Other parameters
        dropout=0.1,
        layer_norm_eps=1e-5,
    )

    # 2. GrokFast Configuration
    grokfast_config = GrokFastConfig(
        # Aggressive for core and memory in stages 1-2
        core_alpha=0.98,
        core_lamb=2.0,
        memory_alpha=0.95,
        memory_lamb=1.5,
        # Disabled for ACT halting
        halting_enabled=False,
        # Stage-specific scheduling
        stage_1_2_enabled=True,
        stage_3_4_reduction=0.6,
        # Monitoring
        grokking_detection_threshold=0.7,
        monitoring_interval=50,
    )

    # 3. Multi-Optimizer Configuration
    optimizer_config = MultiOptimizerConfig(
        # Core refinement - higher learning rate for grokking
        core_lr=3e-4,
        core_weight_decay=0.01,
        # Memory - conservative for stability
        memory_lr=1e-4,
        memory_weight_decay=0.001,
        # ACT halting - moderate
        halting_lr=5e-4,
        halting_weight_decay=0.01,
        # Scheduler
        scheduler_type="cosine",
        warmup_steps=1000,
        min_lr_ratio=0.1,
    )

    # 4. Training Configuration
    training_config = TrainingConfig(
        model_config=model_config,
        optimizer_config=optimizer_config,
        grokfast_config=grokfast_config,
        # Training parameters
        max_epochs=20,
        gradient_clip_norm=1.0,
        accumulation_steps=1,
        # Evaluation
        eval_interval=500,
        save_interval=2000,
        log_interval=100,
        # Early stopping
        early_stopping_patience=5000,
        min_delta=1e-4,
        # Mixed precision
        use_amp=True,
        # Curriculum
        use_curriculum=True,
        auto_advance_stages=True,
        # Memory management
        ltm_decay_interval=100,
        ltm_consolidation_interval=5000,
    )

    return model_config, training_config


def create_mock_datasets():
    """Create mock datasets for each curriculum stage."""

    datasets = {}

    # Stage 0: Sanity Checks - Simple sequences
    vocab_size = 1000
    seq_len = 128
    batch_size = 16

    # Generate simple synthetic data
    sanity_data = torch.randint(1, vocab_size, (batch_size * 10, seq_len))
    sanity_labels = sanity_data.clone()  # Simple language modeling
    datasets["sanity"] = DataLoader(TensorDataset(sanity_data, sanity_labels), batch_size=batch_size, shuffle=True)

    # Stage 1: ARC Visual - Pattern sequences (simulated)
    arc_data = torch.randint(1, vocab_size, (batch_size * 20, 256))
    arc_labels = arc_data.clone()
    datasets["arc_visual"] = DataLoader(TensorDataset(arc_data, arc_labels), batch_size=8, shuffle=True)

    # Stage 2: Algorithmic - Structured sequences
    algo_data = torch.randint(1, vocab_size, (batch_size * 30, 512))
    algo_labels = algo_data.clone()
    datasets["algorithmic"] = DataLoader(TensorDataset(algo_data, algo_labels), batch_size=6, shuffle=True)

    # Stage 3: Math & Text - Complex sequences
    math_data = torch.randint(1, vocab_size, (batch_size * 40, 1024))
    math_labels = math_data.clone()
    datasets["math_text"] = DataLoader(TensorDataset(math_data, math_labels), batch_size=4, shuffle=True)

    # Stage 4: Long Context - Very long sequences
    long_data = torch.randint(1, vocab_size, (batch_size * 10, 2048))
    long_labels = long_data.clone()
    datasets["long_context"] = DataLoader(TensorDataset(long_data, long_labels), batch_size=2, shuffle=True)

    return datasets


def run_training_example():
    """
    Complete example of training Cogment with the new training system.
    """
    print("=" * 80)
    print("COGMENT TRAINING SYSTEM EXAMPLE")
    print("=" * 80)

    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create configurations
    model_config, training_config = create_example_training_setup()

    # Create model
    print("\n1. Creating Cogment model...")
    model = Cogment(model_config)
    print(f"Model parameters: {model.count_parameters():,}")
    print(f"Parameter breakdown: {model.parameter_breakdown()}")

    # Create trainer
    print("\n2. Initializing trainer...")
    trainer = CogmentTrainer(model=model, config=training_config, device=device)

    # Create datasets
    print("\n3. Creating mock datasets...")
    datasets = create_mock_datasets()
    print(f"Created datasets for {len(datasets)} stages")

    # Demonstrate curriculum
    print("\n4. Curriculum Overview:")
    curriculum = FourStageCurriculum()
    for stage in CurriculumStage:
        config = curriculum.get_stage_config(stage)
        print(f"  Stage {stage.value}: {config.name}")
        print(f"    - Max steps: {config.max_steps}")
        print(f"    - Batch size: {config.batch_size}")
        print(f"    - GrokFast: {config.grokfast_enabled}")
        print(f"    - Refinement steps: {config.max_refinement_steps}")

    # Show stage progression
    progression = curriculum.get_stage_progression()
    print("\n5. Stage Progression:")
    for stage, status, completed in progression:
        print(f"  {stage.name}: {status}")

    # Demonstrate training (mock - not actually training)
    print("\n6. Training System Components:")

    # GrokFast integration
    grokfast_summary = trainer.grokfast_manager.get_grokking_summary()
    print(f"  GrokFast Manager: {grokfast_summary['total_components']} components")

    # Loss function
    CogmentLoss()
    print("  Loss Function: 5 specialized loss components")

    # Evaluator
    evaluator = StageEvaluator()
    print(f"  Stage Evaluator: {len(evaluator.stage_thresholds)} stage configurations")

    # Simulate a training step to show integration
    print("\n7. Simulating Training Step:")

    # Get data for current stage
    stage_config = curriculum.get_current_config()
    print(f"  Current stage: {stage_config.name}")

    # Create sample batch
    sample_batch = {"input_ids": torch.randint(1, 1000, (2, 128)), "labels": torch.randint(1, 1000, (2, 128))}

    # Show what would happen in training step
    print(f"  Batch shape: {sample_batch['input_ids'].shape}")
    print(f"  Max refinement steps: {stage_config.max_refinement_steps}")
    print(f"  GrokFast enabled: {stage_config.grokfast_enabled}")
    print(f"  Learning rate: {stage_config.learning_rate}")

    # Show optimizer components
    print("\n8. Multi-Optimizer Setup:")
    for name, optimizer in trainer.optimizers.items():
        param_count = sum(p.numel() for group in optimizer.param_groups for p in group["params"])
        lr = optimizer.param_groups[0]["lr"]
        print(f"  {name}: {param_count:,} parameters, lr={lr}")

    # Show memory management
    print("\n9. Memory Management:")
    if hasattr(model, "gated_ltm"):
        memory_stats = model.gated_ltm.get_memory_stats()
        print(f"  Memory slots: {memory_stats['total_slots']}")
        print(f"  Decay interval: {training_config.ltm_decay_interval}")
        print(f"  Consolidation interval: {training_config.ltm_consolidation_interval}")

    print("\n" + "=" * 80)
    print("TRAINING SYSTEM READY!")
    print("=" * 80)
    print(f"‚úÖ 4-stage curriculum with {len(CurriculumStage)} stages")
    print("‚úÖ GrokFast integration with selective application")
    print(f"‚úÖ Multi-optimizer setup for {len(trainer.optimizers)} components")
    print("‚úÖ Deep supervision + 4 specialized loss functions")
    print("‚úÖ Stage-specific evaluation and convergence detection")
    print("‚úÖ Automatic memory management and consolidation")
    print("‚úÖ Mixed precision training support")
    print("‚úÖ Comprehensive checkpointing and resumption")

    # Show integration points for other agents
    print("\nüîó INTEGRATION POINTS FOR OTHER AGENTS:")
    print("üìä Agent 5 (Data): Use datasets dict for stage-specific data loading")
    print("‚öôÔ∏è  Agent 7 (Config): Override TrainingConfig and GrokFastConfig")
    print("üß™ Agent 8 (Tests): Use StageEvaluator for comprehensive testing")

    return trainer, datasets, curriculum


if __name__ == "__main__":
    # Run the example
    trainer, datasets, curriculum = run_training_example()

    # Show how to start actual training
    print("\nüìù TO START ACTUAL TRAINING:")
    print("```python")
    print("# Select training and eval datasets")
    print("train_loader = datasets['sanity']  # Start with sanity checks")
    print("eval_loader = datasets['sanity']   # Use same for validation")
    print("")
    print("# Run training")
    print("results = trainer.train(train_loader, eval_loader)")
    print("```")
