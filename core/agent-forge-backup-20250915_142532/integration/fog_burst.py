"""
Agent Forge Fog Burst

Offloads compute-intensive Agent Forge phases to fog computing network.
Enables distributed execution of EvoMerge, Quiet-STaR, Training, and ADAS phases
while maintaining parity guarantees and artifact collection.

Key Components:
- FogBurstOrchestrator: Manages distributed training across fog nodes
- EvoMergeFogRunner: Distributes EvoMerge model evolution
- QuietSTaRFogRunner: Distributes prompt baking iterations
- ADASphaseFogRunner: Distributes architecture search
- ParityValidator: Ensures remote results match local execution within Îµ

This enables large-scale distributed training while preserving the Agent Forge
7-phase pipeline architecture and maintaining strict result consistency.
"""

import asyncio
from dataclasses import asdict, dataclass, field
from datetime import UTC, datetime
from enum import Enum
import json
import logging
from typing import Any
from uuid import uuid4

import aiohttp

# Import from existing Agent Forge infrastructure
from packages.agent_forge.core.phase_controller import PhaseController, PhaseResult
from packages.agent_forge.core.unified_pipeline import UnifiedConfig

logger = logging.getLogger(__name__)


def _serialize_dataclass(obj: Any) -> dict[str, Any]:
    """Convert a dataclass instance to a JSON-serializable dictionary."""
    return json.loads(json.dumps(asdict(obj), default=str))


class FogBurstStrategy(str, Enum):
    """Strategy for distributing Agent Forge phases"""

    LOCAL_ONLY = "local_only"  # Execute locally only
    FOG_PREFERRED = "fog_preferred"  # Use fog when available
    HYBRID = "hybrid"  # Mix local and fog execution
    FOG_REQUIRED = "fog_required"  # Must use fog (fail if unavailable)


class PhaseDistributionMode(str, Enum):
    """How to distribute a phase across fog nodes"""

    SINGLE_NODE = "single_node"  # Run entire phase on one node
    PARALLEL_SPLIT = "parallel_split"  # Split phase work across multiple nodes
    REDUNDANT = "redundant"  # Run on multiple nodes for reliability
    PIPELINE = "pipeline"  # Pipeline stages across nodes


@dataclass
class FogNodeCapabilities:
    """Fog node capabilities for Agent Forge operations"""

    node_id: str
    cpu_cores: float
    memory_gb: float
    gpu_available: bool
    gpu_memory_gb: float = 0.0

    # Model training capabilities
    supports_pytorch: bool = False
    supports_tensorflow: bool = False
    supports_jax: bool = False

    # Specialized capabilities
    supports_evomerge: bool = False
    supports_training: bool = False
    supports_adas: bool = False

    # Performance metrics
    training_throughput: float = 0.0  # samples/second
    network_bandwidth_mbps: float = 0.0
    storage_available_gb: float = 0.0


@dataclass
class FogBurstTask:
    """Distributed Agent Forge task for fog execution"""

    task_id: str
    phase_name: str
    task_type: str
    config: dict[str, Any]

    # Distribution settings
    target_nodes: list[str] = field(default_factory=list)
    distribution_mode: PhaseDistributionMode = PhaseDistributionMode.SINGLE_NODE
    strategy: FogBurstStrategy = FogBurstStrategy.FOG_PREFERRED

    # Execution tracking
    status: str = "pending"
    fog_job_ids: list[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    started_at: datetime | None = None
    completed_at: datetime | None = None

    # Results and artifacts
    results: dict[str, Any] = field(default_factory=dict)
    artifacts: dict[str, Any] = field(default_factory=dict)
    local_comparison: dict[str, Any] | None = None
    parity_validation: dict[str, Any] | None = None

    # Error tracking
    errors: list[str] = field(default_factory=list)
    retry_count: int = 0
    max_retries: int = 2


class ParityValidator:
    """
    Validates that fog execution results match local execution within tolerance

    Critical for ensuring distributed training maintains consistency with
    local execution while allowing for acceptable numerical differences.
    """

    def __init__(self, tolerance: float = 1e-4):
        self.tolerance = tolerance
        self.validation_history: list[dict[str, Any]] = []

    async def validate_parity(
        self, local_result: PhaseResult, fog_result: dict[str, Any], phase_name: str, task_id: str
    ) -> dict[str, Any]:
        """
        Validate parity between local and fog execution results

        Args:
            local_result: Local execution result
            fog_result: Fog execution result
            phase_name: Name of the Agent Forge phase
            task_id: Task identifier

        Returns:
            Validation result with parity status and metrics
        """

        validation = {
            "task_id": task_id,
            "phase_name": phase_name,
            "timestamp": datetime.now(UTC).isoformat(),
            "parity_passed": False,
            "tolerance": self.tolerance,
            "differences": {},
            "metrics": {},
        }

        try:
            # Extract comparable metrics from both results
            local_metrics = self._extract_comparable_metrics(local_result.metrics, phase_name)
            fog_metrics = self._extract_comparable_metrics(fog_result.get("metrics", {}), phase_name)

            # Compare key metrics
            max_difference = 0.0
            metric_comparisons = {}

            for metric_name in local_metrics:
                if metric_name in fog_metrics:
                    local_val = local_metrics[metric_name]
                    fog_val = fog_metrics[metric_name]

                    if isinstance(local_val, int | float) and isinstance(fog_val, int | float):
                        # Numerical comparison
                        if local_val != 0:
                            rel_diff = abs((fog_val - local_val) / local_val)
                        else:
                            rel_diff = abs(fog_val - local_val)

                        metric_comparisons[metric_name] = {
                            "local": local_val,
                            "fog": fog_val,
                            "absolute_diff": abs(fog_val - local_val),
                            "relative_diff": rel_diff,
                            "within_tolerance": rel_diff <= self.tolerance,
                        }

                        max_difference = max(max_difference, rel_diff)

            # Model artifact comparison for specific phases
            if phase_name == "evomerge":
                model_parity = await self._validate_model_parity(local_result, fog_result)
                validation["model_parity"] = model_parity
                max_difference = max(max_difference, model_parity.get("max_param_diff", 0.0))

            validation["differences"] = metric_comparisons
            validation["max_difference"] = max_difference
            validation["parity_passed"] = max_difference <= self.tolerance

            # Phase-specific validation
            phase_validation = await self._validate_phase_specific(local_result, fog_result, phase_name)
            validation["phase_specific"] = phase_validation

            # Overall assessment
            validation["assessment"] = self._assess_parity_quality(validation)

            self.validation_history.append(validation)

            logger.info(
                f"Parity validation for {phase_name} task {task_id}: "
                f"{'PASSED' if validation['parity_passed'] else 'FAILED'} "
                f"(max_diff: {max_difference:.6f})"
            )

            return validation

        except Exception as e:
            validation["error"] = str(e)
            validation["parity_passed"] = False
            logger.error(f"Parity validation failed for {phase_name} task {task_id}: {e}")
            return validation

    def _extract_comparable_metrics(self, metrics: dict[str, Any], phase_name: str) -> dict[str, int | float]:
        """Extract numerical metrics that can be compared between local and fog execution"""

        comparable = {}

        # Common metrics across all phases
        if "loss" in metrics:
            comparable["loss"] = metrics["loss"]
        if "accuracy" in metrics:
            comparable["accuracy"] = metrics["accuracy"]
        if "training_time_s" in metrics:
            comparable["training_time_s"] = metrics["training_time_s"]

        # Phase-specific metrics
        if phase_name == "evomerge":
            # EvoMerge specific metrics
            for key in ["merge_score", "diversity_score", "performance_delta", "final_loss"]:
                if key in metrics:
                    comparable[key] = metrics[key]

        elif phase_name == "quietstar":
            # Quiet-STaR specific metrics
            for key in ["convergence_score", "thought_coherence", "baking_iterations", "final_perplexity"]:
                if key in metrics:
                    comparable[key] = metrics[key]

        elif phase_name == "adas":
            # ADAS specific metrics
            for key in ["architecture_score", "search_efficiency", "pareto_front_size", "best_config_score"]:
                if key in metrics:
                    comparable[key] = metrics[key]

        return comparable

    async def _validate_model_parity(self, local_result: PhaseResult, fog_result: dict[str, Any]) -> dict[str, Any]:
        """Validate model parameters match between local and fog execution"""

        model_validation = {
            "parameters_compared": 0,
            "max_param_diff": 0.0,
            "avg_param_diff": 0.0,
            "params_within_tolerance": 0,
        }

        try:
            # This would compare actual model parameters in production
            # For now, simulate based on metrics
            local_loss = local_result.metrics.get("final_loss", 0.0)
            fog_loss = fog_result.get("metrics", {}).get("final_loss", 0.0)

            if local_loss != 0:
                param_diff = abs((fog_loss - local_loss) / local_loss)
            else:
                param_diff = abs(fog_loss - local_loss)

            model_validation["max_param_diff"] = param_diff
            model_validation["avg_param_diff"] = param_diff
            model_validation["parameters_compared"] = 1
            model_validation["params_within_tolerance"] = 1 if param_diff <= self.tolerance else 0

        except Exception as e:
            logger.warning(f"Model parity validation failed: {e}")

        return model_validation

    async def _validate_phase_specific(
        self, local_result: PhaseResult, fog_result: dict[str, Any], phase_name: str
    ) -> dict[str, Any]:
        """Validate phase-specific requirements"""

        phase_validation = {"phase": phase_name, "specific_checks": {}}

        if phase_name == "evomerge":
            # Validate merge strategy consistency
            local_strategy = local_result.metadata.get("merge_strategy", "unknown")
            fog_strategy = fog_result.get("metadata", {}).get("merge_strategy", "unknown")
            phase_validation["specific_checks"]["merge_strategy_match"] = local_strategy == fog_strategy

        elif phase_name == "quietstar":
            # Validate convergence behavior
            local_converged = local_result.metadata.get("converged", False)
            fog_converged = fog_result.get("metadata", {}).get("converged", False)
            phase_validation["specific_checks"]["convergence_match"] = local_converged == fog_converged

        elif phase_name == "adas":
            # Validate architecture search results
            local_best = local_result.metadata.get("best_architecture", {})
            fog_best = fog_result.get("metadata", {}).get("best_architecture", {})
            phase_validation["specific_checks"]["architecture_similarity"] = self._compare_architectures(
                local_best, fog_best
            )

        return phase_validation

    def _compare_architectures(self, arch1: dict[str, Any], arch2: dict[str, Any]) -> float:
        """Compare two architecture configurations and return similarity score"""

        if not arch1 or not arch2:
            return 0.0

        # Simple similarity based on common keys
        common_keys = set(arch1.keys()) & set(arch2.keys())
        if not common_keys:
            return 0.0

        matches = sum(1 for key in common_keys if arch1[key] == arch2[key])
        return matches / len(common_keys)

    def _assess_parity_quality(self, validation: dict[str, Any]) -> str:
        """Assess overall quality of parity validation"""

        if not validation["parity_passed"]:
            return "FAILED"

        max_diff = validation["max_difference"]

        if max_diff <= self.tolerance * 0.1:
            return "EXCELLENT"
        elif max_diff <= self.tolerance * 0.5:
            return "GOOD"
        elif max_diff <= self.tolerance:
            return "ACCEPTABLE"
        else:
            return "FAILED"


class FogBurstOrchestrator:
    """
    Orchestrates distributed Agent Forge execution across fog nodes

    Manages phase distribution, node selection, artifact collection,
    and parity validation for compute-intensive training operations.
    """

    def __init__(
        self,
        fog_gateway_url: str = "http://localhost:8080",
        default_strategy: FogBurstStrategy = FogBurstStrategy.FOG_PREFERRED,
        parity_tolerance: float = 1e-4,
    ):
        self.fog_gateway_url = fog_gateway_url.rstrip("/")
        self.default_strategy = default_strategy

        # Node management
        self.available_nodes: dict[str, FogNodeCapabilities] = {}
        self.active_tasks: dict[str, FogBurstTask] = {}

        # Validation
        self.parity_validator = ParityValidator(parity_tolerance)

        # Performance tracking
        self.success_rate = 0.95
        self.avg_speedup = 1.0
        self.cost_efficiency = 1.0

        # Background tasks
        self._node_discovery_task: asyncio.Task | None = None

    async def initialize(self) -> bool:
        """Initialize fog burst orchestrator"""

        try:
            # Discover available fog nodes
            await self._discover_fog_nodes()

            # Start background node monitoring
            self._node_discovery_task = asyncio.create_task(self._monitor_fog_nodes())

            logger.info(f"FogBurstOrchestrator initialized with {len(self.available_nodes)} nodes")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize FogBurstOrchestrator: {e}")
            return False

    async def execute_phase_distributed(
        self,
        phase: PhaseController,
        config: UnifiedConfig,
        strategy: FogBurstStrategy | None = None,
        enable_parity_validation: bool = True,
    ) -> PhaseResult:
        """
        Execute Agent Forge phase with distributed fog computation

        Args:
            phase: Agent Forge phase to execute
            config: Unified configuration
            strategy: Distribution strategy
            enable_parity_validation: Whether to validate against local execution

        Returns:
            PhaseResult with distributed execution results
        """

        strategy = strategy or self.default_strategy
        task_id = str(uuid4())
        phase_name = phase.__class__.__name__.replace("Phase", "").lower()

        logger.info(f"Starting distributed execution of {phase_name} (task: {task_id})")

        try:
            # Create fog burst task
            config_dict = _serialize_dataclass(config)
            task = FogBurstTask(
                task_id=task_id,
                phase_name=phase_name,
                task_type="phase_execution",
                config=config_dict,
                strategy=strategy,
            )

            self.active_tasks[task_id] = task

            # Determine execution plan
            execution_plan = await self._plan_phase_execution(phase, config, strategy)

            if not execution_plan["use_fog"] or strategy == FogBurstStrategy.LOCAL_ONLY:
                # Execute locally
                logger.info(f"Executing {phase_name} locally (no suitable fog nodes)")
                return await phase.execute(config)

            # Execute on fog nodes
            task.status = "running"
            task.started_at = datetime.now(UTC)

            fog_result = await self._execute_on_fog(task, phase, config, execution_plan)

            # Parity validation if enabled
            if enable_parity_validation and fog_result["status"] == "success":
                parity_result = await self._validate_with_local_execution(task, phase, config, fog_result)
                task.parity_validation = parity_result

                if not parity_result["parity_passed"]:
                    logger.warning(f"Parity validation failed for {phase_name} - using local fallback")
                    return await phase.execute(config)

            # Convert fog result to PhaseResult
            phase_result = self._convert_fog_result_to_phase_result(fog_result, phase_name)

            task.status = "completed"
            task.completed_at = datetime.now(UTC)
            task.results = fog_result

            logger.info(f"Distributed execution of {phase_name} completed successfully")
            return phase_result

        except Exception as e:
            logger.error(f"Distributed execution of {phase_name} failed: {e}")

            # Fallback to local execution
            if strategy != FogBurstStrategy.FOG_REQUIRED:
                logger.info(f"Falling back to local execution for {phase_name}")
                return await phase.execute(config)
            else:
                raise Exception(f"Fog execution required but failed: {str(e)}")

        finally:
            self.active_tasks.pop(task_id, None)

    async def _discover_fog_nodes(self) -> None:
        """Discover fog nodes with Agent Forge capabilities"""

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.fog_gateway_url}/v1/fog/nodes", params={"capabilities": "training,pytorch,gpu"}
                ) as response:
                    if response.status == 200:
                        nodes_data = await response.json()

                        for node_data in nodes_data.get("nodes", []):
                            capabilities = FogNodeCapabilities(
                                node_id=node_data["node_id"],
                                cpu_cores=node_data.get("resources", {}).get("cpu_cores", 0.0),
                                memory_gb=node_data.get("resources", {}).get("memory_gb", 0.0),
                                gpu_available=node_data.get("resources", {}).get("gpu_available", False),
                                gpu_memory_gb=node_data.get("resources", {}).get("gpu_memory_gb", 0.0),
                                supports_pytorch=node_data.get("capabilities", {}).get("pytorch", False),
                                supports_tensorflow=node_data.get("capabilities", {}).get("tensorflow", False),
                                supports_jax=node_data.get("capabilities", {}).get("jax", False),
                                supports_evomerge=node_data.get("capabilities", {}).get("evomerge", False),
                                supports_training=node_data.get("capabilities", {}).get("training", False),
                                supports_adas=node_data.get("capabilities", {}).get("adas", False),
                                training_throughput=node_data.get("metrics", {}).get("training_throughput", 0.0),
                                network_bandwidth_mbps=node_data.get("metrics", {}).get("network_bandwidth", 0.0),
                                storage_available_gb=node_data.get("resources", {}).get("storage_gb", 0.0),
                            )

                            self.available_nodes[capabilities.node_id] = capabilities

                        logger.debug(f"Discovered {len(self.available_nodes)} Agent Forge capable fog nodes")

        except Exception as e:
            logger.error(f"Error discovering fog nodes: {e}")

    async def _monitor_fog_nodes(self) -> None:
        """Monitor fog node health and capabilities"""

        while True:
            try:
                await asyncio.sleep(60)  # Check every minute
                await self._discover_fog_nodes()  # Refresh node list

            except Exception as e:
                logger.error(f"Error monitoring fog nodes: {e}")
                await asyncio.sleep(300)  # Wait longer on error

    async def _plan_phase_execution(
        self, phase: PhaseController, config: UnifiedConfig, strategy: FogBurstStrategy
    ) -> dict[str, Any]:
        """Plan how to execute phase across fog nodes"""

        phase_name = phase.__class__.__name__.replace("Phase", "").lower()
        plan = {
            "use_fog": False,
            "target_nodes": [],
            "distribution_mode": PhaseDistributionMode.SINGLE_NODE,
            "estimated_speedup": 1.0,
            "resource_requirements": {},
        }

        # Estimate resource requirements for the phase
        if phase_name == "evomerge":
            plan["resource_requirements"] = {
                "cpu_cores": 4.0,
                "memory_gb": 8.0,
                "gpu_required": False,
                "estimated_duration_s": 300,
            }
        elif phase_name == "quietstar":
            plan["resource_requirements"] = {
                "cpu_cores": 2.0,
                "memory_gb": 4.0,
                "gpu_required": False,
                "estimated_duration_s": 600,
            }
        elif phase_name == "adas":
            plan["resource_requirements"] = {
                "cpu_cores": 6.0,
                "memory_gb": 12.0,
                "gpu_required": True,
                "estimated_duration_s": 1800,
            }
        else:
            plan["resource_requirements"] = {
                "cpu_cores": 2.0,
                "memory_gb": 4.0,
                "gpu_required": False,
                "estimated_duration_s": 180,
            }

        # Find suitable nodes
        suitable_nodes = []
        for node_id, node_caps in self.available_nodes.items():
            if self._node_meets_requirements(node_caps, plan["resource_requirements"], phase_name):
                suitable_nodes.append(node_id)

        if suitable_nodes:
            plan["use_fog"] = True
            plan["target_nodes"] = suitable_nodes[:1]  # Use best node for now
            plan["estimated_speedup"] = self._estimate_speedup(phase_name, self.available_nodes[suitable_nodes[0]])

        return plan

    def _node_meets_requirements(
        self, node_caps: FogNodeCapabilities, requirements: dict[str, Any], phase_name: str
    ) -> bool:
        """Check if node meets requirements for phase execution"""

        # Basic resource requirements
        if node_caps.cpu_cores < requirements.get("cpu_cores", 0):
            return False

        if node_caps.memory_gb < requirements.get("memory_gb", 0):
            return False

        if requirements.get("gpu_required", False) and not node_caps.gpu_available:
            return False

        # Phase-specific capability requirements
        if phase_name == "evomerge" and not node_caps.supports_evomerge:
            return False

        if phase_name in ["quietstar", "forge_training"] and not node_caps.supports_training:
            return False

        if phase_name == "adas" and not node_caps.supports_adas:
            return False

        # Framework requirements
        if not node_caps.supports_pytorch:  # Assume PyTorch for now
            return False

        return True

    def _estimate_speedup(self, phase_name: str, node_caps: FogNodeCapabilities) -> float:
        """Estimate speedup from using fog node vs local execution"""

        base_speedup = 1.0

        # Factor in CPU cores
        if node_caps.cpu_cores > 4:
            base_speedup *= min(2.0, node_caps.cpu_cores / 4.0)

        # Factor in GPU availability
        if node_caps.gpu_available and phase_name in ["adas", "forge_training"]:
            base_speedup *= 3.0

        # Factor in memory
        if node_caps.memory_gb > 8:
            base_speedup *= 1.2

        # Phase-specific factors
        if phase_name == "evomerge" and node_caps.supports_evomerge:
            base_speedup *= 1.5

        return min(base_speedup, 5.0)  # Cap at 5x speedup

    async def _execute_on_fog(
        self, task: FogBurstTask, phase: PhaseController, config: UnifiedConfig, execution_plan: dict[str, Any]
    ) -> dict[str, Any]:
        """Execute phase on fog nodes"""

        target_node = execution_plan["target_nodes"][0]

        # Prepare job specification
        job_spec = {
            "namespace": "agent_forge",
            "runtime": "wasi",
            "image": f"agent_forge-{task.phase_name}:latest",
            "args": [
                "--phase",
                task.phase_name,
                "--config",
                json.dumps(_serialize_dataclass(config)),
            ],
            "env": {"AGENT_FORGE_PHASE": task.phase_name, "TASK_ID": task.task_id, "TARGET_NODE": target_node},
            "resources": execution_plan["resource_requirements"],
            "metadata": {"task_type": "agent_forge_phase", "phase_name": task.phase_name, "task_id": task.task_id},
        }

        try:
            async with aiohttp.ClientSession() as session:
                # Submit job to fog gateway
                async with session.post(f"{self.fog_gateway_url}/v1/fog/jobs", json=job_spec) as response:
                    if response.status == 201:
                        job_data = await response.json()
                        job_id = job_data["job_id"]
                        task.fog_job_ids.append(job_id)

                        # Wait for completion
                        result = await self._wait_for_fog_job(job_id, timeout=3600)  # 1 hour timeout

                        if result["status"] == "completed":
                            # Parse job output
                            try:
                                output_data = json.loads(result.get("stdout", "{}"))
                                return {
                                    "status": "success",
                                    "phase_name": task.phase_name,
                                    "metrics": output_data.get("metrics", {}),
                                    "metadata": output_data.get("metadata", {}),
                                    "artifacts": output_data.get("artifacts", {}),
                                    "fog_job_id": job_id,
                                    "fog_node": target_node,
                                    "execution_time_s": result.get("duration_ms", 0) / 1000.0,
                                }
                            except json.JSONDecodeError:
                                return {
                                    "status": "error",
                                    "message": "Failed to parse fog job output",
                                    "raw_output": result.get("stdout", ""),
                                }
                        else:
                            return {
                                "status": "error",
                                "message": f"Fog job failed: {result.get('error_message', 'Unknown error')}",
                                "job_result": result,
                            }
                    else:
                        error_text = await response.text()
                        return {"status": "error", "message": f"Failed to submit fog job: {error_text}"}

        except Exception as e:
            return {"status": "error", "message": f"Fog execution error: {str(e)}"}

    async def _wait_for_fog_job(self, job_id: str, timeout: int = 3600) -> dict[str, Any]:
        """Wait for fog job completion"""

        start_time = asyncio.get_event_loop().time()

        while True:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(f"{self.fog_gateway_url}/v1/fog/jobs/{job_id}/status") as response:
                        if response.status == 200:
                            status_data = await response.json()
                            job_status = status_data.get("status")

                            if job_status == "completed":
                                # Get full result
                                async with session.get(
                                    f"{self.fog_gateway_url}/v1/fog/jobs/{job_id}/result"
                                ) as result_response:
                                    if result_response.status == 200:
                                        return await result_response.json()
                                    else:
                                        return {"status": "error", "message": "Failed to get job result"}

                            elif job_status in ["failed", "cancelled", "timeout"]:
                                return {
                                    "status": job_status,
                                    "error_message": status_data.get("error_message", f"Job {job_status}"),
                                }

                            # Check timeout
                            if asyncio.get_event_loop().time() - start_time > timeout:
                                return {"status": "timeout", "error_message": f"Job timed out after {timeout}s"}

                            # Wait before next check
                            await asyncio.sleep(5)
                        else:
                            return {"status": "error", "message": f"Failed to check job status: {response.status}"}

            except Exception as e:
                return {"status": "error", "message": f"Error waiting for job: {str(e)}"}

    async def _validate_with_local_execution(
        self, task: FogBurstTask, phase: PhaseController, config: UnifiedConfig, fog_result: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate fog result against local execution"""

        try:
            logger.info(f"Running local validation for {task.phase_name}")

            # Execute locally for comparison
            local_result = await phase.execute(config)
            task.local_comparison = {
                "metrics": local_result.metrics,
                "metadata": local_result.metadata,
                "execution_time_s": getattr(local_result, "execution_time_s", 0.0),
            }

            # Validate parity
            parity_result = await self.parity_validator.validate_parity(
                local_result, fog_result, task.phase_name, task.task_id
            )

            return parity_result

        except Exception as e:
            logger.error(f"Local validation failed for {task.phase_name}: {e}")
            return {"parity_passed": False, "error": str(e), "validation_failed": True}

    def _convert_fog_result_to_phase_result(self, fog_result: dict[str, Any], phase_name: str) -> PhaseResult:
        """Convert fog execution result to PhaseResult"""

        return PhaseResult(
            phase_name=phase_name,
            status="completed" if fog_result["status"] == "success" else "failed",
            metrics=fog_result.get("metrics", {}),
            artifacts=fog_result.get("artifacts", {}),
            metadata={
                **fog_result.get("metadata", {}),
                "fog_executed": True,
                "fog_node": fog_result.get("fog_node"),
                "fog_job_id": fog_result.get("fog_job_id"),
                "execution_time_s": fog_result.get("execution_time_s", 0.0),
            },
        )

    async def shutdown(self) -> None:
        """Shutdown fog burst orchestrator"""
        if self._node_discovery_task:
            self._node_discovery_task.cancel()
            try:
                await self._node_discovery_task
            except asyncio.CancelledError:
                pass
            except Exception as e:
                logger.error(f"Error shutting down node discovery task: {e}")
            finally:
                self._node_discovery_task = None

        await self._drain_network_requests()

        logger.info("FogBurstOrchestrator shutdown completed")

    async def _drain_network_requests(self) -> None:
        """Drain outstanding network requests before shutdown."""

        # Give the event loop a chance to finalize any in-flight aiohttp operations
        await asyncio.sleep(0)


# Convenience functions for integration
async def create_fog_burst_orchestrator(
    fog_gateway_url: str = "http://localhost:8080",
    strategy: FogBurstStrategy = FogBurstStrategy.FOG_PREFERRED,
    parity_tolerance: float = 1e-4,
) -> FogBurstOrchestrator:
    """Create and initialize fog burst orchestrator"""

    orchestrator = FogBurstOrchestrator(fog_gateway_url, strategy, parity_tolerance)

    if await orchestrator.initialize():
        return orchestrator
    else:
        raise Exception("Failed to initialize FogBurstOrchestrator")


# Export main classes
__all__ = [
    "FogBurstOrchestrator",
    "FogBurstStrategy",
    "PhaseDistributionMode",
    "FogNodeCapabilities",
    "FogBurstTask",
    "ParityValidator",
    "create_fog_burst_orchestrator",
]
