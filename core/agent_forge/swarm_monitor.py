#!/usr/bin/env python3
"""
Agent Forge Swarm Monitoring and Quality Gates

Real-time monitoring, performance analysis, and quality gate enforcement
for the Agent Forge swarm coordination system.

Features:
- Real-time performance monitoring and bottleneck detection
- Theater detection and reality validation
- NASA POT10 compliance checking
- Quality gate enforcement with automated thresholds
- Cross-phase memory persistence and learning transfer
- Resource utilization tracking and optimization
"""

import asyncio
import json
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import torch
import torch.nn as nn

logger = logging.getLogger(__name__)


class AlertLevel(Enum):
    """Alert severity levels for monitoring system."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class QualityGateStatus(Enum):
    """Quality gate status indicators."""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    BLOCKED = "blocked"


@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for monitoring."""
    timestamp: datetime
    phase: int
    agent_id: str

    # Execution metrics
    task_completion_time: float = 0.0
    memory_usage_mb: float = 0.0
    cpu_utilization: float = 0.0
    gpu_utilization: float = 0.0

    # Quality metrics
    accuracy: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0

    # Theater detection metrics
    implementation_depth_score: float = 0.0
    reality_validation_score: float = 0.0
    performance_authenticity: float = 0.0

    # Bottleneck indicators
    bottleneck_detected: bool = False
    bottleneck_type: Optional[str] = None
    resolution_suggestion: Optional[str] = None


@dataclass
class QualityGate:
    """Quality gate definition with thresholds and validation logic."""
    name: str
    phase: int
    thresholds: Dict[str, float]
    validators: List[str]
    blocking: bool = True
    timeout_seconds: int = 300

    # State tracking
    status: QualityGateStatus = QualityGateStatus.PENDING
    start_time: Optional[datetime] = None
    completion_time: Optional[datetime] = None
    validation_results: Dict[str, Any] = field(default_factory=dict)
    failure_reasons: List[str] = field(default_factory=list)


@dataclass
class MonitoringAlert:
    """Alert generated by monitoring system."""
    timestamp: datetime
    level: AlertLevel
    source: str
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    resolved: bool = False
    resolution_time: Optional[datetime] = None


class TheaterDetector:
    """Advanced theater detection system for identifying fake implementations."""

    def __init__(self):
        self.logger = logging.getLogger("TheaterDetector")
        self.detection_history = []

        # Theater detection patterns
        self.patterns = {
            "fake_metrics": {
                "suspiciously_high_improvement": 0.8,  # >80% improvement is suspicious
                "perfect_scores": 1.0,  # Perfect scores are often fake
                "metric_inconsistency": 0.3  # Inconsistent metrics between related measures
            },
            "shallow_implementation": {
                "minimal_code_changes": 0.1,  # <10% code change for major feature
                "missing_core_components": True,  # Core functionality missing
                "placeholder_implementations": True  # Stub/placeholder code
            },
            "performance_theater": {
                "unrealistic_speedup": 5.0,  # >5x speedup without architectural changes
                "memory_usage_anomalies": 0.5,  # Suspicious memory patterns
                "benchmark_gaming": True  # Optimized only for specific benchmarks
            }
        }

    async def detect_theater(self, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Comprehensive theater detection analysis."""
        detection_start = time.time()

        # Analyze different theater types
        fake_metrics = self._detect_fake_metrics(phase_data)
        shallow_impl = self._detect_shallow_implementation(phase_data)
        perf_theater = self._detect_performance_theater(phase_data)

        # Calculate overall theater score
        theater_indicators = {
            "fake_metrics": fake_metrics,
            "shallow_implementation": shallow_impl,
            "performance_theater": perf_theater
        }

        theater_score = sum(indicator["score"] for indicator in theater_indicators.values()) / len(theater_indicators)
        theater_detected = theater_score > 0.5

        # Generate detailed analysis
        analysis = {
            "theater_detected": theater_detected,
            "theater_score": theater_score,
            "confidence": min(0.95, theater_score * 1.2),  # Cap confidence at 95%
            "indicators": theater_indicators,
            "recommendations": self._generate_remediation_recommendations(theater_indicators),
            "detection_time": time.time() - detection_start
        }

        # Store in history for learning
        self.detection_history.append({
            "timestamp": datetime.now(),
            "phase": phase_data.get("phase", 0),
            "analysis": analysis
        })

        self.logger.info(f"Theater detection completed: {'DETECTED' if theater_detected else 'CLEAR'} (score: {theater_score:.3f})")
        return analysis

    def _detect_fake_metrics(self, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Detect fake or manipulated metrics."""
        metrics = phase_data.get("metrics", {})

        indicators = {
            "suspiciously_high": False,
            "perfect_scores": False,
            "inconsistent": False
        }

        # Check for suspiciously high improvements
        improvement = metrics.get("performance_improvement", 0.0)
        if improvement > self.patterns["fake_metrics"]["suspiciously_high_improvement"]:
            indicators["suspiciously_high"] = True

        # Check for perfect scores (often fake)
        accuracy = metrics.get("accuracy", 0.0)
        if accuracy >= self.patterns["fake_metrics"]["perfect_scores"]:
            indicators["perfect_scores"] = True

        # Check for metric inconsistency
        speed_improvement = metrics.get("speed_improvement", 0.0)
        memory_reduction = metrics.get("memory_reduction", 0.0)
        if abs(speed_improvement - memory_reduction) > self.patterns["fake_metrics"]["metric_inconsistency"]:
            indicators["inconsistent"] = True

        score = sum(indicators.values()) / len(indicators)

        return {
            "score": score,
            "indicators": indicators,
            "evidence": f"Metrics analysis: {sum(indicators.values())}/{len(indicators)} red flags"
        }

    def _detect_shallow_implementation(self, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Detect shallow or incomplete implementations."""
        implementation = phase_data.get("implementation", {})

        indicators = {
            "minimal_changes": False,
            "missing_components": False,
            "placeholder_code": False
        }

        # Check code change volume
        lines_changed = implementation.get("lines_changed", 0)
        total_lines = implementation.get("total_lines", 1000)
        change_ratio = lines_changed / total_lines

        if change_ratio < self.patterns["shallow_implementation"]["minimal_code_changes"]:
            indicators["minimal_changes"] = True

        # Check for missing core components
        required_components = implementation.get("required_components", [])
        implemented_components = implementation.get("implemented_components", [])
        missing_ratio = 1 - (len(implemented_components) / max(len(required_components), 1))

        if missing_ratio > 0.5:  # More than 50% missing
            indicators["missing_components"] = True

        # Check for placeholder implementations
        placeholder_patterns = implementation.get("placeholder_patterns", 0)
        if placeholder_patterns > 0:
            indicators["placeholder_code"] = True

        score = sum(indicators.values()) / len(indicators)

        return {
            "score": score,
            "indicators": indicators,
            "evidence": f"Implementation depth: {(1-score)*100:.1f}% complete"
        }

    def _detect_performance_theater(self, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Detect performance theater and benchmark gaming."""
        performance = phase_data.get("performance", {})

        indicators = {
            "unrealistic_speedup": False,
            "memory_anomalies": False,
            "benchmark_gaming": False
        }

        # Check for unrealistic speedup claims
        speedup = performance.get("speedup_factor", 1.0)
        if speedup > self.patterns["performance_theater"]["unrealistic_speedup"]:
            indicators["unrealistic_speedup"] = True

        # Check memory usage patterns
        memory_baseline = performance.get("memory_baseline", 100)
        memory_optimized = performance.get("memory_optimized", 100)
        memory_reduction = (memory_baseline - memory_optimized) / memory_baseline

        if memory_reduction > 0.9:  # >90% memory reduction is suspicious
            indicators["memory_anomalies"] = True

        # Check for benchmark-specific optimizations
        benchmark_scores = performance.get("benchmark_scores", {})
        if len(benchmark_scores) == 1 and list(benchmark_scores.values())[0] > 0.95:
            indicators["benchmark_gaming"] = True

        score = sum(indicators.values()) / len(indicators)

        return {
            "score": score,
            "indicators": indicators,
            "evidence": f"Performance authenticity: {(1-score)*100:.1f}%"
        }

    def _generate_remediation_recommendations(self, indicators: Dict[str, Any]) -> List[str]:
        """Generate recommendations for addressing detected theater."""
        recommendations = []

        for indicator_type, analysis in indicators.items():
            if analysis["score"] > 0.3:  # Significant theater detected
                if indicator_type == "fake_metrics":
                    recommendations.append("Implement independent metric validation and cross-verification")
                    recommendations.append("Add realistic baseline comparisons and error margins")
                elif indicator_type == "shallow_implementation":
                    recommendations.append("Deepen implementation with core functionality")
                    recommendations.append("Replace placeholder code with working implementations")
                elif indicator_type == "performance_theater":
                    recommendations.append("Validate performance claims with independent benchmarks")
                    recommendations.append("Implement comprehensive performance monitoring")

        return recommendations


class QualityGateValidator:
    """Quality gate validation system with NASA POT10 compliance."""

    def __init__(self):
        self.logger = logging.getLogger("QualityGateValidator")
        self.gates = {}
        self.validation_history = []

        # Initialize standard quality gates
        self._initialize_standard_gates()

    def _initialize_standard_gates(self):
        """Initialize standard quality gates for all phases."""

        # Phase 3: Quiet-STaR Remediation Gates
        self.gates["phase3_theater_elimination"] = QualityGate(
            name="Theater Elimination",
            phase=3,
            thresholds={
                "theater_score": 0.3,  # Must be below 30%
                "implementation_depth": 0.7,  # Must be above 70%
                "reality_validation": 0.8   # Must be above 80%
            },
            validators=["theater_detector", "implementation_validator"],
            blocking=True
        )

        self.gates["phase3_reasoning_quality"] = QualityGate(
            name="Reasoning Quality",
            phase=3,
            thresholds={
                "reasoning_coherence": 0.75,
                "thought_depth": 0.6,
                "integration_quality": 0.8
            },
            validators=["reasoning_specialist", "quality_gate"],
            blocking=True
        )

        # Phase 4: BitNet Compression Gates
        self.gates["phase4_compression_efficiency"] = QualityGate(
            name="Compression Efficiency",
            phase=4,
            thresholds={
                "compression_ratio": 0.158,  # BitNet 1.58 target
                "accuracy_retention": 0.95,
                "memory_reduction": 0.8
            },
            validators=["bitnet_compression", "benchmarking_agent"],
            blocking=True
        )

        self.gates["phase4_performance_validation"] = QualityGate(
            name="Performance Validation",
            phase=4,
            thresholds={
                "inference_speedup": 0.3,  # 30% minimum speedup
                "energy_efficiency": 0.4,
                "model_stability": 0.9
            },
            validators=["performance_optimizer", "security_validator"],
            blocking=True
        )

        # Phase 5: Training Quality Gates
        self.gates["phase5_training_convergence"] = QualityGate(
            name="Training Convergence",
            phase=5,
            thresholds={
                "convergence_rate": 0.8,
                "final_accuracy": 0.9,
                "grokfast_effectiveness": 0.15
            },
            validators=["training_orchestrator", "model_optimizer"],
            blocking=True
        )

        # NASA POT10 Compliance Gate (applies to all phases)
        self.gates["nasa_pot10_compliance"] = QualityGate(
            name="NASA POT10 Compliance",
            phase=0,  # Universal gate
            thresholds={
                "security_score": 0.95,
                "reliability_score": 0.9,
                "maintainability_score": 0.85,
                "documentation_coverage": 0.8
            },
            validators=["security_validator", "quality_gate"],
            blocking=True,
            timeout_seconds=600  # Longer timeout for compliance checks
        )

    async def validate_phase_gates(self, phase: int, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate all quality gates for a specific phase."""
        validation_start = time.time()

        # Get gates for this phase
        phase_gates = [gate for gate in self.gates.values()
                      if gate.phase == phase or gate.phase == 0]  # Include universal gates

        if not phase_gates:
            self.logger.warning(f"No quality gates defined for Phase {phase}")
            return {"all_gates_passed": True, "gates_validated": 0}

        # Validate each gate
        gate_results = {}
        all_passed = True

        for gate in phase_gates:
            gate.status = QualityGateStatus.RUNNING
            gate.start_time = datetime.now()

            try:
                result = await self._validate_single_gate(gate, phase_data)
                gate_results[gate.name] = result

                if result["passed"]:
                    gate.status = QualityGateStatus.PASSED
                    gate.completion_time = datetime.now()
                else:
                    gate.status = QualityGateStatus.FAILED
                    gate.failure_reasons = result.get("failure_reasons", [])
                    if gate.blocking:
                        all_passed = False

            except Exception as e:
                gate.status = QualityGateStatus.FAILED
                gate.failure_reasons = [f"Validation error: {str(e)}"]
                gate_results[gate.name] = {
                    "passed": False,
                    "error": str(e),
                    "failure_reasons": [str(e)]
                }
                if gate.blocking:
                    all_passed = False

        # Store validation history
        validation_record = {
            "timestamp": datetime.now(),
            "phase": phase,
            "gates_validated": len(phase_gates),
            "gates_passed": sum(1 for result in gate_results.values() if result.get("passed", False)),
            "all_passed": all_passed,
            "validation_time": time.time() - validation_start,
            "gate_results": gate_results
        }
        self.validation_history.append(validation_record)

        self.logger.info(f"Phase {phase} quality gates: {'PASSED' if all_passed else 'FAILED'} "
                        f"({validation_record['gates_passed']}/{validation_record['gates_validated']})")

        return {
            "all_gates_passed": all_passed,
            "gates_validated": len(phase_gates),
            "gates_passed": validation_record['gates_passed'],
            "validation_time": validation_record['validation_time'],
            "gate_results": gate_results,
            "blocking_failures": [name for name, result in gate_results.items()
                                if not result.get("passed", False) and
                                any(gate.name == name and gate.blocking for gate in phase_gates)]
        }

    async def _validate_single_gate(self, gate: QualityGate, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate a single quality gate."""

        # Extract relevant metrics from phase data
        metrics = phase_data.get("metrics", {})
        implementation = phase_data.get("implementation", {})
        performance = phase_data.get("performance", {})

        # Check each threshold
        threshold_results = {}
        passed_thresholds = 0

        for metric_name, threshold in gate.thresholds.items():
            # Get metric value from appropriate source
            value = self._extract_metric_value(metric_name, metrics, implementation, performance)

            # Determine pass/fail based on metric type
            if metric_name in ["theater_score", "error_rate"]:
                # Lower is better
                passed = value <= threshold
            else:
                # Higher is better
                passed = value >= threshold

            threshold_results[metric_name] = {
                "value": value,
                "threshold": threshold,
                "passed": passed
            }

            if passed:
                passed_thresholds += 1

        # Gate passes if all thresholds are met
        gate_passed = passed_thresholds == len(gate.thresholds)

        # Generate failure reasons if gate failed
        failure_reasons = []
        if not gate_passed:
            for metric_name, result in threshold_results.items():
                if not result["passed"]:
                    failure_reasons.append(
                        f"{metric_name}: {result['value']:.3f} vs threshold {result['threshold']:.3f}"
                    )

        return {
            "passed": gate_passed,
            "threshold_results": threshold_results,
            "passed_thresholds": passed_thresholds,
            "total_thresholds": len(gate.thresholds),
            "failure_reasons": failure_reasons
        }

    def _extract_metric_value(self, metric_name: str, metrics: Dict,
                            implementation: Dict, performance: Dict) -> float:
        """Extract metric value from appropriate data source."""

        # Theater and implementation metrics
        if metric_name in ["theater_score", "implementation_depth", "reality_validation"]:
            return implementation.get(metric_name, 0.0)

        # Performance metrics
        elif metric_name in ["inference_speedup", "energy_efficiency", "compression_ratio"]:
            return performance.get(metric_name, 0.0)

        # Quality metrics
        elif metric_name in ["accuracy_retention", "convergence_rate", "final_accuracy"]:
            return metrics.get(metric_name, 0.0)

        # Security and compliance metrics
        elif metric_name in ["security_score", "reliability_score", "maintainability_score"]:
            return metrics.get(metric_name, 0.0)

        # Default fallback
        else:
            return metrics.get(metric_name, 0.0)


class SwarmMonitor:
    """Comprehensive monitoring system for Agent Forge swarm."""

    def __init__(self, coordinator):
        self.coordinator = coordinator
        self.logger = logging.getLogger("SwarmMonitor")

        # Initialize monitoring components
        self.theater_detector = TheaterDetector()
        self.quality_gate_validator = QualityGateValidator()

        # Monitoring state
        self.alerts = []
        self.performance_history = []
        self.resource_usage = {}
        self.bottleneck_patterns = {}

        # Monitoring configuration
        self.monitoring_interval = 1.0  # seconds
        self.alert_thresholds = {
            "high_memory_usage": 0.85,
            "high_cpu_usage": 0.9,
            "long_execution_time": 300,  # 5 minutes
            "high_error_rate": 0.1
        }

        # Start monitoring loop
        self.monitoring_active = False

    async def start_monitoring(self):
        """Start the monitoring system."""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.logger.info("Starting swarm monitoring system")

        # Start monitoring tasks
        await asyncio.gather(
            self._monitor_agents(),
            self._monitor_resources(),
            self._detect_bottlenecks(),
            return_exceptions=True
        )

    async def stop_monitoring(self):
        """Stop the monitoring system."""
        self.monitoring_active = False
        self.logger.info("Stopping swarm monitoring system")

    async def _monitor_agents(self):
        """Monitor individual agent performance."""
        while self.monitoring_active:
            try:
                for agent_id, agent in self.coordinator.agents.items():
                    # Collect performance metrics
                    metrics = PerformanceMetrics(
                        timestamp=datetime.now(),
                        phase=agent.config.phase,
                        agent_id=agent_id,
                        memory_usage_mb=self._get_agent_memory_usage(agent),
                        cpu_utilization=self._get_agent_cpu_usage(agent),
                        task_completion_time=self._get_task_completion_time(agent)
                    )

                    # Check for performance issues
                    await self._check_agent_performance(agent, metrics)

                    # Store metrics
                    self.performance_history.append(metrics)

                # Clean old metrics (keep last 1000 entries)
                if len(self.performance_history) > 1000:
                    self.performance_history = self.performance_history[-1000:]

                await asyncio.sleep(self.monitoring_interval)

            except Exception as e:
                self.logger.error(f"Agent monitoring error: {str(e)}")
                await asyncio.sleep(5)  # Wait before retrying

    async def _monitor_resources(self):
        """Monitor system resource usage."""
        while self.monitoring_active:
            try:
                # Monitor system resources
                self.resource_usage = {
                    "timestamp": datetime.now(),
                    "total_memory_mb": self._get_total_memory_usage(),
                    "cpu_utilization": self._get_total_cpu_usage(),
                    "gpu_utilization": self._get_gpu_usage(),
                    "active_agents": len([a for a in self.coordinator.agents.values()
                                        if a.state != "idle"]),
                    "total_agents": len(self.coordinator.agents)
                }

                # Check resource thresholds
                await self._check_resource_thresholds()

                await asyncio.sleep(self.monitoring_interval * 5)  # Check every 5 seconds

            except Exception as e:
                self.logger.error(f"Resource monitoring error: {str(e)}")
                await asyncio.sleep(10)

    async def _detect_bottlenecks(self):
        """Detect performance bottlenecks in the swarm."""
        while self.monitoring_active:
            try:
                # Analyze performance patterns
                if len(self.performance_history) > 10:
                    bottlenecks = self._analyze_bottleneck_patterns()

                    for bottleneck in bottlenecks:
                        await self._handle_bottleneck(bottleneck)

                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                self.logger.error(f"Bottleneck detection error: {str(e)}")
                await asyncio.sleep(30)

    def _get_agent_memory_usage(self, agent) -> float:
        """Get agent memory usage (simulated)."""
        # In real implementation, this would query actual memory usage
        base_usage = 64  # MB base usage
        task_usage = 32 if agent.current_task else 0
        return base_usage + task_usage

    def _get_agent_cpu_usage(self, agent) -> float:
        """Get agent CPU usage (simulated)."""
        if agent.state == "executing":
            return 0.7 + (hash(agent.config.role.value) % 30) / 100  # 70-100%
        return 0.05  # 5% idle usage

    def _get_task_completion_time(self, agent) -> float:
        """Get current task completion time."""
        if agent.start_time:
            return time.time() - agent.start_time
        return 0.0

    def _get_total_memory_usage(self) -> float:
        """Get total system memory usage (simulated)."""
        return sum(self._get_agent_memory_usage(agent)
                  for agent in self.coordinator.agents.values())

    def _get_total_cpu_usage(self) -> float:
        """Get total CPU usage (simulated)."""
        active_agents = [a for a in self.coordinator.agents.values() if a.state != "idle"]
        return min(1.0, len(active_agents) * 0.15)  # 15% per active agent

    def _get_gpu_usage(self) -> float:
        """Get GPU usage (simulated)."""
        # Simulate GPU usage based on active training/compression agents
        gpu_agents = [a for a in self.coordinator.agents.values()
                     if a.config.role.value in ["training_orchestrator", "bitnet_compression"]
                     and a.state == "executing"]
        return min(1.0, len(gpu_agents) * 0.4)  # 40% per GPU-intensive agent

    async def _check_agent_performance(self, agent, metrics: PerformanceMetrics):
        """Check individual agent performance and generate alerts."""

        # Check memory usage
        if metrics.memory_usage_mb > 500:  # 500MB threshold
            await self._create_alert(
                AlertLevel.WARNING,
                f"agent_{agent.config.role.value}",
                f"High memory usage: {metrics.memory_usage_mb:.1f}MB",
                {"agent_id": agent.config.role.value, "memory_mb": metrics.memory_usage_mb}
            )

        # Check execution time
        if metrics.task_completion_time > self.alert_thresholds["long_execution_time"]:
            await self._create_alert(
                AlertLevel.WARNING,
                f"agent_{agent.config.role.value}",
                f"Long execution time: {metrics.task_completion_time:.1f}s",
                {"agent_id": agent.config.role.value, "execution_time": metrics.task_completion_time}
            )

    async def _check_resource_thresholds(self):
        """Check system resource thresholds."""
        usage = self.resource_usage

        # Check memory threshold
        if usage["total_memory_mb"] > 3000:  # 3GB threshold
            await self._create_alert(
                AlertLevel.ERROR,
                "system",
                f"High system memory usage: {usage['total_memory_mb']:.1f}MB",
                usage
            )

        # Check CPU threshold
        if usage["cpu_utilization"] > self.alert_thresholds["high_cpu_usage"]:
            await self._create_alert(
                AlertLevel.WARNING,
                "system",
                f"High CPU utilization: {usage['cpu_utilization']:.1%}",
                usage
            )

    def _analyze_bottleneck_patterns(self) -> List[Dict[str, Any]]:
        """Analyze performance history for bottleneck patterns."""
        bottlenecks = []

        # Analyze recent performance metrics
        recent_metrics = self.performance_history[-100:]  # Last 100 metrics

        # Group by agent role
        role_metrics = {}
        for metric in recent_metrics:
            role = metric.agent_id.split('_')[-1] if '_' in metric.agent_id else metric.agent_id
            if role not in role_metrics:
                role_metrics[role] = []
            role_metrics[role].append(metric)

        # Check for slow agents
        for role, metrics in role_metrics.items():
            if len(metrics) > 5:
                avg_completion_time = sum(m.task_completion_time for m in metrics) / len(metrics)
                if avg_completion_time > 60:  # 1 minute average
                    bottlenecks.append({
                        "type": "slow_agent",
                        "role": role,
                        "avg_completion_time": avg_completion_time,
                        "metric_count": len(metrics)
                    })

        return bottlenecks

    async def _handle_bottleneck(self, bottleneck: Dict[str, Any]):
        """Handle detected bottleneck."""
        bottleneck_type = bottleneck["type"]

        if bottleneck_type == "slow_agent":
            await self._create_alert(
                AlertLevel.WARNING,
                "bottleneck_detector",
                f"Slow agent detected: {bottleneck['role']} "
                f"(avg: {bottleneck['avg_completion_time']:.1f}s)",
                bottleneck
            )

            # Store bottleneck pattern for learning
            self.bottleneck_patterns[bottleneck["role"]] = bottleneck

    async def _create_alert(self, level: AlertLevel, source: str,
                          message: str, details: Dict[str, Any]):
        """Create and store monitoring alert."""
        alert = MonitoringAlert(
            timestamp=datetime.now(),
            level=level,
            source=source,
            message=message,
            details=details
        )

        self.alerts.append(alert)

        # Log alert
        log_method = {
            AlertLevel.INFO: self.logger.info,
            AlertLevel.WARNING: self.logger.warning,
            AlertLevel.ERROR: self.logger.error,
            AlertLevel.CRITICAL: self.logger.critical
        }[level]

        log_method(f"[{level.value.upper()}] {source}: {message}")

        # Keep only recent alerts (last 1000)
        if len(self.alerts) > 1000:
            self.alerts = self.alerts[-1000:]

    async def get_monitoring_status(self) -> Dict[str, Any]:
        """Get comprehensive monitoring status."""
        recent_alerts = [a for a in self.alerts
                        if (datetime.now() - a.timestamp).total_seconds() < 3600]  # Last hour

        return {
            "monitoring_active": self.monitoring_active,
            "total_agents": len(self.coordinator.agents),
            "active_agents": len([a for a in self.coordinator.agents.values()
                                if a.state != "idle"]),
            "resource_usage": self.resource_usage,
            "recent_alerts": len(recent_alerts),
            "alert_breakdown": {
                level.value: len([a for a in recent_alerts if a.level == level])
                for level in AlertLevel
            },
            "bottleneck_patterns": len(self.bottleneck_patterns),
            "performance_metrics_count": len(self.performance_history),
            "quality_gates_available": len(self.quality_gate_validator.gates)
        }

    async def run_theater_detection(self, phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Run theater detection analysis."""
        return await self.theater_detector.detect_theater(phase_data)

    async def validate_quality_gates(self, phase: int,
                                   phase_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate quality gates for a phase."""
        return await self.quality_gate_validator.validate_phase_gates(phase, phase_data)


# Integration function for creating monitoring system
def create_swarm_monitor(coordinator) -> SwarmMonitor:
    """Create and configure swarm monitoring system."""
    return SwarmMonitor(coordinator)