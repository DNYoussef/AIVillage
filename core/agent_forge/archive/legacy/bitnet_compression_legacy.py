#!/usr/bin/env python3
"""
Agent Forge Phase 3: BitNet 1.58-bit Compression

This phase implements initial compression using BitNet 1.58-bit quantization before
the main training loop. This provides memory efficiency during training while
maintaining model performance.

Key Features:
- BitNet 1.58-bit quantization ({-1, 0, +1} weights)
- Memory-efficient compression for training
- Grokfast integration for accelerated fine-tuning
- Comprehensive evaluation and benchmarking
- Production-ready compression pipeline
- Preservation of critical layer precision

Consolidates implementations from:
- packages/agent_forge/legacy_src/compression/bitnet_enhanced.py (main implementation)
- packages/agent_forge/legacy_src/compression/bitnet.py (core compression)
- packages/agent_forge/legacy_src/foundation/bitnet.py (foundation components)
"""

from dataclasses import dataclass, field
import json
import logging
from pathlib import Path
from typing import Any

from datasets import load_dataset
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments

from packages.agent_forge.core.phase_controller import PhaseController, PhaseResult
from packages.agent_forge.legacy_src.training.grokfast_ctrl import GrokfastOptimizer

logger = logging.getLogger(__name__)


# ============================================================================
# Configuration
# ============================================================================


@dataclass
class PhaseConfig:
    """Base configuration class for Agent Forge phases."""

    pass


@dataclass
class BitNetConfig(PhaseConfig):
    """Configuration for BitNet compression phase."""

    pass


@dataclass
class BitNetCompressionConfig(PhaseConfig):
    """Configuration for BitNet compression phase."""

    # Model configuration
    model_path: str = ""
    output_path: str = ""
    tokenizer_path: str | None = None

    # BitNet quantization settings
    quantization_bits: float = 1.58  # BitNet 1.58-bit quantization
    preserve_embedding_precision: bool = True  # Keep embeddings in higher precision
    preserve_output_precision: bool = True  # Keep final layers in higher precision
    sparsity_threshold: float = 0.1  # Threshold for setting weights to 0

    # Calibration settings
    calibration_samples: int = 1000
    calibration_dataset: str = "openwebtext"  # or "c4" or "wikitext"
    calibration_batch_size: int = 4
    calibration_sequence_length: int = 512

    # Fine-tuning configuration (to recover accuracy after compression)
    enable_fine_tuning: bool = True
    fine_tune_epochs: int = 2
    fine_tune_lr: float = 1e-5
    warmup_steps: int = 50
    weight_decay: float = 0.01

    # Grokfast integration
    enable_grokfast: bool = True
    grokfast_ema_alpha: float = 0.98
    grokfast_lambda: float = 2.0

    # Evaluation configuration
    eval_samples: int = 500
    eval_tasks: list[str] = field(default_factory=lambda: ["perplexity", "generation_quality"])

    # Compression targets
    target_compression_ratio: float = 8.0  # Target 8x compression
    max_accuracy_drop: float = 0.05  # Max 5% accuracy drop

    # System configuration
    device: str = "auto"
    mixed_precision: bool = True
    seed: int = 42

    # W&B tracking
    wandb_project: str = "agent_forge"
    wandb_entity: str | None = None
    wandb_tags: list[str] = field(default_factory=lambda: ["bitnet", "compression", "phase3"])


# ============================================================================
# BitNet Quantization Core
# ============================================================================


class BitNetQuantizer:
    """
    Core BitNet 1.58-bit quantization implementation.

    Quantizes weights to {-1, 0, +1} with dynamic scaling and sparsity.
    """

    def __init__(self, config: BitNetCompressionConfig):
        self.config = config
        self.quantization_stats = {
            "layers_quantized": 0,
            "total_parameters": 0,
            "quantized_parameters": 0,
            "sparsity_ratio": 0.0,
        }

    def quantize_tensor(self, tensor: torch.Tensor, preserve_precision: bool = False) -> dict[str, Any]:
        """
        Apply BitNet 1.58-bit quantization to a tensor.

        Args:
            tensor: Input tensor to quantize
            preserve_precision: If True, skip quantization for this tensor

        Returns:
            Dictionary containing quantized data and metadata
        """
        if preserve_precision or tensor.numel() < 1024:  # Keep small tensors unquantized
            return {
                "weights": tensor.cpu().numpy(),
                "scale": 1.0,
                "quantization_type": "none",
                "is_quantized": False,
                "shape": tensor.shape,
                "dtype": str(tensor.dtype),
            }

        # Calculate dynamic scale per channel/row for better precision
        if len(tensor.shape) >= 2:
            # Per-output-channel scaling for Linear layers
            scale = tensor.abs().mean(dim=list(range(1, len(tensor.shape))), keepdim=True)
        else:
            # Global scaling for 1D tensors
            scale = tensor.abs().mean()

        # Avoid division by zero
        scale = torch.clamp(scale, min=1e-8)

        # Normalize by scale
        normalized = tensor / scale

        # Apply sparsity threshold - set small values to 0
        sparsity_mask = tensor.abs() < (scale * self.config.sparsity_threshold)

        # Quantize to {-1, 0, +1}
        quantized = torch.sign(normalized)
        quantized[sparsity_mask] = 0

        # Calculate sparsity for statistics
        sparsity = (quantized == 0).float().mean().item()

        # Convert to int8 for storage efficiency
        quantized_int8 = quantized.to(torch.int8)

        # Update statistics
        self.quantization_stats["total_parameters"] += tensor.numel()
        self.quantization_stats["quantized_parameters"] += tensor.numel()
        self.quantization_stats["sparsity_ratio"] = (
            self.quantization_stats["sparsity_ratio"] * self.quantization_stats["layers_quantized"] + sparsity
        ) / (self.quantization_stats["layers_quantized"] + 1)
        self.quantization_stats["layers_quantized"] += 1

        return {
            "weights": quantized_int8.cpu().numpy(),
            "scale": scale.cpu().numpy(),
            "quantization_type": "bitnet_1.58",
            "is_quantized": True,
            "shape": tensor.shape,
            "dtype": str(tensor.dtype),
            "sparsity": sparsity,
        }

    def dequantize_tensor(self, quantized_data: dict[str, Any]) -> torch.Tensor:
        """Dequantize a tensor from BitNet format."""
        if not quantized_data.get("is_quantized", False):
            # Return unquantized tensor as-is
            weights = np.array(quantized_data["weights"])
            return torch.from_numpy(weights).reshape(quantized_data["shape"])

        # Reconstruct quantized tensor
        weights = torch.from_numpy(quantized_data["weights"]).float()
        scale = torch.from_numpy(quantized_data["scale"]).float()

        # Restore original scale
        dequantized = weights * scale

        return dequantized.reshape(quantized_data["shape"])


class BitNetCompressedModel(nn.Module):
    """
    Wrapper for BitNet-compressed models with transparent operation.

    This allows compressed models to be used as drop-in replacements
    for original models during training and inference.
    """

    def __init__(self, original_model: nn.Module, quantizer: BitNetQuantizer):
        super().__init__()
        self.original_model = original_model
        self.quantizer = quantizer
        self.compressed_layers = {}
        self.layer_map = {}

        # Compress the model
        self._compress_model()

    def _compress_model(self):
        """Compress all applicable layers in the model."""
        logger.info("Compressing model layers...")

        for name, module in self.original_model.named_modules():
            if isinstance(module, nn.Linear | nn.Conv1d | nn.Conv2d):
                # Determine if layer should preserve precision
                preserve = self._should_preserve_precision(name, module)

                if hasattr(module, "weight") and module.weight is not None:
                    compressed_weight = self.quantizer.quantize_tensor(module.weight.data, preserve_precision=preserve)
                    self.compressed_layers[f"{name}.weight"] = compressed_weight

                if hasattr(module, "bias") and module.bias is not None:
                    # Always preserve bias precision for stability
                    compressed_bias = self.quantizer.quantize_tensor(module.bias.data, preserve_precision=True)
                    self.compressed_layers[f"{name}.bias"] = compressed_bias

                self.layer_map[name] = module

        logger.info(f"Compressed {len(self.compressed_layers)} layer parameters")

        # Replace original weights with compressed versions
        self._apply_compressed_weights()

    def _should_preserve_precision(self, layer_name: str, module: nn.Module) -> bool:
        """Determine if a layer should preserve full precision."""
        # Preserve embedding layers
        if self.quantizer.config.preserve_embedding_precision:
            if "embed" in layer_name.lower() or isinstance(module.weight, nn.Embedding):
                return True

        # Preserve output layers (final classification/generation layers)
        if self.quantizer.config.preserve_output_precision:
            if any(keyword in layer_name.lower() for keyword in ["output", "classifier", "head", "lm_head"]):
                return True

        # Preserve very small layers
        if hasattr(module, "weight") and module.weight.numel() < 1024:
            return True

        return False

    def _apply_compressed_weights(self):
        """Apply compressed weights back to model."""
        for layer_name, compressed_data in self.compressed_layers.items():
            module_name, param_name = layer_name.rsplit(".", 1)

            if module_name in self.layer_map:
                module = self.layer_map[module_name]

                # Get dequantized weight
                dequantized = self.quantizer.dequantize_tensor(compressed_data)

                # Apply to module
                if param_name == "weight":
                    module.weight.data = dequantized.to(module.weight.device, module.weight.dtype)
                elif param_name == "bias":
                    module.bias.data = dequantized.to(module.bias.device, module.bias.dtype)

    def forward(self, *args, **kwargs):
        """Forward pass through the compressed model."""
        return self.original_model(*args, **kwargs)

    def get_compression_stats(self) -> dict[str, Any]:
        """Get compression statistics."""
        original_size = sum(p.numel() * p.element_size() for p in self.original_model.parameters())

        # Calculate compressed size
        compressed_size = 0
        for compressed_data in self.compressed_layers.values():
            if compressed_data.get("is_quantized", False):
                # BitNet quantized: 1.58 bits per parameter + scale overhead
                weights_size = np.array(compressed_data["weights"]).nbytes
                scale_size = np.array(compressed_data["scale"]).nbytes
                compressed_size += weights_size + scale_size
            else:
                # Unquantized parameters
                compressed_size += np.array(compressed_data["weights"]).nbytes

        compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0

        return {
            "original_size_mb": original_size / (1024 * 1024),
            "compressed_size_mb": compressed_size / (1024 * 1024),
            "compression_ratio": compression_ratio,
            "layers_compressed": len(self.compressed_layers),
            "quantization_stats": self.quantizer.quantization_stats,
        }


# ============================================================================
# Calibration Dataset
# ============================================================================


class CalibrationDataset(Dataset):
    """Dataset for calibration during compression."""

    def __init__(self, dataset_name: str, num_samples: int, tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Load calibration dataset
        if dataset_name == "openwebtext":
            # Use a subset of OpenWebText for calibration
            try:
                dataset = load_dataset("openwebtext", split="train", streaming=True)
                self.examples = self._prepare_text_samples(dataset, num_samples)
            except:
                # Fallback to a simpler dataset
                logger.warning("OpenWebText not available, using wikitext as fallback")
                dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
                self.examples = self._prepare_text_samples(dataset, num_samples)

        elif dataset_name == "c4":
            dataset = load_dataset("c4", "en", split="train", streaming=True)
            self.examples = self._prepare_text_samples(dataset, num_samples)

        elif dataset_name == "wikitext":
            dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
            self.examples = self._prepare_text_samples(dataset, num_samples)

        else:
            raise ValueError(f"Unknown calibration dataset: {dataset_name}")

        logger.info(f"Prepared {len(self.examples)} calibration samples")

    def _prepare_text_samples(self, dataset, num_samples: int) -> list[str]:
        """Prepare text samples from dataset."""
        samples = []

        for i, item in enumerate(dataset):
            if len(samples) >= num_samples:
                break

            # Extract text from different dataset formats
            if "text" in item and item["text"].strip():
                samples.append(item["text"])
            elif "content" in item and item["content"].strip():
                samples.append(item["content"])

        return samples[:num_samples]

    def __len__(self) -> int:
        return len(self.examples)

    def __getitem__(self, idx):
        text = self.examples[idx]

        # Tokenize
        encoding = self.tokenizer(
            text, truncation=True, padding="max_length", max_length=self.max_length, return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": encoding["input_ids"].squeeze(),  # For language modeling
        }


# ============================================================================
# Compression Pipeline
# ============================================================================


class BitNetCompressionPipeline:
    """
    Complete BitNet compression pipeline with calibration and fine-tuning.
    """

    def __init__(self, config: BitNetCompressionConfig):
        self.config = config
        self.device = torch.device(
            config.device if config.device != "auto" else "cuda" if torch.cuda.is_available() else "cpu"
        )
        self.quantizer = BitNetQuantizer(config)

    async def compress_model(self, model_path: str) -> dict[str, Any]:
        """
        Complete model compression pipeline.

        Args:
            model_path: Path to input model

        Returns:
            Compression results and statistics
        """
        logger.info(f"🗜️ Starting BitNet compression pipeline for {model_path}")

        # Load model and tokenizer
        model, tokenizer = self._load_model(model_path)

        # Pre-compression evaluation
        logger.info("Evaluating model before compression...")
        pre_compression_metrics = await self._evaluate_model(model, tokenizer, "pre_compression")

        # Create calibration dataset
        calibration_dataset = CalibrationDataset(
            self.config.calibration_dataset,
            self.config.calibration_samples,
            tokenizer,
            self.config.calibration_sequence_length,
        )

        # Perform calibration-aware compression
        logger.info("Performing calibration-aware compression...")
        compressed_model = self._compress_with_calibration(model, calibration_dataset)

        # Post-compression evaluation
        logger.info("Evaluating model after compression...")
        post_compression_metrics = await self._evaluate_model(compressed_model, tokenizer, "post_compression")

        # Fine-tune if enabled and accuracy drop is too high
        if self.config.enable_fine_tuning:
            accuracy_drop = pre_compression_metrics.get("perplexity", float("inf")) - post_compression_metrics.get(
                "perplexity", 0
            )

            if accuracy_drop > self.config.max_accuracy_drop:
                logger.info("Fine-tuning compressed model to recover accuracy...")
                compressed_model = await self._fine_tune_compressed_model(
                    compressed_model, tokenizer, calibration_dataset
                )

                # Final evaluation
                final_metrics = await self._evaluate_model(compressed_model, tokenizer, "post_fine_tune")
            else:
                final_metrics = post_compression_metrics
        else:
            final_metrics = post_compression_metrics

        # Save compressed model
        self._save_compressed_model(compressed_model, tokenizer)

        # Compile results
        compression_stats = compressed_model.get_compression_stats()

        results = {
            "success": True,
            "model_path": self.config.output_path,
            "compression_ratio": compression_stats["compression_ratio"],
            "original_size_mb": compression_stats["original_size_mb"],
            "compressed_size_mb": compression_stats["compressed_size_mb"],
            "layers_compressed": compression_stats["layers_compressed"],
            "quantization_stats": compression_stats["quantization_stats"],
            "pre_compression_metrics": pre_compression_metrics,
            "post_compression_metrics": post_compression_metrics,
            "final_metrics": final_metrics,
            "accuracy_preserved": final_metrics.get("perplexity", 0)
            <= pre_compression_metrics.get("perplexity", float("inf")) + self.config.max_accuracy_drop,
        }

        logger.info(f"✅ Compression complete: {compression_stats['compression_ratio']:.2f}x ratio")

        return results

    def _load_model(self, model_path: str) -> tuple[nn.Module, AutoTokenizer]:
        """Load model and tokenizer."""
        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path or model_path)

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForCausalLM.from_pretrained(
            model_path, torch_dtype=torch.float16 if self.config.device == "cuda" else torch.float32
        )

        model.to(self.device)

        return model, tokenizer

    def _compress_with_calibration(
        self, model: nn.Module, calibration_dataset: CalibrationDataset
    ) -> BitNetCompressedModel:
        """Compress model with calibration for better quantization."""
        logger.info("Performing calibration for optimal quantization...")

        # Set model to evaluation mode for calibration
        model.eval()

        # Create dataloader for calibration
        dataloader = DataLoader(calibration_dataset, batch_size=self.config.calibration_batch_size, shuffle=False)

        # Collect activation statistics during calibration
        activation_stats = {}

        def collect_stats_hook(module, input, output, name):
            if name not in activation_stats:
                activation_stats[name] = []
            if isinstance(output, torch.Tensor):
                activation_stats[name].append(output.detach().cpu())

        # Register hooks for collecting statistics
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear | nn.Conv1d | nn.Conv2d):
                hook = module.register_forward_hook(lambda m, i, o, n=name: collect_stats_hook(m, i, o, n))
                hooks.append(hook)

        # Run calibration samples through model
        with torch.no_grad():
            for i, batch in enumerate(tqdm(dataloader, desc="Calibration")):
                if i >= min(100, len(dataloader)):  # Limit calibration samples
                    break

                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)

                try:
                    _ = model(input_ids=input_ids, attention_mask=attention_mask)
                except Exception as e:
                    logger.warning(f"Calibration batch {i} failed: {e}")
                    continue

        # Remove hooks
        for hook in hooks:
            hook.remove()

        # Create compressed model with calibration-aware quantization
        compressed_model = BitNetCompressedModel(model, self.quantizer)

        logger.info("Calibration-aware compression completed")

        return compressed_model

    async def _evaluate_model(self, model: nn.Module, tokenizer, eval_prefix: str) -> dict[str, float]:
        """Evaluate model performance."""
        model.eval()

        # Simple perplexity evaluation
        eval_dataset = CalibrationDataset("wikitext", self.config.eval_samples, tokenizer, 256)

        dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False)

        total_loss = 0.0
        total_tokens = 0

        with torch.no_grad():
            for batch in tqdm(dataloader, desc=f"Evaluating ({eval_prefix})"):
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                labels = batch["labels"].to(self.device)

                try:
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

                    loss = outputs.loss
                    total_loss += loss.item() * input_ids.size(0)
                    total_tokens += input_ids.size(0)

                except Exception as e:
                    logger.warning(f"Evaluation batch failed: {e}")
                    continue

        perplexity = torch.exp(torch.tensor(total_loss / total_tokens)).item()

        return {"perplexity": perplexity, "eval_loss": total_loss / total_tokens, "eval_samples": total_tokens}

    async def _fine_tune_compressed_model(
        self, model: BitNetCompressedModel, tokenizer, training_dataset: CalibrationDataset
    ) -> BitNetCompressedModel:
        """Fine-tune compressed model to recover accuracy."""
        logger.info("Fine-tuning compressed model...")

        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(Path(self.config.output_path).parent / "fine_tune_checkpoints"),
            num_train_epochs=self.config.fine_tune_epochs,
            per_device_train_batch_size=self.config.calibration_batch_size,
            warmup_steps=self.config.warmup_steps,
            weight_decay=self.config.weight_decay,
            learning_rate=self.config.fine_tune_lr,
            logging_steps=10,
            save_steps=100,
            save_total_limit=2,
            remove_unused_columns=False,
            push_to_hub=False,
            fp16=self.config.mixed_precision,
            seed=self.config.seed,
            report_to=[],
        )

        # Create trainer
        trainer = Trainer(model=model, args=training_args, train_dataset=training_dataset, tokenizer=tokenizer)

        # Replace optimizer with Grokfast if enabled
        if self.config.enable_grokfast:
            base_optimizer = torch.optim.AdamW(
                model.parameters(), lr=self.config.fine_tune_lr, weight_decay=self.config.weight_decay
            )

            grokfast_optimizer = GrokfastOptimizer(
                base_optimizer, alpha=self.config.grokfast_ema_alpha, lamb=self.config.grokfast_lambda
            )

            trainer.optimizers = (grokfast_optimizer, None)

        # Fine-tune
        result = trainer.train()

        logger.info(f"Fine-tuning completed: {result.training_loss:.4f} final loss")

        return model

    def _save_compressed_model(self, model: BitNetCompressedModel, tokenizer):
        """Save compressed model to disk."""
        output_path = Path(self.config.output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        # Save the underlying model
        model.original_model.save_pretrained(output_path)
        tokenizer.save_pretrained(output_path)

        # Save compression metadata
        compression_stats = model.get_compression_stats()
        metadata = {
            "compression_method": "BitNet-1.58",
            "quantization_bits": self.config.quantization_bits,
            "compression_stats": compression_stats,
            "config": self.config.__dict__,
        }

        with open(output_path / "compression_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2, default=str)

        logger.info(f"Compressed model saved to {output_path}")


# ============================================================================
# Main Phase Controller
# ============================================================================


class BitNetCompressionPhase(PhaseController):
    """
    Phase 3: BitNet 1.58-bit Compression Controller

    Performs initial compression using BitNet quantization before main training.
    This reduces memory requirements during training while maintaining performance.
    """

    def __init__(self, config: BitNetCompressionConfig):
        self.config = config
        self.phase_name = "BitNet 1.58-bit Compression"
        self.phase_number = 3

        # Set random seeds
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)

    async def execute(self, input_model_path: str, **kwargs) -> PhaseResult:
        """Execute Phase 3: BitNet compression."""
        try:
            logger.info(f"🗜️ Starting {self.phase_name}")

            # Update config with input model path
            self.config.model_path = input_model_path

            # Create compression pipeline
            pipeline = BitNetCompressionPipeline(self.config)

            # Run compression
            compression_results = await pipeline.compress_model(input_model_path)

            # Determine success
            success = (
                compression_results["success"]
                and compression_results["compression_ratio"] >= 2.0
                and compression_results.get("accuracy_preserved", False)  # At least 2x compression
            )

            # Create phase result
            result = PhaseResult(
                phase_name=self.phase_name,
                success=success,
                model_path=self.config.output_path,
                metrics={
                    "compression_ratio": compression_results["compression_ratio"],
                    "original_size_mb": compression_results["original_size_mb"],
                    "compressed_size_mb": compression_results["compressed_size_mb"],
                    "layers_compressed": compression_results["layers_compressed"],
                    "pre_perplexity": compression_results["pre_compression_metrics"].get("perplexity", 0),
                    "post_perplexity": compression_results["final_metrics"].get("perplexity", 0),
                    "accuracy_preserved": compression_results.get("accuracy_preserved", False),
                    "quantization_method": "BitNet-1.58",
                    "sparsity_ratio": compression_results["quantization_stats"].get("sparsity_ratio", 0),
                },
                artifacts={
                    "compression_results": compression_results,
                    "quantization_stats": compression_results["quantization_stats"],
                    "config": self.config.__dict__,
                },
                duration_seconds=0,  # Will be calculated by orchestrator
                memory_usage_mb=0,  # Will be calculated by orchestrator
            )

            status = "✅ SUCCESS" if success else "⚠️  PARTIAL"
            logger.info(
                f"{status} - Compression: {compression_results['compression_ratio']:.2f}x, Accuracy preserved: {compression_results.get('accuracy_preserved', False)}"
            )

            return result

        except Exception as e:
            logger.exception(f"BitNet compression failed: {e}")

            return PhaseResult(
                phase_name=self.phase_name,
                success=False,
                model_path="",
                metrics={"error": str(e)},
                artifacts={},
                duration_seconds=0,
                memory_usage_mb=0,
                error_message=str(e),
            )


# ============================================================================
# Factory Function
# ============================================================================


def create_bitnet_compression_phase(
    model_path: str = "",
    output_path: str = "",
    target_compression_ratio: float = 8.0,
    calibration_samples: int = 1000,
    enable_fine_tuning: bool = True,
    enable_grokfast: bool = True,
    device: str = "auto",
    **kwargs,
) -> BitNetCompressionPhase:
    """
    Factory function to create BitNet compression phase.

    Args:
        model_path: Path to input model from Quiet-STaR
        output_path: Path for compressed model output
        target_compression_ratio: Target compression ratio
        calibration_samples: Number of calibration samples
        enable_fine_tuning: Enable fine-tuning after compression
        enable_grokfast: Enable Grokfast optimization
        device: Device to use
        **kwargs: Additional configuration options

    Returns:
        BitNetCompressionPhase: Configured phase controller
    """
    config = BitNetCompressionConfig(
        model_path=model_path,
        output_path=output_path,
        target_compression_ratio=target_compression_ratio,
        calibration_samples=calibration_samples,
        enable_fine_tuning=enable_fine_tuning,
        enable_grokfast=enable_grokfast,
        device=device,
        **kwargs,
    )

    return BitNetCompressionPhase(config)


# ============================================================================
# Example Usage
# ============================================================================


if __name__ == "__main__":

    async def main():
        # Example: Create and run BitNet compression phase
        phase = create_bitnet_compression_phase(
            model_path="./phase2_quietstar_output",
            output_path="./phase3_bitnet_output",
            target_compression_ratio=8.0,
            calibration_samples=500,  # Smaller for testing
            enable_fine_tuning=True,
        )

        result = await phase.execute("./phase2_quietstar_output")

        print(f"Phase Result: {result.success}")
        print(f"Compression Ratio: {result.metrics.get('compression_ratio', 0):.2f}x")
        print(f"Accuracy Preserved: {result.metrics.get('accuracy_preserved', False)}")
        print(f"Model Path: {result.model_path}")

    # Uncomment to run example
    # asyncio.run(main())
