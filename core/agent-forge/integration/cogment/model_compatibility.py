"""
Model Compatibility Utilities for Cogment Integration.

Ensures ACT halting mechanism and LTM memory dynamics are preserved 
during evolutionary merging operations and model transitions.
"""

import logging
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn

from core.agent_forge.models.cogment.core.model import Cogment
from core.agent_forge.models.cogment.core.config import CogmentConfig
from core.agent_forge.models.cogment.core.act_halting import ACTHalting

logger = logging.getLogger(__name__)


class CogmentCompatibilityValidator:
    """
    Validates and ensures compatibility of Cogment models during merging operations.
    
    Critical for preserving:
    - ACT halting mechanism functionality
    - LTM memory state and dynamics
    - Specialized head compatibility
    - Model architecture consistency
    """
    
    def __init__(self):
        self.compatibility_cache: Dict[str, Any] = {}
        logger.info("Initialized CogmentCompatibilityValidator")
    
    def check_merge_compatibility(self, models: List[Cogment]) -> List[str]:
        """
        Check compatibility issues before merging Cogment models.
        
        Args:
            models: List of Cogment models to be merged
            
        Returns:
            List of compatibility issues (empty if all compatible)
        """
        issues = []
        
        if len(models) < 2:
            issues.append("Need at least 2 models for merging")
            return issues
        
        # Check basic architecture compatibility
        base_config = models[0].config
        
        for i, model in enumerate(models[1:], 1):
            # Check core dimensions
            if model.config.d_model != base_config.d_model:
                issues.append(f"Model {i} d_model mismatch: {model.config.d_model} vs {base_config.d_model}")
            
            if model.config.n_head != base_config.n_head:
                issues.append(f"Model {i} n_head mismatch: {model.config.n_head} vs {base_config.n_head}")
            
            if model.config.n_layers != base_config.n_layers:
                issues.append(f"Model {i} n_layers mismatch: {model.config.n_layers} vs {base_config.n_layers}")
            
            if model.config.vocab_size != base_config.vocab_size:
                issues.append(f"Model {i} vocab_size mismatch: {model.config.vocab_size} vs {base_config.vocab_size}")
        
        # Check ACT compatibility
        act_issues = self._check_act_compatibility(models)
        issues.extend(act_issues)
        
        # Check LTM compatibility
        ltm_issues = self._check_ltm_compatibility(models)
        issues.extend(ltm_issues)
        
        # Check state dict compatibility
        state_issues = self._check_state_dict_compatibility(models)
        issues.extend(state_issues)
        
        if issues:
            logger.warning(f"Found {len(issues)} compatibility issues")
            for issue in issues:
                logger.warning(f"  - {issue}")
        else:
            logger.info("✅ All models are compatible for merging")
        
        return issues
    
    def _check_act_compatibility(self, models: List[Cogment]) -> List[str]:
        """Check ACT halting mechanism compatibility."""
        issues = []
        
        # Check ACT component presence
        for i, model in enumerate(models):
            if not hasattr(model, 'act_halting'):
                issues.append(f"Model {i} missing ACT halting component")
                continue
            
            if not isinstance(model.act_halting, ACTHalting):
                issues.append(f"Model {i} has invalid ACT halting type: {type(model.act_halting)}")
        
        # Check ACT parameter compatibility
        base_act = models[0].act_halting if hasattr(models[0], 'act_halting') else None
        if base_act is None:
            issues.append("Base model missing ACT halting")
            return issues
        
        for i, model in enumerate(models[1:], 1):
            if not hasattr(model, 'act_halting'):
                continue
            
            act = model.act_halting
            
            # Check threshold compatibility (should be similar for stable merging)
            if abs(act.threshold - base_act.threshold) > 0.1:
                issues.append(f"Model {i} ACT threshold significantly different: {act.threshold} vs {base_act.threshold}")
            
            # Check epsilon compatibility
            if abs(act.epsilon - base_act.epsilon) > 0.01:
                issues.append(f"Model {i} ACT epsilon significantly different: {act.epsilon} vs {base_act.epsilon}")
        
        return issues
    
    def _check_ltm_compatibility(self, models: List[Cogment]) -> List[str]:
        """Check LTM memory system compatibility."""
        issues = []
        
        # Check refinement core presence
        for i, model in enumerate(models):
            if not hasattr(model, 'refinement_core'):
                issues.append(f"Model {i} missing refinement core (LTM component)")
                continue
        
        # Check LTM parameter compatibility
        base_config = models[0].config
        
        for i, model in enumerate(models[1:], 1):
            config = model.config
            
            # Check LTM dimensions
            if config.ltm_capacity != base_config.ltm_capacity:
                issues.append(f"Model {i} LTM capacity mismatch: {config.ltm_capacity} vs {base_config.ltm_capacity}")
            
            if config.ltm_dim != base_config.ltm_dim:
                issues.append(f"Model {i} LTM dimension mismatch: {config.ltm_dim} vs {base_config.ltm_dim}")
            
            if config.memory_fusion_dim != base_config.memory_fusion_dim:
                issues.append(f"Model {i} memory fusion dim mismatch: {config.memory_fusion_dim} vs {base_config.memory_fusion_dim}")
        
        return issues
    
    def _check_state_dict_compatibility(self, models: List[Cogment]) -> List[str]:
        """Check state dictionary parameter compatibility."""
        issues = []
        
        base_state = models[0].state_dict()
        base_keys = set(base_state.keys())
        
        for i, model in enumerate(models[1:], 1):
            model_state = model.state_dict()
            model_keys = set(model_state.keys())
            
            # Check for missing keys
            missing_keys = base_keys - model_keys
            if missing_keys:
                issues.append(f"Model {i} missing parameters: {list(missing_keys)[:5]}...")
            
            # Check for extra keys
            extra_keys = model_keys - base_keys
            if extra_keys:
                issues.append(f"Model {i} has extra parameters: {list(extra_keys)[:5]}...")
            
            # Check parameter shapes for common keys
            common_keys = base_keys & model_keys
            shape_mismatches = []
            
            for key in common_keys:
                if base_state[key].shape != model_state[key].shape:
                    shape_mismatches.append(f"{key}: {base_state[key].shape} vs {model_state[key].shape}")
            
            if shape_mismatches:
                issues.append(f"Model {i} parameter shape mismatches: {shape_mismatches[:3]}...")
        
        return issues
    
    def resolve_compatibility_issues(
        self, 
        models: List[Cogment], 
        issues: List[str]
    ) -> List[Cogment]:
        """
        Attempt to resolve compatibility issues automatically.
        
        Args:
            models: List of models with compatibility issues
            issues: List of detected issues
            
        Returns:
            List of models with resolved issues (may be subset of input)
        """
        try:
            logger.info(f"Attempting to resolve {len(issues)} compatibility issues...")
            
            resolved_models = []
            
            # Find the most compatible base model
            base_model = self._find_best_base_model(models)
            resolved_models.append(base_model)
            
            # Try to fix each model to match the base
            for i, model in enumerate(models):
                if model is base_model:
                    continue
                
                try:
                    fixed_model = self._fix_model_compatibility(model, base_model, issues)
                    if fixed_model is not None:
                        resolved_models.append(fixed_model)
                        logger.info(f"✅ Fixed compatibility for model {i}")
                    else:
                        logger.warning(f"❌ Could not fix model {i}, excluding from merge")
                
                except Exception as e:
                    logger.warning(f"❌ Failed to fix model {i}: {e}")
                    continue
            
            logger.info(f"Resolved compatibility: {len(resolved_models)}/{len(models)} models usable")
            return resolved_models
            
        except Exception as e:
            logger.error(f"Compatibility resolution failed: {e}")
            return models  # Return original models as fallback
    
    def _find_best_base_model(self, models: List[Cogment]) -> Cogment:
        """Find the model that would serve as the best base for compatibility."""
        if len(models) == 1:
            return models[0]
        
        # Score models based on "completeness"
        scores = []
        
        for model in models:
            score = 0
            
            # Prefer models with complete ACT implementation
            if hasattr(model, 'act_halting') and isinstance(model.act_halting, ACTHalting):
                score += 10
            
            # Prefer models with complete refinement core
            if hasattr(model, 'refinement_core'):
                score += 10
            
            # Prefer models with reasonable parameter counts
            param_count = model.count_parameters()
            if 10_000_000 <= param_count <= 50_000_000:
                score += 5
            
            # Prefer models with standard configurations
            config = model.config
            if config.d_model in [256, 320, 384, 512]:
                score += 3
            
            scores.append(score)
        
        # Return model with highest score
        best_idx = scores.index(max(scores))
        logger.info(f"Selected model {best_idx} as base (score: {scores[best_idx]})")
        return models[best_idx]
    
    def _fix_model_compatibility(
        self, 
        model: Cogment, 
        base_model: Cogment, 
        issues: List[str]
    ) -> Optional[Cogment]:
        """Fix compatibility issues in a model to match the base model."""
        try:
            # Check if the model can be fixed
            model_issues = [issue for issue in issues if self._issue_affects_model(issue, model)]
            
            if not model_issues:
                return model  # No issues to fix
            
            # Try to create a fixed version
            fixed_config = self._create_compatible_config(model.config, base_model.config)
            fixed_model = Cogment(fixed_config)
            
            # Transfer compatible weights
            success = self._transfer_compatible_weights(model, fixed_model)
            
            if success:
                logger.info("✅ Created compatible model version")
                return fixed_model
            else:
                logger.warning("❌ Weight transfer failed during compatibility fix")
                return None
                
        except Exception as e:
            logger.error(f"Model compatibility fix failed: {e}")
            return None
    
    def _issue_affects_model(self, issue: str, model: Cogment) -> bool:
        """Check if a compatibility issue affects a specific model."""
        # Simple heuristic: check if model is mentioned in issue string
        return any(f"Model {i}" in issue for i in range(10))
    
    def _create_compatible_config(
        self, 
        model_config: CogmentConfig, 
        base_config: CogmentConfig
    ) -> CogmentConfig:
        """Create a config compatible with the base model."""
        # Start with base config and override selectively
        compatible_config = CogmentConfig()
        compatible_config.__dict__.update(base_config.__dict__)
        
        # Preserve some model-specific settings that don't affect merging
        compatible_config.dropout = model_config.dropout
        compatible_config.ponder_cost_weight = model_config.ponder_cost_weight
        
        return compatible_config
    
    def _transfer_compatible_weights(self, source: Cogment, target: Cogment) -> bool:
        """Transfer weights between compatible Cogment models."""
        try:
            source_dict = source.state_dict()
            target_dict = target.state_dict()
            
            transferred = 0
            total = len(target_dict)
            
            for key, target_param in target_dict.items():
                if key in source_dict and source_dict[key].shape == target_param.shape:
                    target_dict[key] = source_dict[key].clone()
                    transferred += 1
            
            target.load_state_dict(target_dict, strict=False)
            
            transfer_rate = transferred / total
            logger.info(f"Weight transfer: {transferred}/{total} ({transfer_rate:.1%})")
            
            return transfer_rate > 0.7  # Success if >70% transfer rate
            
        except Exception as e:
            logger.error(f"Weight transfer failed: {e}")
            return False
    
    def validate_cogment_model(self, model: Cogment) -> List[str]:
        """Validate a single Cogment model for issues."""
        issues = []
        
        try:
            # Check basic structure
            if not hasattr(model, 'backbone'):
                issues.append("Missing backbone component")
            
            if not hasattr(model, 'refinement_core'):
                issues.append("Missing refinement core component")
            
            if not hasattr(model, 'act_halting'):
                issues.append("Missing ACT halting component")
            
            # Check parameter count
            param_count = model.count_parameters()
            if param_count < 1_000_000:
                issues.append(f"Parameter count too low: {param_count:,}")
            elif param_count > 100_000_000:
                issues.append(f"Parameter count too high: {param_count:,}")
            
            # Test forward pass
            try:
                test_input = torch.randint(0, model.config.vocab_size, (1, 10))
                with torch.no_grad():
                    output = model(test_input)
                    
                if output.logits is None:
                    issues.append("Forward pass produces no logits")
                elif output.logits.shape[-1] != model.config.vocab_size:
                    issues.append(f"Output vocab size mismatch: {output.logits.shape[-1]} vs {model.config.vocab_size}")
                    
            except Exception as e:
                issues.append(f"Forward pass failed: {str(e)}")
            
            # Check ACT functionality
            if hasattr(model, 'act_halting'):
                try:
                    test_halt_probs = torch.rand(1, 5, 1)  # [B, T, 1]
                    test_outputs = torch.randn(1, 5, model.config.vocab_size)  # [B, T, vocab]
                    
                    with torch.no_grad():
                        final_output, ponder_cost, weights = model.act_halting(test_halt_probs, test_outputs)
                        
                    if final_output is None:
                        issues.append("ACT halting produces no output")
                    elif ponder_cost is None:
                        issues.append("ACT halting produces no ponder cost")
                        
                except Exception as e:
                    issues.append(f"ACT halting test failed: {str(e)}")
            
        except Exception as e:
            issues.append(f"Model validation failed: {str(e)}")
        
        return issues
    
    def resolve_model_issues(self, model: Cogment, issues: List[str]) -> Cogment:
        """Resolve issues in a single Cogment model."""
        try:
            logger.info(f"Resolving {len(issues)} model issues...")
            
            # For now, return the original model
            # In a full implementation, we could:
            # - Reinitialize missing components
            # - Fix configuration issues
            # - Repair broken state dictionaries
            
            resolved_model = model
            
            # Log which issues were resolved
            for issue in issues:
                if "Missing" in issue:
                    logger.warning(f"Issue '{issue}' requires manual intervention")
                else:
                    logger.info(f"Resolved: {issue}")
            
            return resolved_model
            
        except Exception as e:
            logger.error(f"Model issue resolution failed: {e}")
            return model
    
    def get_merge_safety_score(self, models: List[Cogment]) -> float:
        """
        Calculate a safety score for merging operations.
        
        Args:
            models: Models to be merged
            
        Returns:
            Score from 0.0 (unsafe) to 1.0 (perfectly safe)
        """
        if len(models) < 2:
            return 0.0
        
        issues = self.check_merge_compatibility(models)
        
        # Base score
        safety_score = 1.0
        
        # Penalize each issue type
        for issue in issues:
            if "mismatch" in issue.lower():
                safety_score -= 0.2  # Architecture mismatches are serious
            elif "missing" in issue.lower():
                safety_score -= 0.3  # Missing components are very serious
            elif "threshold" in issue.lower() or "epsilon" in issue.lower():
                safety_score -= 0.1  # ACT parameter differences are moderate
            else:
                safety_score -= 0.05  # Other issues are minor
        
        # Ensure score stays in [0, 1] range
        safety_score = max(0.0, min(1.0, safety_score))
        
        logger.info(f"Merge safety score: {safety_score:.2f} ({len(issues)} issues)")
        return safety_score
    
    def create_compatibility_report(self, models: List[Cogment]) -> Dict[str, Any]:
        """Create a comprehensive compatibility report."""
        report = {
            'models_analyzed': len(models),
            'compatibility_issues': self.check_merge_compatibility(models),
            'safety_score': self.get_merge_safety_score(models),
            'model_details': [],
            'recommendations': []
        }
        
        # Add details for each model
        for i, model in enumerate(models):
            model_issues = self.validate_cogment_model(model)
            model_detail = {
                'model_index': i,
                'parameter_count': model.count_parameters(),
                'config': model.config.__dict__,
                'issues': model_issues,
                'health_score': 1.0 - (len(model_issues) * 0.1)
            }
            report['model_details'].append(model_detail)
        
        # Generate recommendations
        if report['safety_score'] < 0.5:
            report['recommendations'].append("Merge safety score is low - consider resolving issues first")
        
        if len(report['compatibility_issues']) > 5:
            report['recommendations'].append("Many compatibility issues detected - consider using fewer models")
        
        if report['safety_score'] > 0.8:
            report['recommendations'].append("Models are highly compatible - merge should succeed")
        
        return report