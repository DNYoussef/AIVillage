#!/usr/bin/env python3
"""
Full 25M Cognate Model with ACT, LTM, and GrokFast Integration

This module creates the complete 25M parameter Cognate architecture with:
- ACT halting for adaptive computation
- Titans-style Long-Term Memory with cross-attention
- GrokFast optimization support
- Train-many/infer-few paradigm

Parameter Breakdown (~25M total):
- Embeddings: 32000 × 216 = 6.9M 
- 11 Layers: ~16M (attention + FFN + norms)
- Memory System: ~1.5M
- Heads (ACT + Edit): ~0.5M
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..', 'packages'))

from dataclasses import asdict, dataclass
import json
import logging
from pathlib import Path
from typing import Any

import torch
import torch.nn as nn

from agent_forge.models.cognate.halting_head import ACTScheduler
from agent_forge.models.cognate.memory_cross_attn import MemoryScheduler

# Import Cognate components
from agent_forge.models.cognate.refiner_core import CognateConfig, CognateRefiner

logger = logging.getLogger(__name__)


@dataclass
class Cognate25MConfig(CognateConfig):
    """Enhanced configuration for 25M Cognate models with variant specializations."""
    
    # Variant-specific configurations
    variant_name: str = "base"
    variant_description: str = "Base 25M Cognate model"
    
    # ACT variant parameters
    act_threshold: float = 0.99
    act_target_steps: float = 4.0
    act_variance_penalty: float = 0.01
    
    # Memory variant parameters  
    mem_capacity: int = 4096
    surprise_weight: float = 0.5
    novelty_weight: float = 0.5
    memory_gate_threshold: float = 0.5
    
    # Training paradigm
    train_max_steps: int = 16    # Train-many
    infer_max_steps: int = 6     # Infer-few
    
    # Model metadata
    model_version: str = "1.0.0"
    architecture_type: str = "cognate-act-ltm"
    parameter_count: int = 25000000  # Target parameter count


def create_standard_25m_config(variant_name: str = "base", seed: int = None) -> Cognate25MConfig:
    """Create standard 25M config - all models identical except weight initialization."""
    return Cognate25MConfig(
        variant_name=variant_name,
        variant_description=f"25M Cognate model #{variant_name}",
        
        # All models use identical hyperparameters
        act_threshold=0.99,
        act_target_steps=4.0,
        max_act_steps=16,
        
        # All models use identical memory settings
        mem_capacity=4096,
        surprise_weight=0.5,
        novelty_weight=0.5,
        entropy_threshold=0.8,
        mem_topk=4,
        
        # All models use identical training paradigm
        train_max_steps=16,
        infer_max_steps=6
    )


class Enhanced25MCognate(nn.Module):
    """
    Enhanced 25M parameter Cognate model with full ACT, LTM, and variant specialization.
    
    This extends the base CognateRefiner with:
    - Proper 25M parameter configuration
    - Variant-specific optimizations
    - Enhanced memory scheduling
    - Train-many/infer-few paradigm support
    """
    
    def __init__(self, config: Cognate25MConfig):
        super().__init__()
        self.config = config
        self.variant_name = config.variant_name
        
        # Core Cognate refiner with fixed 25M parameters
        self.cognate_core = CognateRefiner(config)
        
        # Enhanced schedulers for variant-specific behavior
        self.act_scheduler = ACTScheduler(
            min_steps=1,
            max_steps=config.max_act_steps,
            target_steps=config.act_target_steps,
            early_stopping_threshold=config.act_threshold
        )
        
        self.memory_scheduler = MemoryScheduler(
            read_policy=config.read_policy,
            write_policy=config.write_policy,
            entropy_threshold=config.entropy_threshold,
            surprise_threshold=config.surprise_threshold,
            novelty_threshold=config.novelty_threshold
        )
        
        # Training/inference mode tracking
        self.training_mode = True
        self.max_steps = config.train_max_steps
    
    def set_inference_mode(self, inference: bool = True):
        """Switch between train-many and infer-few paradigms."""
        self.training_mode = not inference
        self.max_steps = self.config.train_max_steps if self.training_mode else self.config.infer_max_steps
        logger.info(f"Switched to {'training' if self.training_mode else 'inference'} mode (max_steps={self.max_steps})")
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        memory_context: dict[str, Any] | None = None,
        return_dict: bool = True,
        **kwargs
    ) -> dict[str, Any]:
        """
        Enhanced forward pass with variant-specific optimizations.
        
        Args:
            input_ids: Token IDs (B, L)
            attention_mask: Attention mask (B, L) 
            memory_context: Optional memory context for continuation
            return_dict: Whether to return structured output
            
        Returns:
            Dictionary with logits, memory state, ACT info, and variant metrics
        """
        # Delegate to core cognate with proper max_steps
        original_max_steps = self.cognate_core.max_act_steps
        self.cognate_core.max_act_steps = self.max_steps
        
        try:
            # Core cognate forward pass
            outputs = self.cognate_core(
                input_ids=input_ids,
                attention_mask=attention_mask,
                memory_context=memory_context,
                return_dict=True,
                **kwargs
            )
            
            # Add variant-specific metrics
            if return_dict:
                outputs.update({
                    "variant_name": self.variant_name,
                    "training_mode": self.training_mode,
                    "max_steps_used": self.max_steps,
                    "variant_config": {
                        "act_threshold": self.config.act_threshold,
                        "memory_capacity": self.config.mem_capacity,
                        "surprise_weight": self.config.surprise_weight,
                        "novelty_weight": self.config.novelty_weight,
                    }
                })
            
            return outputs
            
        finally:
            # Restore original max_steps
            self.cognate_core.max_act_steps = original_max_steps
    
    def get_memory_stats(self) -> dict[str, Any]:
        """Get memory utilization and performance statistics."""
        memory_stats = {
            "capacity": self.config.mem_capacity,
            "variant_optimization": self.variant_name,
            "scheduler_stats": self.memory_scheduler.get_stats(),
            "act_stats": self.act_scheduler.get_statistics(),
        }
        
        # Add core memory bank stats if available
        if hasattr(self.cognate_core, 'memory_bank'):
            memory_stats.update({
                "memory_bank_size": len(self.cognate_core.memory_bank.items),
                "memory_utilization": len(self.cognate_core.memory_bank.items) / self.config.mem_capacity,
            })
        
        return memory_stats
    
    def count_parameters(self) -> dict[str, int]:
        """Count parameters by component for 25M validation."""
        param_counts = {}
        total = 0
        
        for name, module in self.named_children():
            if name == "cognate_core":
                # Break down core components
                core_counts = {}
                for core_name, core_module in module.named_children():
                    count = sum(p.numel() for p in core_module.parameters())
                    core_counts[core_name] = count
                    total += count
                param_counts["core_breakdown"] = core_counts
            else:
                count = sum(p.numel() for p in module.parameters())
                param_counts[name] = count
                total += count
        
        param_counts["total"] = total
        param_counts["target"] = self.config.parameter_count
        param_counts["accuracy"] = f"{total/self.config.parameter_count*100:.1f}%"
        
        return param_counts
    
    def save_pretrained(self, save_directory: str, **kwargs):
        """Save model in HuggingFace format with variant metadata."""
        save_path = Path(save_directory)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # Save model state dict
        torch.save(self.state_dict(), save_path / "pytorch_model.bin")
        
        # Save configuration
        config_dict = asdict(self.config)
        with open(save_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2)
        
        # Save parameter breakdown
        param_counts = self.count_parameters()
        with open(save_path / "parameter_breakdown.json", "w") as f:
            json.dump(param_counts, f, indent=2)
        
        # Save variant metadata
        variant_info = {
            "variant_name": self.variant_name,
            "variant_description": self.config.variant_description,
            "architecture_type": self.config.architecture_type,
            "model_version": self.config.model_version,
            "parameter_count": param_counts["total"],
            "specialization": {
                "act_threshold": self.config.act_threshold,
                "memory_capacity": self.config.mem_capacity,
                "surprise_weight": self.config.surprise_weight,
                "novelty_weight": self.config.novelty_weight,
            },
            "training_paradigm": {
                "train_max_steps": self.config.train_max_steps,
                "infer_max_steps": self.config.infer_max_steps,
            }
        }
        
        with open(save_path / "variant_info.json", "w") as f:
            json.dump(variant_info, f, indent=2)
        
        logger.info(f"Saved {self.variant_name} model ({param_counts['total']:,} params) to {save_directory}")
    
    @classmethod
    def from_pretrained(cls, model_path: str):
        """Load model from HuggingFace format."""
        model_path = Path(model_path)
        
        # Load configuration
        with open(model_path / "config.json") as f:
            config_dict = json.load(f)
        config = Cognate25MConfig(**config_dict)
        
        # Create model
        model = cls(config)
        
        # Load weights
        state_dict = torch.load(model_path / "pytorch_model.bin", map_location="cpu")
        model.load_state_dict(state_dict)
        
        logger.info(f"Loaded {config.variant_name} model from {model_path}")
        return model


def create_three_25m_models() -> list[Enhanced25MCognate]:
    """Factory function to create 3 identical Cognate 25M models with different random seeds."""
    
    models = []
    model_names = ["model-1", "model-2", "model-3"] 
    seeds = [42, 1337, 2023]  # Different seeds for different weight initialization
    
    for i, (name, seed) in enumerate(zip(model_names, seeds)):
        # Set seed for this model's weight initialization
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
        
        # Create identical config
        config = create_standard_25m_config(variant_name=name)
        model = Enhanced25MCognate(config)
        models.append(model)
        
        # Log parameter count validation
        param_counts = model.count_parameters()
        logger.info(f"Created {name} (seed={seed}): {param_counts['total']:,} params ({param_counts['accuracy']})")
    
    return models


def validate_25m_architecture():
    """Validate that all 3 models hit the 25M parameter target."""
    models = create_three_25m_models()
    
    results = {}
    for model in models:
        param_counts = model.count_parameters()
        results[model.variant_name] = {
            "total_params": param_counts["total"],
            "target_params": param_counts["target"],
            "accuracy_pct": float(param_counts["accuracy"].rstrip("%")),
            "within_tolerance": abs(param_counts["total"] - param_counts["target"]) / param_counts["target"] < 0.05
        }
    
    return results


if __name__ == "__main__":
    # Test the 25M architecture
    logging.basicConfig(level=logging.INFO)
    
    print("Creating and validating 25M Cognate models...")
    validation_results = validate_25m_architecture()
    
    print("\n=== 25M Parameter Validation ===")
    for variant, stats in validation_results.items():
        status = "✅ PASS" if stats["within_tolerance"] else "❌ FAIL"
        print(f"{variant}: {stats['total_params']:,} params ({stats['accuracy_pct']:.1f}%) {status}")
    
    print("\n=== Testing Forward Pass ===")
    models = create_three_25m_models()
    
    # Test each model
    for model in models:
        print(f"\nTesting {model.variant_name}...")
        
        # Create test input
        batch_size = 2
        seq_len = 64
        input_ids = torch.randint(0, 32000, (batch_size, seq_len))
        attention_mask = torch.ones(batch_size, seq_len)
        
        # Test training mode
        model.set_inference_mode(False)
        outputs = model(input_ids, attention_mask)
        
        print(f"  Training mode: {outputs['logits'].shape}, steps: {len(outputs['all_hidden_states'])}")
        
        # Test inference mode
        model.set_inference_mode(True)
        outputs = model(input_ids, attention_mask)
        
        print(f"  Inference mode: {outputs['logits'].shape}, steps: {len(outputs['all_hidden_states'])}")
        
        # Memory stats
        memory_stats = model.get_memory_stats()
        print(f"  Memory capacity: {memory_stats['capacity']}")
    
    print("\n✅ All 25M Cognate models validated successfully!")