"""Integration Test Suite for WhatsApp Wave Bridge
Tests core functionality and performance requirements.
"""

import time
from unittest.mock import Mock, patch

from app import app
import httpx
from language_support import auto_translate_flow, detect_language
from metrics import ResponseMetrics
from prompt_tuning import ABTestManager, PromptTuner
import pytest
from tutor_engine import AITutor

# Test configuration
TEST_BASE_URL = "http://localhost:8000"
PERFORMANCE_TARGET = 5.0  # seconds


class TestWhatsAppIntegration:
    """Test WhatsApp webhook integration."""

    @pytest.mark.asyncio
    async def test_webhook_response_time(self) -> None:
        """Test that webhook responds within target time."""
        # Mock Twilio webhook payload
        payload = {
            "Body": "Hello, I need help with math",
            "From": "whatsapp:+1234567890",
            "MessageSid": "test_message_123",
            "To": "whatsapp:+14155238886",
        }

        start_time = time.time()

        async with httpx.AsyncClient(app=app) as client:
            response = await client.post("/whatsapp/webhook", data=payload)

        response_time = time.time() - start_time

        # Assert response time meets target
        assert (
            response_time < PERFORMANCE_TARGET
        ), f"Response took {response_time:.2f}s, target is {PERFORMANCE_TARGET}s"
        assert response.status_code == 200
        assert "application/xml" in response.headers["content-type"]

        # Verify TwiML response format
        response_text = response.text
        assert "<Response>" in response_text
        assert "<Message>" in response_text
        assert "<Body>" in response_text

        print(f"‚úÖ Webhook response time: {response_time:.2f}s")

    @pytest.mark.asyncio
    async def test_greeting_variants(self) -> None:
        """Test A/B testing for greeting messages."""
        ab_manager = ABTestManager()

        # Test consistent assignment
        user_id = "test_user_123"
        variant1 = ab_manager.get_greeting_variant(user_id)
        variant2 = ab_manager.get_greeting_variant(user_id)

        assert variant1 == variant2, "User should get consistent variant"
        assert variant1 in ["enthusiastic", "professional", "friendly"]

        print(f"‚úÖ Greeting variant assignment: {variant1}")

    @pytest.mark.asyncio
    async def test_multi_language_support(self) -> None:
        """Test language detection and translation."""
        test_cases = [
            ("Hello, how are you?", "en"),
            ("Hola, ¬øc√≥mo est√°s?", "es"),
            ("‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?", "hi"),
            ("Bonjour, comment allez-vous?", "fr"),
        ]

        for text, expected_lang in test_cases:
            detected = await detect_language(text)

            # Language detection should be reasonably accurate
            assert detected is not None
            print(f"‚úÖ '{text[:20]}...' detected as: {detected}")

            # Test translation (if not English)
            if expected_lang != "en":
                start_time = time.time()
                translated = await auto_translate_flow(text, "en", expected_lang)
                translation_time = time.time() - start_time

                assert translated != text  # Should be different
                assert translation_time < 3.0  # Translation should be fast
                print(f"‚úÖ Translation time: {translation_time:.2f}s")


class TestAITutorEngine:
    """Test AI Tutor functionality."""

    @pytest.mark.asyncio
    async def test_subject_detection(self) -> None:
        """Test subject area detection."""
        tutor = AITutor()

        test_cases = [
            ("Help me solve this algebra equation", "mathematics"),
            ("Explain photosynthesis", "science"),
            ("How do I write a Python function?", "programming"),
            ("What happened in World War 2?", "history"),
            ("Random question about anything", "general"),
        ]

        for message, _expected_subject in test_cases:
            detected = tutor.detect_subject_interest(message)
            print(f"‚úÖ '{message}' -> {detected}")

            # Should detect correct subject or general
            assert detected in tutor.subject_experts

    @pytest.mark.asyncio
    async def test_response_generation_speed(self) -> None:
        """Test that response generation meets speed requirements."""
        tutor = AITutor()

        test_messages = [
            "What is 2 + 2?",
            "Explain gravity",
            "Help me with programming",
            "Tell me about history",
        ]

        for message in test_messages:
            start_time = time.time()

            # Mock the AI model response to test speed
            with patch.object(tutor, "anthropic_client") as mock_client:
                mock_response = Mock()
                mock_response.content = [Mock(text="Test response")]
                mock_client.messages.create.return_value = mock_response

                response = await tutor.generate_response(
                    user_message=message,
                    prompt_template="Test prompt: {user_message}",
                    language="en",
                    session_id="test_session",
                )

            response_time = time.time() - start_time

            assert response_time < PERFORMANCE_TARGET
            assert len(response) > 0
            print(f"‚úÖ Response for '{message}': {response_time:.2f}s")


class TestPerformanceMonitoring:
    """Test performance monitoring and metrics."""

    @pytest.mark.asyncio
    async def test_metrics_tracking(self) -> None:
        """Test metrics collection and reporting."""
        metrics = ResponseMetrics()

        # Simulate multiple responses
        test_data = [
            {"response_time": 1.5, "language": "en", "session_id": "test1"},
            {"response_time": 2.3, "language": "es", "session_id": "test2"},
            {"response_time": 4.8, "language": "hi", "session_id": "test3"},
            {"response_time": 1.2, "language": "en", "session_id": "test4"},
        ]

        for data in test_data:
            await metrics.update_metrics(data)

        # Get performance summary
        summary = await metrics.get_summary()

        assert "overall_performance" in summary
        assert summary["overall_performance"]["total_requests"] == len(test_data)
        assert "language_performance" in summary

        avg_time = summary["overall_performance"]["avg_response_time"]
        target_rate = summary["overall_performance"]["target_achievement_rate"]

        print(f"‚úÖ Average response time: {avg_time:.2f}s")
        print(f"‚úÖ Target achievement rate: {target_rate:.1%}")

    @pytest.mark.asyncio
    async def test_performance_alerts(self) -> None:
        """Test performance alerting system."""
        metrics = ResponseMetrics()

        # Simulate slow response that should trigger alert
        slow_response_data = {
            "response_time": 6.5,  # Exceeds 5s target
            "language": "en",
            "session_id": "slow_test",
        }

        # Track if alert was generated
        initial_alert_count = len(metrics.alert_history)

        await metrics.update_metrics(slow_response_data)

        # Should have generated an alert
        assert len(metrics.alert_history) > initial_alert_count

        latest_alert = metrics.alert_history[-1]
        assert latest_alert["type"] == "response_time_exceeded"
        assert latest_alert["response_time"] == 6.5

        print(f"‚úÖ Performance alert generated: {latest_alert['type']}")


class TestPromptOptimization:
    """Test W&B prompt tuning functionality."""

    @pytest.mark.asyncio
    async def test_prompt_variant_selection(self) -> None:
        """Test prompt variant selection logic."""
        tuner = PromptTuner()

        # Get optimized prompt
        prompt = await tuner.get_optimized_prompt(
            message_type="tutoring",
            language="en",
            context={"user_message": "Test question"},
        )

        assert prompt is not None
        assert len(prompt) > 0
        assert "{user_message}" in prompt  # Should be a template

        print(f"‚úÖ Selected prompt variant (first 50 chars): {prompt[:50]}...")

    @pytest.mark.asyncio
    async def test_performance_recording(self) -> None:
        """Test prompt performance recording."""
        tuner = PromptTuner()

        # Record performance metrics
        await tuner.record_prompt_performance(
            variant_id="test_variant",
            session_id="test_session",
            user_hash="test_user_hash",
            performance_metrics={
                "response_time": 2.5,
                "satisfaction": 0.8,
                "conversion": True,
                "language": "en",
            },
        )

        # Check that performance was recorded
        assert "test_variant" in tuner.performance_history
        assert len(tuner.performance_history["test_variant"]) > 0

        print("‚úÖ Prompt performance recorded successfully")


# Health check test
@pytest.mark.asyncio
async def test_health_endpoint() -> None:
    """Test service health endpoint."""
    async with httpx.AsyncClient(app=app) as client:
        response = await client.get("/health")

    assert response.status_code == 200

    health_data = response.json()
    assert health_data["status"] == "healthy"
    assert "timestamp" in health_data

    print("‚úÖ Health endpoint responding correctly")


# Metrics endpoint test
@pytest.mark.asyncio
async def test_metrics_endpoint() -> None:
    """Test metrics endpoint."""
    async with httpx.AsyncClient(app=app) as client:
        response = await client.get("/metrics")

    assert response.status_code == 200

    metrics_data = response.json()
    # Should return metrics even if no data yet
    assert isinstance(metrics_data, dict)

    print("‚úÖ Metrics endpoint responding correctly")


def run_performance_benchmark() -> None:
    """Run comprehensive performance benchmark."""
    print("\nüöÄ Running WhatsApp Wave Bridge Performance Benchmark")
    print("=" * 60)

    # Run the tests
    pytest.main([__file__, "-v", "--tb=short"])

    print("\nüìä Performance Summary:")
    print(f"‚Ä¢ Target Response Time: {PERFORMANCE_TARGET}s")
    print("‚Ä¢ Languages Supported: 10")
    print("‚Ä¢ A/B Test Variants: 6")
    print("‚Ä¢ Subject Areas: 6")
    print("\n‚úÖ WhatsApp Wave Bridge Sprint R-3+AF4 Complete!")


if __name__ == "__main__":
    run_performance_benchmark()
