# Planner pretraining configuration
vocab_size: 32000
d_model: 512
n_layers: 16
n_head: 8
max_seq_len: 2048
rope_base: 10000

# HRM parameters
max_H: 4
inner_T: 4

# Control tokens
control_tokens: 5
lambda_ctrl: 0.2

# Training
dropout: 0.0
tie_embeddings: true

# Training hyperparameters
learning_rate: 3e-4
weight_decay: 0.1
warmup_steps: 2000
max_steps: 50000
batch_size: 32
gradient_accumulation_steps: 4

# Scheduler
scheduler_type: "cosine"
min_lr_ratio: 0.1

# Logging
log_interval: 50
checkpoint_interval: 5000
