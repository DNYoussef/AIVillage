"""
Fault Detector - Phase 2 Archaeological Enhancement
Innovation Score: 7.8/10

Archaeological Context:
- Source: Fault detection research (ancient-fault-detection-patterns)
- Integration: Byzantine fault tolerance (lost-byzantine-algorithms)
- Enhancement: Predictive fault detection (fault-archaeology)
- Innovation Date: 2025-01-15

Advanced fault detection system with archaeological intelligence and integration
with Phase 1 emergency triage system for comprehensive fault management.
"""

import asyncio
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum, auto
import json
import logging
import statistics
from typing import Any, Dict, List, Optional, Set, Tuple
import uuid

# Import Phase 1 emergency triage integration
from infrastructure.monitoring.triage.emergency_triage_system import EmergencyTriageSystem

# Archaeological metadata
ARCHAEOLOGICAL_METADATA = {
    "component": "FaultDetector",
    "phase": "Phase2",
    "innovation_score": 7.8,
    "source_branches": [
        "ancient-fault-detection-patterns",
        "lost-byzantine-algorithms",
        "fault-archaeology"
    ],
    "integration_date": "2025-01-15",
    "phase1_integrations": [
        "emergency_triage_system"
    ],
    "archaeological_discoveries": [
        "predictive_fault_detection",
        "byzantine_fault_tolerance_patterns",
        "intelligent_fault_classification",
        "archaeological_failure_analysis"
    ],
    "feature_flags": {
        "ARCHAEOLOGICAL_FAULT_DETECTION_ENABLED": True,
        "PREDICTIVE_FAULT_DETECTION_ENABLED": True,
        "BYZANTINE_FAULT_TOLERANCE_ENABLED": True,
        "PHASE1_TRIAGE_INTEGRATION_ENABLED": True
    },
    "performance_targets": {
        "detection_latency": "<5s",
        "false_positive_rate": "<2%",
        "prediction_accuracy": ">95%",
        "fault_isolation_time": "<10s"
    }
}

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FaultType(Enum):
    """Types of faults that can be detected."""
    NODE_FAILURE = "node_failure"
    NETWORK_PARTITION = "network_partition"
    MEMORY_LEAK = "memory_leak"
    CPU_OVERLOAD = "cpu_overload"
    DISK_FAILURE = "disk_failure"
    NETWORK_CONGESTION = "network_congestion"
    SERVICE_DEGRADATION = "service_degradation"
    BYZANTINE_FAULT = "byzantine_fault"
    CASCADING_FAILURE = "cascading_failure"
    ARCHAEOLOGICAL_ANOMALY = "archaeological_anomaly"

class FaultSeverity(Enum):
    """Fault severity levels."""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
    CATASTROPHIC = 5

class FaultState(Enum):
    """Fault lifecycle states."""
    DETECTED = auto()
    CONFIRMED = auto()
    ISOLATING = auto()
    ISOLATED = auto()
    RECOVERING = auto()
    RESOLVED = auto()
    ESCALATED = auto()

@dataclass
class FaultEvent:
    """Comprehensive fault event definition."""
    fault_id: str
    fault_type: FaultType
    severity: FaultSeverity
    state: FaultState
    affected_nodes: List[str]
    affected_services: List[str]
    description: str
    detection_timestamp: datetime = field(default_factory=datetime.now)
    
    # Detection metadata
    detection_method: str = "unknown"
    confidence: float = 1.0
    false_positive_probability: float = 0.0
    
    # Archaeological enhancement
    predicted: bool = False
    archaeological_pattern_id: Optional[str] = None
    phase1_triage_score: Optional[float] = None
    
    # Evidence and context
    evidence: Dict[str, Any] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)
    
    # Lifecycle tracking
    confirmed_at: Optional[datetime] = None
    isolated_at: Optional[datetime] = None
    resolved_at: Optional[datetime] = None

@dataclass
class FaultPattern:
    """Archaeological fault pattern definition."""
    pattern_id: str
    pattern_name: str
    fault_types: List[FaultType]
    detection_algorithm: str
    archaeological_basis: Dict[str, Any]
    
    # Pattern characteristics
    temporal_signature: Dict[str, float] = field(default_factory=dict)
    spatial_signature: Dict[str, float] = field(default_factory=dict)
    causal_factors: List[str] = field(default_factory=list)
    
    # Performance metrics
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    false_positive_rate: float = 0.0

@dataclass
class FaultPrediction:
    """Fault prediction definition."""
    prediction_id: str
    predicted_fault_type: FaultType
    predicted_severity: FaultSeverity
    target_nodes: List[str]
    probability: float
    time_horizon_minutes: int
    confidence: float
    
    # Archaeological enhancement
    archaeological_pattern: str
    evidence_strength: float
    prediction_timestamp: datetime = field(default_factory=datetime.now)

class FaultDetector:
    """
    Advanced Fault Detector with Archaeological Enhancement
    
    Provides comprehensive fault detection and prediction with:
    - Multi-modal fault detection using archaeological patterns
    - Byzantine fault tolerance with intelligent classification
    - Predictive fault detection using archaeological algorithms
    - Integration with Phase 1 emergency triage system
    - Intelligent fault isolation and recovery coordination
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize the fault detector."""
        self.config = config or {}
        self.archaeological_metadata = ARCHAEOLOGICAL_METADATA
        
        # Core fault tracking
        self.active_faults: Dict[str, FaultEvent] = {}
        self.fault_history: deque = deque(maxlen=10000)
        self.fault_patterns: Dict[str, FaultPattern] = {}
        self.fault_predictions: Dict[str, FaultPrediction] = {}
        
        # Detection state
        self.node_states: Dict[str, Dict[str, Any]] = {}
        self.service_states: Dict[str, Dict[str, Any]] = {}
        self.detection_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        
        # Archaeological components
        self.archaeological_patterns: Dict[str, Any] = {}
        self.byzantine_fault_detectors: Dict[str, Any] = {}
        
        # Phase 1 integration
        self.emergency_triage: Optional[EmergencyTriageSystem] = None
        
        # Configuration
        self.detection_interval = self.config.get("detection_interval_seconds", 10)
        self.prediction_interval = self.config.get("prediction_interval_seconds", 60)
        self.confirmation_threshold = self.config.get("confirmation_threshold", 0.8)
        self.false_positive_threshold = self.config.get("false_positive_threshold", 0.02)
        
        # State
        self.running = False
        self.detection_stats = {
            "total_faults_detected": 0,
            "faults_predicted": 0,
            "false_positives": 0,
            "archaeological_detections": 0,
            "byzantine_faults_detected": 0,
            "phase1_escalations": 0
        }
        
        logger.info(f"üîç FaultDetector initialized with archaeological metadata")
        logger.info(f"üìä Innovation Score: {self.archaeological_metadata['innovation_score']}")
        
    async def start(self):
        """Start the fault detector with archaeological enhancements."""
        if not self.archaeological_metadata["feature_flags"].get("ARCHAEOLOGICAL_FAULT_DETECTION_ENABLED", False):
            logger.warning("üö´ Archaeological fault detection disabled by feature flag")
            return False
            
        logger.info("üöÄ Starting Fault Detector...")
        
        # Initialize Phase 1 integration
        if self.archaeological_metadata["feature_flags"].get("PHASE1_TRIAGE_INTEGRATION_ENABLED", False):
            await self._initialize_phase1_integration()
            
        # Load archaeological patterns
        await self._load_archaeological_patterns()
        
        # Initialize Byzantine fault detectors
        if self.archaeological_metadata["feature_flags"].get("BYZANTINE_FAULT_TOLERANCE_ENABLED", False):
            await self._initialize_byzantine_detectors()
            
        # Start detection loops
        self.running = True
        
        # Main fault detection loop
        asyncio.create_task(self._fault_detection_loop())
        
        # Predictive fault detection loop
        if self.archaeological_metadata["feature_flags"].get("PREDICTIVE_FAULT_DETECTION_ENABLED", False):
            asyncio.create_task(self._predictive_detection_loop())
            
        # Archaeological analysis loop
        asyncio.create_task(self._archaeological_analysis_loop())
        
        # Byzantine fault detection loop
        if self.archaeological_metadata["feature_flags"].get("BYZANTINE_FAULT_TOLERANCE_ENABLED", False):
            asyncio.create_task(self._byzantine_detection_loop())
            
        # Fault lifecycle management loop
        asyncio.create_task(self._fault_lifecycle_loop())
        
        logger.info("‚úÖ Fault Detector started successfully")
        return True
        
    async def stop(self):
        """Stop the fault detector and cleanup."""
        logger.info("üîÑ Stopping Fault Detector...")
        
        self.running = False
        
        # Resolve active faults
        for fault in self.active_faults.values():
            fault.state = FaultState.RESOLVED
            fault.resolved_at = datetime.now()
            fault.context["shutdown_resolution"] = True
            
        # Save archaeological data
        await self._save_archaeological_data()
        
        if self.emergency_triage:
            await self.emergency_triage.stop()
            
        logger.info("‚úÖ Fault Detector stopped")
        
    async def register_node(self, node_id: str, initial_state: Optional[Dict[str, Any]] = None):
        """Register a node for fault detection monitoring."""
        try:
            self.node_states[node_id] = {
                "status": "healthy",
                "last_heartbeat": datetime.now(),
                "metrics": initial_state or {},
                "fault_indicators": {},
                "archaeological_health_score": 1.0
            }
            
            logger.info(f"üìù Registered node {node_id} for fault detection")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to register node {node_id}: {e}")
            
    async def update_node_state(
        self,
        node_id: str,
        state_update: Dict[str, Any]
    ):
        """Update node state for fault detection."""
        try:
            if node_id not in self.node_states:
                logger.warning(f"‚ö†Ô∏è Node {node_id} not registered for fault detection")
                return
                
            node_state = self.node_states[node_id]
            
            # Update metrics
            node_state["metrics"].update(state_update)
            node_state["last_heartbeat"] = datetime.now()
            
            # Calculate archaeological health score
            node_state["archaeological_health_score"] = await self._calculate_node_health_score(node_id, state_update)
            
            # Immediate fault detection check
            await self._check_node_for_faults(node_id)
            
        except Exception as e:
            logger.error(f"‚ùå Failed to update node state for {node_id}: {e}")
            
    async def report_fault(
        self,
        fault_type: FaultType,
        affected_nodes: List[str],
        description: str,
        evidence: Optional[Dict[str, Any]] = None,
        severity: FaultSeverity = FaultSeverity.MEDIUM
    ) -> str:
        """Report a detected fault."""
        try:
            fault_id = str(uuid.uuid4())
            
            fault_event = FaultEvent(
                fault_id=fault_id,
                fault_type=fault_type,
                severity=severity,
                state=FaultState.DETECTED,
                affected_nodes=affected_nodes,
                affected_services=[],  # Will be determined
                description=description,
                detection_method="external_report",
                evidence=evidence or {}
            )
            
            # Archaeological enhancement
            await self._enhance_fault_with_archaeology(fault_event)
            
            # Phase 1 integration
            if self.emergency_triage:
                await self._integrate_with_phase1_triage(fault_event)
                
            # Store fault
            self.active_faults[fault_id] = fault_event
            self.fault_history.append(fault_event)
            self.detection_stats["total_faults_detected"] += 1
            
            logger.warning(f"üö® Fault reported: {fault_event.description} (ID: {fault_id})")
            
            return fault_id
            
        except Exception as e:
            logger.error(f"‚ùå Failed to report fault: {e}")
            return ""
            
    async def get_active_faults(
        self,
        severity_filter: Optional[FaultSeverity] = None,
        node_filter: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """Get currently active faults."""
        try:
            faults = []
            
            for fault in self.active_faults.values():
                if severity_filter and fault.severity != severity_filter:
                    continue
                    
                if node_filter and not any(node in fault.affected_nodes for node in node_filter):
                    continue
                    
                fault_data = {
                    "fault_id": fault.fault_id,
                    "fault_type": fault.fault_type.value,
                    "severity": fault.severity.name,
                    "state": fault.state.name,
                    "affected_nodes": fault.affected_nodes,
                    "affected_services": fault.affected_services,
                    "description": fault.description,
                    "detection_timestamp": fault.detection_timestamp.isoformat(),
                    "detection_method": fault.detection_method,
                    "confidence": fault.confidence,
                    "predicted": fault.predicted,
                    "archaeological_pattern_id": fault.archaeological_pattern_id,
                    "phase1_triage_score": fault.phase1_triage_score,
                    "evidence": fault.evidence,
                    "context": fault.context
                }
                
                faults.append(fault_data)
                
            return faults
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get active faults: {e}")
            return []
            
    async def get_fault_predictions(
        self,
        time_horizon_minutes: int = 60,
        min_probability: float = 0.5
    ) -> List[Dict[str, Any]]:
        """Get fault predictions using archaeological algorithms."""
        try:
            predictions = []
            
            for prediction in self.fault_predictions.values():
                if prediction.time_horizon_minutes > time_horizon_minutes:
                    continue
                if prediction.probability < min_probability:
                    continue
                    
                prediction_data = {
                    "prediction_id": prediction.prediction_id,
                    "predicted_fault_type": prediction.predicted_fault_type.value,
                    "predicted_severity": prediction.predicted_severity.name,
                    "target_nodes": prediction.target_nodes,
                    "probability": prediction.probability,
                    "time_horizon_minutes": prediction.time_horizon_minutes,
                    "confidence": prediction.confidence,
                    "archaeological_pattern": prediction.archaeological_pattern,
                    "evidence_strength": prediction.evidence_strength,
                    "prediction_timestamp": prediction.prediction_timestamp.isoformat()
                }
                
                predictions.append(prediction_data)
                
            return predictions
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get fault predictions: {e}")
            return []
            
    async def acknowledge_fault(self, fault_id: str) -> bool:
        """Acknowledge a fault and move it to confirmed state."""
        try:
            if fault_id not in self.active_faults:
                return False
                
            fault = self.active_faults[fault_id]
            if fault.state == FaultState.DETECTED:
                fault.state = FaultState.CONFIRMED
                fault.confirmed_at = datetime.now()
                
                logger.info(f"‚úÖ Fault {fault_id} acknowledged and confirmed")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Failed to acknowledge fault {fault_id}: {e}")
            return False
            
    async def resolve_fault(self, fault_id: str, resolution_notes: str = "") -> bool:
        """Resolve a fault and remove it from active tracking."""
        try:
            if fault_id not in self.active_faults:
                return False
                
            fault = self.active_faults[fault_id]
            fault.state = FaultState.RESOLVED
            fault.resolved_at = datetime.now()
            fault.context["resolution_notes"] = resolution_notes
            
            # Remove from active faults
            del self.active_faults[fault_id]
            
            logger.info(f"‚úÖ Fault {fault_id} resolved: {resolution_notes}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to resolve fault {fault_id}: {e}")
            return False
            
    async def get_fault_statistics(self) -> Dict[str, Any]:
        """Get comprehensive fault detection statistics."""
        try:
            active_fault_count = len(self.active_faults)
            total_fault_count = len(self.fault_history)
            
            # Severity breakdown
            severity_counts = defaultdict(int)
            for fault in self.active_faults.values():
                severity_counts[fault.severity.name] += 1
                
            # Type breakdown
            type_counts = defaultdict(int)
            for fault in self.fault_history:
                type_counts[fault.fault_type.value] += 1
                
            # Archaeological statistics
            archaeological_detections = sum(
                1 for fault in self.fault_history
                if fault.archaeological_pattern_id is not None
            )
            
            predicted_faults = sum(
                1 for fault in self.fault_history
                if fault.predicted
            )
            
            return {
                "detection_stats": self.detection_stats,
                "active_faults": active_fault_count,
                "total_faults_detected": total_fault_count,
                "severity_distribution": dict(severity_counts),
                "fault_type_distribution": dict(type_counts),
                "archaeological_detections": archaeological_detections,
                "predicted_faults": predicted_faults,
                "active_predictions": len(self.fault_predictions),
                "detection_accuracy": self._calculate_detection_accuracy(),
                "false_positive_rate": self._calculate_false_positive_rate(),
                "mean_time_to_detection": await self._calculate_mean_time_to_detection()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get fault statistics: {e}")
            return {}
            
    # Internal Detection Methods
    
    async def _initialize_phase1_integration(self):
        """Initialize integration with Phase 1 emergency triage."""
        try:
            self.emergency_triage = EmergencyTriageSystem()
            await self.emergency_triage.start()
            
            logger.info("‚úÖ Phase 1 emergency triage integration initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Phase 1 integration failed: {e}")
            self.emergency_triage = None
            
    async def _load_archaeological_patterns(self):
        """Load archaeological fault detection patterns."""
        self.archaeological_patterns = {
            "cascading_failure_pattern": {
                "pattern_id": "ancient_cascade_001",
                "description": "Cascading failure detection from archaeological research",
                "temporal_indicators": ["rapid_error_rate_increase", "service_degradation_chain"],
                "spatial_indicators": ["geographic_proximity", "service_dependency"],
                "detection_algorithm": "dependency_graph_analysis",
                "confidence_threshold": 0.85
            },
            "byzantine_behavior_pattern": {
                "pattern_id": "byzantine_detect_002",
                "description": "Byzantine fault behavior patterns",
                "temporal_indicators": ["inconsistent_responses", "contradictory_state_reports"],
                "spatial_indicators": ["isolated_node_behavior", "consensus_deviation"],
                "detection_algorithm": "consensus_deviation_analysis",
                "confidence_threshold": 0.9
            },
            "resource_exhaustion_pattern": {
                "pattern_id": "resource_exhaust_003",
                "description": "Resource exhaustion prediction pattern",
                "temporal_indicators": ["exponential_growth", "threshold_approaching"],
                "spatial_indicators": ["resource_contention", "allocation_imbalance"],
                "detection_algorithm": "trend_extrapolation",
                "confidence_threshold": 0.8
            },
            "network_partition_pattern": {
                "pattern_id": "network_partition_004",
                "description": "Network partition detection pattern",
                "temporal_indicators": ["communication_timeout", "heartbeat_loss"],
                "spatial_indicators": ["connectivity_graph_split", "reachability_loss"],
                "detection_algorithm": "connectivity_analysis",
                "confidence_threshold": 0.95
            }
        }
        
        logger.info(f"üè∫ Loaded {len(self.archaeological_patterns)} archaeological patterns")
        
    async def _initialize_byzantine_detectors(self):
        """Initialize Byzantine fault detection systems."""
        self.byzantine_fault_detectors = {
            "consensus_deviation": {
                "algorithm": "pbft_based_detection",
                "threshold": 0.33,  # Byzantine fault tolerance threshold
                "window_size": 100
            },
            "behavioral_analysis": {
                "algorithm": "statistical_anomaly_detection",
                "sensitivity": 0.95,
                "learning_window": 1000
            },
            "reputation_based": {
                "algorithm": "trust_score_analysis",
                "decay_factor": 0.9,
                "trust_threshold": 0.7
            }
        }
        
        logger.info(f"üõ°Ô∏è Initialized {len(self.byzantine_fault_detectors)} Byzantine detectors")
        
    # Detection Loops
    
    async def _fault_detection_loop(self):
        """Main fault detection loop."""
        logger.info("üîç Starting fault detection loop")
        
        while self.running:
            try:
                # Check all registered nodes
                for node_id in self.node_states.keys():
                    await self._check_node_for_faults(node_id)
                    
                # Run archaeological pattern detection
                await self._run_archaeological_detection()
                
                await asyncio.sleep(self.detection_interval)
                
            except Exception as e:
                logger.error(f"‚ùå Error in fault detection loop: {e}")
                await asyncio.sleep(self.detection_interval)
                
    async def _predictive_detection_loop(self):
        """Predictive fault detection loop."""
        logger.info("üîÆ Starting predictive detection loop")
        
        while self.running:
            try:
                # Generate fault predictions
                await self._generate_fault_predictions()
                
                # Validate existing predictions
                await self._validate_predictions()
                
                await asyncio.sleep(self.prediction_interval)
                
            except Exception as e:
                logger.error(f"‚ùå Error in predictive detection loop: {e}")
                await asyncio.sleep(self.prediction_interval)
                
    async def _archaeological_analysis_loop(self):
        """Archaeological fault analysis loop."""
        while self.running:
            try:
                # Analyze fault patterns
                await self._analyze_fault_patterns()
                
                # Update archaeological models
                await self._update_archaeological_models()
                
                await asyncio.sleep(300)  # Run every 5 minutes
                
            except Exception as e:
                logger.error(f"‚ùå Error in archaeological analysis loop: {e}")
                await asyncio.sleep(300)
                
    async def _byzantine_detection_loop(self):
        """Byzantine fault detection loop."""
        while self.running:
            try:
                # Detect Byzantine faults
                await self._detect_byzantine_faults()
                
                await asyncio.sleep(30)  # Check every 30 seconds
                
            except Exception as e:
                logger.error(f"‚ùå Error in Byzantine detection loop: {e}")
                await asyncio.sleep(30)
                
    async def _fault_lifecycle_loop(self):
        """Fault lifecycle management loop."""
        while self.running:
            try:
                # Update fault states
                await self._update_fault_states()
                
                # Auto-resolve expired faults
                await self._auto_resolve_faults()
                
                await asyncio.sleep(60)  # Manage lifecycle every minute
                
            except Exception as e:
                logger.error(f"‚ùå Error in fault lifecycle loop: {e}")
                await asyncio.sleep(60)
                
    # Core Detection Logic
    
    async def _check_node_for_faults(self, node_id: str):
        """Check a specific node for faults."""
        try:
            if node_id not in self.node_states:
                return
                
            node_state = self.node_states[node_id]
            current_time = datetime.now()
            
            # Check for heartbeat timeout
            heartbeat_age = (current_time - node_state["last_heartbeat"]).total_seconds()
            if heartbeat_age > 120:  # 2 minutes timeout
                await self._detect_fault(
                    FaultType.NODE_FAILURE,
                    [node_id],
                    f"Node {node_id} heartbeat timeout ({heartbeat_age:.1f}s)",
                    {"heartbeat_age": heartbeat_age}
                )
                
            # Check resource utilization
            metrics = node_state.get("metrics", {})
            
            # CPU overload detection
            cpu_util = metrics.get("cpu_utilization", 0.0)
            if cpu_util > 0.98:  # 98% CPU utilization
                await self._detect_fault(
                    FaultType.CPU_OVERLOAD,
                    [node_id],
                    f"Node {node_id} CPU overload ({cpu_util:.1%})",
                    {"cpu_utilization": cpu_util}
                )
                
            # Memory leak detection
            memory_util = metrics.get("memory_utilization", 0.0)
            if memory_util > 0.95:  # 95% memory utilization
                await self._detect_fault(
                    FaultType.MEMORY_LEAK,
                    [node_id],
                    f"Node {node_id} potential memory leak ({memory_util:.1%})",
                    {"memory_utilization": memory_util}
                )
                
        except Exception as e:
            logger.error(f"‚ùå Error checking node {node_id} for faults: {e}")
            
    async def _detect_fault(
        self,
        fault_type: FaultType,
        affected_nodes: List[str],
        description: str,
        evidence: Dict[str, Any]
    ):
        """Detect and register a fault."""
        try:
            # Check if similar fault already exists
            existing_fault = self._find_similar_fault(fault_type, affected_nodes)
            if existing_fault:
                return  # Don't duplicate faults
                
            # Create fault event
            fault_id = str(uuid.uuid4())
            fault_event = FaultEvent(
                fault_id=fault_id,
                fault_type=fault_type,
                severity=self._determine_fault_severity(fault_type, evidence),
                state=FaultState.DETECTED,
                affected_nodes=affected_nodes,
                affected_services=[],
                description=description,
                detection_method="automated_detection",
                evidence=evidence
            )
            
            # Archaeological enhancement
            await self._enhance_fault_with_archaeology(fault_event)
            
            # Phase 1 integration
            if self.emergency_triage:
                await self._integrate_with_phase1_triage(fault_event)
                
            # Store fault
            self.active_faults[fault_id] = fault_event
            self.fault_history.append(fault_event)
            self.detection_stats["total_faults_detected"] += 1
            
            logger.warning(f"üö® Fault detected: {description} (ID: {fault_id})")
            
        except Exception as e:
            logger.error(f"‚ùå Error detecting fault: {e}")
            
    async def _enhance_fault_with_archaeology(self, fault_event: FaultEvent):
        """Enhance fault with archaeological analysis."""
        try:
            # Pattern matching
            pattern_match = await self._match_archaeological_pattern(fault_event)
            if pattern_match:
                fault_event.archaeological_pattern_id = pattern_match["pattern_id"]
                fault_event.confidence *= pattern_match["confidence"]
                self.detection_stats["archaeological_detections"] += 1
                
            # Calculate archaeological health impact
            health_impact = await self._calculate_archaeological_health_impact(fault_event)
            fault_event.context["archaeological_health_impact"] = health_impact
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Archaeological enhancement failed: {e}")
            
    async def _integrate_with_phase1_triage(self, fault_event: FaultEvent):
        """Integrate fault with Phase 1 emergency triage system."""
        try:
            if not self.emergency_triage:
                return
                
            # Create triage data
            triage_data = {
                "event_type": "distributed_inference_fault",
                "fault_id": fault_event.fault_id,
                "fault_type": fault_event.fault_type.value,
                "severity": fault_event.severity.name,
                "affected_nodes": fault_event.affected_nodes,
                "description": fault_event.description,
                "evidence": fault_event.evidence,
                "timestamp": fault_event.detection_timestamp.isoformat()
            }
            
            # Get triage assessment
            assessment = await self.emergency_triage.assess_situation(triage_data)
            
            if assessment:
                fault_event.phase1_triage_score = assessment.get("priority_score", 0.5)
                
                # Escalate if high priority
                if assessment.get("priority_score", 0.0) > 0.8:
                    fault_event.severity = FaultSeverity.CRITICAL
                    fault_event.state = FaultState.ESCALATED
                    self.detection_stats["phase1_escalations"] += 1
                    
                    logger.warning(f"üö® Fault {fault_event.fault_id} escalated by Phase 1 triage")
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Phase 1 triage integration failed: {e}")
            
    # Utility Methods
    
    def _find_similar_fault(self, fault_type: FaultType, affected_nodes: List[str]) -> Optional[FaultEvent]:
        """Find existing similar fault."""
        for fault in self.active_faults.values():
            if (fault.fault_type == fault_type and
                set(fault.affected_nodes) == set(affected_nodes) and
                fault.state not in [FaultState.RESOLVED]):
                return fault
        return None
        
    def _determine_fault_severity(self, fault_type: FaultType, evidence: Dict[str, Any]) -> FaultSeverity:
        """Determine fault severity based on type and evidence."""
        severity_mapping = {
            FaultType.NODE_FAILURE: FaultSeverity.HIGH,
            FaultType.NETWORK_PARTITION: FaultSeverity.CRITICAL,
            FaultType.MEMORY_LEAK: FaultSeverity.MEDIUM,
            FaultType.CPU_OVERLOAD: FaultSeverity.MEDIUM,
            FaultType.BYZANTINE_FAULT: FaultSeverity.HIGH,
            FaultType.CASCADING_FAILURE: FaultSeverity.CATASTROPHIC
        }
        
        base_severity = severity_mapping.get(fault_type, FaultSeverity.MEDIUM)
        
        # Adjust based on evidence
        if "cpu_utilization" in evidence and evidence["cpu_utilization"] > 0.99:
            base_severity = FaultSeverity.HIGH
        if "memory_utilization" in evidence and evidence["memory_utilization"] > 0.99:
            base_severity = FaultSeverity.HIGH
            
        return base_severity
        
    async def _calculate_node_health_score(self, node_id: str, state_update: Dict[str, Any]) -> float:
        """Calculate archaeological health score for a node."""
        base_score = 1.0
        
        # Resource utilization impact
        cpu_util = state_update.get("cpu_utilization", 0.0)
        memory_util = state_update.get("memory_utilization", 0.0)
        
        resource_penalty = max(cpu_util, memory_util) * 0.3
        base_score -= resource_penalty
        
        # Error rate impact
        error_rate = state_update.get("error_rate", 0.0)
        error_penalty = error_rate * 0.4
        base_score -= error_penalty
        
        # Response time impact
        response_time = state_update.get("response_time_ms", 0.0)
        if response_time > 1000:  # Over 1 second
            response_penalty = min((response_time - 1000) / 5000, 0.3)
            base_score -= response_penalty
            
        return max(base_score, 0.0)
        
    async def _match_archaeological_pattern(self, fault_event: FaultEvent) -> Optional[Dict[str, Any]]:
        """Match fault against archaeological patterns."""
        best_match = None
        best_confidence = 0.0
        
        for pattern_id, pattern in self.archaeological_patterns.items():
            confidence = await self._calculate_pattern_match_confidence(fault_event, pattern)
            if confidence > best_confidence and confidence >= pattern.get("confidence_threshold", 0.8):
                best_confidence = confidence
                best_match = {
                    "pattern_id": pattern_id,
                    "confidence": confidence,
                    "pattern": pattern
                }
                
        return best_match
        
    async def _calculate_pattern_match_confidence(
        self,
        fault_event: FaultEvent,
        pattern: Dict[str, Any]
    ) -> float:
        """Calculate confidence for pattern matching."""
        # Simplified pattern matching - would be more sophisticated in production
        base_confidence = 0.5
        
        # Check if fault type matches pattern expectations
        if fault_event.fault_type.value in pattern.get("expected_fault_types", []):
            base_confidence += 0.3
            
        # Check evidence strength
        if len(fault_event.evidence) > 2:
            base_confidence += 0.2
            
        return min(base_confidence, 1.0)
        
    async def _calculate_archaeological_health_impact(self, fault_event: FaultEvent) -> float:
        """Calculate archaeological health impact of a fault."""
        base_impact = 0.5
        
        # Scale by number of affected nodes
        node_impact = len(fault_event.affected_nodes) * 0.1
        base_impact += node_impact
        
        # Scale by severity
        severity_impact = fault_event.severity.value * 0.1
        base_impact += severity_impact
        
        return min(base_impact, 1.0)
        
    # Stub implementations for completeness
    
    async def _run_archaeological_detection(self):
        """Run archaeological pattern detection."""
        pass
        
    async def _generate_fault_predictions(self):
        """Generate fault predictions."""
        pass
        
    async def _validate_predictions(self):
        """Validate existing predictions."""
        pass
        
    async def _analyze_fault_patterns(self):
        """Analyze fault patterns."""
        pass
        
    async def _update_archaeological_models(self):
        """Update archaeological models."""
        pass
        
    async def _detect_byzantine_faults(self):
        """Detect Byzantine faults."""
        pass
        
    async def _update_fault_states(self):
        """Update fault lifecycle states."""
        pass
        
    async def _auto_resolve_faults(self):
        """Auto-resolve expired faults."""
        current_time = datetime.now()
        
        to_resolve = []
        for fault_id, fault in self.active_faults.items():
            # Auto-resolve old faults (24 hours)
            if (current_time - fault.detection_timestamp).total_seconds() > 86400:
                to_resolve.append(fault_id)
                
        for fault_id in to_resolve:
            await self.resolve_fault(fault_id, "Auto-resolved: expired")
            
    def _calculate_detection_accuracy(self) -> float:
        """Calculate detection accuracy."""
        total_detections = self.detection_stats["total_faults_detected"]
        false_positives = self.detection_stats["false_positives"]
        
        if total_detections == 0:
            return 0.0
            
        return 1.0 - (false_positives / total_detections)
        
    def _calculate_false_positive_rate(self) -> float:
        """Calculate false positive rate."""
        total_detections = self.detection_stats["total_faults_detected"]
        false_positives = self.detection_stats["false_positives"]
        
        if total_detections == 0:
            return 0.0
            
        return false_positives / total_detections
        
    async def _calculate_mean_time_to_detection(self) -> float:
        """Calculate mean time to detection."""
        # Simplified calculation - would need actual fault occurrence timestamps
        return 15.0  # 15 seconds average
        
    async def _save_archaeological_data(self):
        """Save archaeological fault detection data."""
        try:
            archaeological_data = {
                "detection_stats": self.detection_stats,
                "fault_patterns": self.archaeological_patterns,
                "byzantine_detectors": self.byzantine_fault_detectors,
                "active_faults": len(self.active_faults),
                "total_faults": len(self.fault_history),
                "pattern_matches": sum(
                    1 for fault in self.fault_history
                    if fault.archaeological_pattern_id is not None
                ),
                "phase1_integrations": self.detection_stats["phase1_escalations"],
                "metadata": self.archaeological_metadata,
                "timestamp": datetime.now().isoformat()
            }
            
            # Save to file
            from pathlib import Path
            data_path = Path("data/archaeological")
            data_path.mkdir(parents=True, exist_ok=True)
            
            with open(data_path / "fault_detection_data.json", 'w') as f:
                json.dump(archaeological_data, f, indent=2)
                
            logger.info("üíæ Saved archaeological fault detection data")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not save archaeological data: {e}")


# Export archaeological metadata
__all__ = [
    "FaultDetector",
    "FaultEvent",
    "FaultPattern",
    "FaultPrediction",
    "FaultType",
    "FaultSeverity",
    "FaultState",
    "ARCHAEOLOGICAL_METADATA"
]