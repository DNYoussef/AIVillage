#!/usr/bin/env python3
"""Validate downloaded Magi seed models with smoke tests"""

import json
from pathlib import Path
import re
import sys
import time
import traceback

try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import torch/transformers: {e}")
    TORCH_AVAILABLE = False


def validate_json_output(text: str, expected_function: str = "get_weather") -> dict:
    """Validate if output contains valid JSON with expected function call"""
    result = {
        "has_json": False,
        "valid_json": False,
        "correct_function": False,
        "correct_args": False,
        "parsed_json": None,
    }

    # Look for JSON-like content
    json_patterns = [
        r'\{[^{}]*"tool_call"[^{}]*\}',
        r'\{[^{}]*"name"[^{}]*"get_weather"[^{}]*\}',
        r"\{.*?\}",
    ]

    for pattern in json_patterns:
        matches = re.findall(pattern, text, re.DOTALL)
        if matches:
            result["has_json"] = True
            for match in matches:
                try:
                    parsed = json.loads(match)
                    result["valid_json"] = True
                    result["parsed_json"] = parsed

                    # Check for function call structure
                    if "tool_call" in parsed:
                        tool_call = parsed["tool_call"]
                        if tool_call.get("name") == expected_function:
                            result["correct_function"] = True
                            if "arguments" in tool_call:
                                args = tool_call["arguments"]
                                if args.get("city") == "Paris" and "C" in str(
                                    args.get("unit", "")
                                ):
                                    result["correct_args"] = True
                    elif parsed.get("name") == expected_function:
                        result["correct_function"] = True
                        if "Paris" in str(parsed) and "C" in str(parsed):
                            result["correct_args"] = True
                    break
                except json.JSONDecodeError:
                    continue

    return result


def test_model(model_path: Path, category: str) -> dict:
    """Test a single model with category-specific prompts"""
    result = {
        "model_path": str(model_path),
        "category": category,
        "load_ok": False,
        "tokenizer_ok": False,
        "generation_ok": False,
        "test_results": {},
        "error": None,
        "model_size_mb": 0,
        "tokens_per_sec": 0,
    }

    if not TORCH_AVAILABLE:
        result["error"] = "PyTorch/transformers not available"
        return result

    try:
        # Calculate model size
        total_size = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())
        result["model_size_mb"] = total_size / (1024 * 1024)

        print(f"  Loading tokenizer from {model_path}...")
        tokenizer = AutoTokenizer.from_pretrained(
            str(model_path), trust_remote_code=True
        )
        result["tokenizer_ok"] = True

        print(f"  Loading model from {model_path}...")
        model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            device_map="cpu",
            torch_dtype=torch.float32,
            trust_remote_code=True,
        )
        result["load_ok"] = True

        # Define category-specific tests
        if category == "coding":
            prompt = "Write a Python function to check if a string is a palindrome."

            def expected_check(x):
                return "def " in x and ("palindrome" in x.lower() or "return" in x)

        elif category == "math":
            prompt = "If a train travels 60 km in 1.5 hours, what is its average speed in km/h? Show reasoning briefly."

            def expected_check(x):
                return any(str(i) in x for i in [40, "40"]) and (
                    "km/h" in x or "speed" in x.lower()
                )

        elif category == "tools":
            prompt = """You are a tool-use model. Call the function in JSON only.
Function: {"name":"get_weather","parameters":{"city":"string","unit":"string"}}
Task: Get weather for Paris in Celsius.
Return JSON: {"tool_call":{"name":"get_weather","arguments":{"city":"Paris","unit":"C"}}}"""

            def expected_check(x):
                return validate_json_output(x)["correct_function"]
        else:
            prompt = "Hello, how are you?"

            def expected_check(x):
                return len(x.strip()) > 0

        # Run generation test
        print(f"  Testing {category} generation...")
        inputs = tokenizer(prompt, return_tensors="pt")

        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
            )

        generation_time = time.time() - start_time
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Remove prompt from output
        if prompt in output_text:
            generated_text = output_text[len(prompt) :].strip()
        else:
            generated_text = output_text.strip()

        # Calculate tokens per second
        output_tokens = len(outputs[0]) - len(inputs.input_ids[0])
        result["tokens_per_sec"] = (
            output_tokens / generation_time if generation_time > 0 else 0
        )

        result["generation_ok"] = True
        result["test_results"] = {
            "prompt": prompt,
            "generated_text": generated_text[:500] + "..."
            if len(generated_text) > 500
            else generated_text,
            "generation_time": generation_time,
            "output_tokens": int(output_tokens),
            "meets_criteria": expected_check(generated_text),
        }

        # Special handling for tools category
        if category == "tools":
            json_validation = validate_json_output(generated_text)
            result["test_results"]["json_validation"] = json_validation
            result["test_results"]["meets_criteria"] = json_validation[
                "has_json"
            ]  # Relaxed criteria

        print(
            f"    ‚úì Generated {output_tokens} tokens in {generation_time:.2f}s ({result['tokens_per_sec']:.1f} tok/s)"
        )

    except Exception as e:
        result["error"] = str(e)
        print(f"    ‚úó Error: {e}")
        traceback.print_exc()

    return result


def main():
    """Main validation function"""
    if not TORCH_AVAILABLE:
        print("ERROR: PyTorch and transformers are required but not available")
        print("Install with: pip install torch transformers")
        return None

    print("=== MAGI SEED MODEL VALIDATION ===")

    # Define expected models
    expected_models = {
        "coding": "Qwen--Qwen2.5-Coder-1.5B",
        "math": "Qwen--Qwen2.5-Math-1.5B-Instruct",
        "tools": "Qwen--Qwen2.5-1.5B-Instruct",
    }

    models_base_path = Path("../models/seeds/magi")
    results = {}

    for category, _expected_dir in expected_models.items():
        print(f"\n=== Testing {category.upper()} Model ===")

        # Find the model directory
        category_path = models_base_path / category
        model_dirs = list(category_path.glob("*"))

        if not model_dirs:
            print(f"  ‚úó No model found in {category_path}")
            results[category] = {
                "error": f"No model directory found in {category_path}",
                "load_ok": False,
            }
            continue

        # Use the first (and hopefully only) model directory
        model_path = model_dirs[0]
        print(f"  Found model at: {model_path}")

        # Validate the model
        result = test_model(model_path, category)
        results[category] = result

    # Save validation results
    with open("validation_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)

    # Print summary report
    print("\n=== VALIDATION SUMMARY ===")
    success_count = 0
    total_size_mb = 0

    for category, result in results.items():
        if result.get("load_ok", False):
            success_count += 1
            size = result.get("model_size_mb", 0)
            total_size_mb += size
            gen_status = "‚úì" if result.get("generation_ok", False) else "‚úó"
            criteria_status = (
                "‚úì"
                if result.get("test_results", {}).get("meets_criteria", False)
                else "‚úó"
            )

            print(
                f"{category.upper()}: Load ‚úì | Gen {gen_status} | Criteria {criteria_status} | "
                f"{size:.0f}MB | {result.get('tokens_per_sec', 0):.1f} tok/s"
            )
        else:
            print(
                f"{category.upper()}: Load ‚úó | Error: {result.get('error', 'Unknown')}"
            )

    print(f"\nOverall: {success_count}/3 models loaded successfully")
    print(f"Total size on disk: {total_size_mb:.0f}MB")

    if success_count == 3:
        print("\nüéâ MAGI_SEEDS_READY: 3 models downloaded and validated!")
        return True
    print(f"\n‚ö†Ô∏è  Validation incomplete: {3 - success_count} models failed")
    return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
