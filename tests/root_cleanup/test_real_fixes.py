#!/usr/bin/env python3
"""
Test Real Implementation Fixes

Minimal test to verify that the core issues are resolved:
1. Memory management works without segfaults
2. W&B handles authentication gracefully
3. Real benchmarking replaces simulation
4. Systems run without crashes

This tests the fixes before attempting full deployment.
"""

import asyncio
import logging
import sys

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


async def test_memory_management():
    """Test memory management utilities."""
    logger.info("Testing memory management...")

    try:
        from agent_forge.memory_manager import memory_manager

        # Test memory stats
        stats = memory_manager.get_memory_stats()
        logger.info(f"Memory stats: {stats}")

        # Test memory guard
        with memory_manager.memory_guard("test_operation"):
            logger.info("Memory guard working")

        logger.info("‚úÖ Memory management test passed")
        return True

    except Exception as e:
        logger.error(f"‚ùå Memory management test failed: {e}")
        return False


async def test_wandb_manager():
    """Test W&B manager with graceful fallback."""
    logger.info("Testing W&B manager...")

    try:
        from agent_forge.wandb_manager import init_wandb, log_metrics, wandb_manager

        # Test authentication setup (should handle missing keys gracefully)
        auth_success = wandb_manager.setup_authentication()
        logger.info(f"W&B auth setup: {auth_success}")

        # Test initialization (should fallback to offline mode if needed)
        init_success = init_wandb(project="test-agent-forge")
        logger.info(f"W&B init: {init_success}")

        # Test logging (should work in offline mode)
        log_metrics({"test_metric": 0.85})
        logger.info("Metrics logged successfully")

        logger.info("‚úÖ W&B manager test passed")
        return True

    except Exception as e:
        logger.error(f"‚ùå W&B manager test failed: {e}")
        return False


async def test_real_benchmark():
    """Test real benchmarking (minimal)."""
    logger.info("Testing real benchmark...")

    try:
        from agent_forge.real_benchmark import create_real_benchmark

        # Test with a small model that should load quickly
        model_path = "microsoft/DialoGPT-small"  # 117M parameters
        benchmark = create_real_benchmark(model_path, "test_model")

        # Test model loading
        benchmark.load_model()
        logger.info("Model loaded successfully")

        # Test simple generation
        response = benchmark.generate_response("Hello", max_length=10)
        logger.info(f"Generated response: {response}")

        # Cleanup
        del benchmark

        logger.info("‚úÖ Real benchmark test passed")
        return True

    except Exception as e:
        logger.error(f"‚ùå Real benchmark test failed: {e}")
        return False


async def test_quietstar_fixes():
    """Test Quiet-STaR fixes without full execution."""
    logger.info("Testing Quiet-STaR fixes...")

    try:
        from agent_forge.quietstar_baker import QuietSTaRBaker, QuietSTaRConfig

        # Create minimal config
        config = QuietSTaRConfig(
            model_path="microsoft/DialoGPT-small",  # Small model for testing
            output_path="./test_output",
            eval_samples=5,  # Minimal samples
            max_thought_length=16,  # Reduced for testing
        )

        # Test baker initialization
        baker = QuietSTaRBaker(config)
        logger.info("QuietSTaR Baker initialized")

        # Test W&B initialization (should handle gracefully)
        baker.initialize_wandb()
        logger.info("W&B initialization handled")

        logger.info("‚úÖ Quiet-STaR fixes test passed")
        return True

    except Exception as e:
        logger.error(f"‚ùå Quiet-STaR fixes test failed: {e}")
        return False


async def test_evolution_fixes():
    """Test evolution benchmarking fixes without full run."""
    logger.info("Testing evolution fixes...")

    try:
        # Import the fixed evolution class
        sys.path.append("./scripts")
        from run_50gen_evolution import Enhanced50GenEvolutionMerger

        # Create minimal merger for testing
        merger = Enhanced50GenEvolutionMerger()
        merger.max_generations = 1  # Only test 1 generation
        merger.population_size = 2  # Minimal population

        logger.info("Evolution merger initialized")

        # Test that it's using async now
        if hasattr(merger, "enhanced_benchmark_model"):
            import inspect

            is_async = inspect.iscoroutinefunction(merger.enhanced_benchmark_model)
            logger.info(f"Benchmarking is async: {is_async}")

            if is_async:
                logger.info("‚úÖ Evolution uses real async benchmarking")
            else:
                logger.warning("‚ö†Ô∏è Evolution benchmarking may still be synchronous")

        logger.info("‚úÖ Evolution fixes test passed")
        return True

    except Exception as e:
        logger.error(f"‚ùå Evolution fixes test failed: {e}")
        return False


async def run_comprehensive_test():
    """Run all tests to verify fixes."""
    logger.info("=" * 60)
    logger.info("TESTING REAL IMPLEMENTATION FIXES")
    logger.info("=" * 60)

    test_results = []

    # Run all tests
    tests = [
        ("Memory Management", test_memory_management),
        ("W&B Manager", test_wandb_manager),
        ("Real Benchmark", test_real_benchmark),
        ("Quiet-STaR Fixes", test_quietstar_fixes),
        ("Evolution Fixes", test_evolution_fixes),
    ]

    for test_name, test_func in tests:
        logger.info(f"\n--- Testing {test_name} ---")
        try:
            result = await test_func()
            test_results.append((test_name, result))
        except Exception as e:
            logger.error(f"Test {test_name} crashed: {e}")
            test_results.append((test_name, False))

    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("TEST RESULTS SUMMARY")
    logger.info("=" * 60)

    passed = 0
    for test_name, result in test_results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        logger.info(f"{test_name}: {status}")
        if result:
            passed += 1

    logger.info(f"\nOverall: {passed}/{len(test_results)} tests passed")

    if passed == len(test_results):
        logger.info("üéâ ALL TESTS PASSED - Real implementations are working!")
        logger.info("Ready to run actual training without simulations.")
    else:
        logger.warning("‚ö†Ô∏è Some tests failed. Review issues before proceeding.")

    return passed == len(test_results)


if __name__ == "__main__":
    # Run the comprehensive test
    success = asyncio.run(run_comprehensive_test())

    if success:
        print("\nüöÄ READY FOR REAL EXECUTION!")
        print("The fixes are working. You can now run:")
        print("1. Real evolution: python scripts/run_50gen_evolution.py")
        print("2. Real magi training: python -m agent_forge.training.magi_specialization")
        print("3. Real benchmarking: python -m agent_forge.real_benchmark")
    else:
        print("\n‚ö†Ô∏è ISSUES DETECTED!")
        print("Review the test failures above before proceeding.")
        sys.exit(1)
