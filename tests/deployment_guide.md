# TDD London School Testing Infrastructure Deployment Guide

## Overview

This guide provides step-by-step instructions for deploying the standardized TDD London School testing infrastructure across the AIVillage project's 821 test files.

## üöÄ Quick Deployment

### 1. Prerequisites Check

```bash
# Ensure pytest is installed
pip install pytest pytest-cov pytest-asyncio pytest-timeout pytest-xdist

# Verify MCP coordination is available
npx claude-flow@alpha --version

# Check Python environment
python --version  # Should be 3.8+
```

### 2. Infrastructure Files Deployed

The following standardization files have been created:

- **`tests/pytest.ini`** - Unified pytest configuration with 90%+ coverage targets
- **`tests/fixtures/tdd_london_mocks.py`** - Behavior verification mock factory
- **`tests/fixtures/unified_conftest.py`** - Consolidated fixtures and test utilities
- **`tests/examples/tdd_london_example.py`** - Implementation examples and patterns
- **`tests/scripts/test_migration_script.py`** - Automated migration tool
- **`tests/scripts/test_execution_pipeline.py`** - MCP-coordinated test runner

## üîß Migration Process

### Step 1: Analyze Current Test Files

```bash
# Run migration analysis (dry run)
python tests/scripts/test_migration_script.py tests --dry-run --verbose
```

This will analyze all 821 test files and identify:
- Framework usage patterns (unittest vs pytest)
- Mock implementation approaches
- Coverage gaps and inconsistencies

### Step 2: Execute Automated Migration

```bash
# Migrate unittest tests to pytest (backup recommended)
git checkout -b testing-infrastructure-migration
python tests/scripts/test_migration_script.py tests --verbose
```

### Step 3: Manual Review for Complex Cases

Review migration report for files requiring manual attention:
- Complex setUp/tearDown methods
- Custom assertion patterns  
- Integration-specific test configurations

## üß™ TDD London School Patterns

### Behavior Verification Example

```python
from tests.fixtures.tdd_london_mocks import MockFactory, ContractTestingMixin

class TestUserService(ContractTestingMixin):
    def test_user_registration_workflow(self, mock_factory):
        # Arrange - Create collaborators
        collaborators = mock_factory.create_collaborator_set(
            'user_repository', 'email_service', 'audit_logger'
        )
        
        # Configure expected behaviors
        collaborators['user_repository'].expect_interaction('save', email='test@example.com')
        collaborators['email_service'].expect_interaction('send_welcome', user_id='123')
        
        # Act - Exercise the system
        service = UserService(**{k: v._mock for k, v in collaborators.items()})
        result = service.register_user({'email': 'test@example.com'})
        
        # Assert - Verify interactions
        assert result['success'] is True
        self.assert_interaction_count(collaborators['user_repository'], 'save', 1)
```

### Outside-In TDD Workflow

1. **Start with acceptance criteria** (user story level)
2. **Discover collaborators** through failing tests
3. **Drive interface design** through mock expectations
4. **Implement just enough** to make tests pass
5. **Refactor** with confidence in behavior verification

## üìä Coverage and Quality Gates

### Coverage Configuration

The unified `pytest.ini` enforces:
- **90%+ code coverage** across all modules
- **Branch coverage** enabled for thorough testing
- **HTML and XML reports** for CI/CD integration
- **Missing line identification** for gap analysis

### Quality Markers

Tests are automatically categorized with markers:
- `@pytest.mark.unit` - Unit tests with mock isolation
- `@pytest.mark.integration` - Component interaction tests
- `@pytest.mark.behavior_verification` - London School patterns
- `@pytest.mark.security` - Security validation tests
- `@pytest.mark.performance` - Performance benchmarking

## ü§ñ MCP Coordination Integration

### Session Management

```bash
# Initialize MCP session for testing
npx claude-flow@alpha hooks pre-task --description "Testing session"

# Run tests with MCP coordination
python tests/scripts/test_execution_pipeline.py --config tests/pipeline_config.json

# Finalize session with metrics
npx claude-flow@alpha hooks session-end --export-metrics true
```

### Memory Pattern Storage

The infrastructure automatically stores:
- **Testing patterns** in MCP memory for reuse
- **Mock contracts** for consistency across tests
- **Performance metrics** for trend analysis
- **Coverage data** for continuous improvement

## üö¶ Test Execution Pipeline

### Run All Test Suites

```bash
# Execute complete testing pipeline
python tests/scripts/test_execution_pipeline.py
```

### Run Specific Test Categories

```bash
# Unit tests only
pytest -m unit --cov=src --cov-fail-under=90

# Integration tests
pytest -m integration --timeout=600

# Behavior verification tests
pytest -m behavior_verification -v

# Security tests
pytest -m security --tb=short
```

### Parallel Execution

```bash
# Run tests in parallel for faster execution
pytest -n auto --dist loadscope
```

## üìà Performance Metrics

Expected improvements after full deployment:

- **Framework Consistency**: 100% pytest adoption (from 48.7%)
- **Mock Standardization**: Centralized behavior verification 
- **Coverage Achievement**: 90%+ across all 821 test files
- **Execution Speed**: 40% faster through optimization
- **Maintenance Reduction**: 60% less configuration overhead

## üõ†Ô∏è Troubleshooting

### Common Migration Issues

1. **Import Errors**: Update Python path in test files
2. **Fixture Conflicts**: Use unified `conftest.py` fixtures
3. **Assertion Patterns**: Convert unittest assertions to pytest
4. **Mock Behavior**: Migrate to behavior verification mocks

### Coverage Issues

1. **Below 90% Threshold**: Review missing test cases
2. **Exclude Patterns**: Update coverage configuration
3. **Branch Coverage**: Add conditional path testing

### MCP Coordination Issues

1. **Hook Timeouts**: Check network connectivity
2. **Memory Storage**: Verify MCP server availability
3. **Session Conflicts**: Use unique session IDs

## üìã Validation Checklist

Before considering deployment complete:

- [ ] All 821 test files use pytest framework
- [ ] Unified `pytest.ini` configuration active
- [ ] TDD London School fixtures available
- [ ] Mock factory patterns implemented
- [ ] 90%+ coverage achieved across all modules
- [ ] MCP coordination hooks functional
- [ ] Test execution pipeline operational
- [ ] Performance metrics baseline established
- [ ] Migration report generated and reviewed
- [ ] Documentation updated for new patterns

## üîÑ Continuous Improvement

### Weekly Reviews

1. **Coverage Trend Analysis**: Track coverage improvements
2. **Performance Monitoring**: Measure test execution times
3. **Pattern Adoption**: Review London School usage
4. **Quality Metrics**: Analyze test effectiveness

### Monthly Optimizations

1. **Fixture Consolidation**: Reduce duplication
2. **Mock Contract Updates**: Enhance behavior verification
3. **Pipeline Optimization**: Improve execution speed
4. **Pattern Refinement**: Update based on usage patterns

## üìö Additional Resources

- **TDD London School Guide**: `tests/examples/tdd_london_example.py`
- **Mock Factory Documentation**: `tests/fixtures/tdd_london_mocks.py`
- **Pipeline Configuration**: `tests/scripts/test_execution_pipeline.py`
- **Migration Tools**: `tests/scripts/test_migration_script.py`

## üéØ Success Metrics

Track these KPIs post-deployment:

- **Test Consistency**: 100% pytest framework adoption
- **Coverage Quality**: Sustained 90%+ coverage across modules
- **Execution Efficiency**: <5 minutes for complete test suite
- **Developer Productivity**: Reduced test writing and maintenance time
- **Quality Gates**: Zero test failures in CI/CD pipeline
- **Behavior Verification**: 100% critical workflows tested with mocks

---

**Next Steps**: Execute migration plan and monitor metrics for continuous improvement of testing infrastructure quality and developer productivity.