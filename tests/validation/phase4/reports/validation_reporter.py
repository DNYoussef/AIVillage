"""
Validation Reporter for Phase 4

Generates comprehensive reports on Phase 4 validation results.
"""

import logging
from typing import Dict, List, Any
from pathlib import Path
import json
from dataclasses import dataclass, asdict
from datetime import datetime


@dataclass
class ValidationSummary:
    """Summary of validation results"""

    total_tests: int
    passed_tests: int
    failed_tests: int
    warnings: int
    success_rate: float
    critical_failures: int
    execution_time_ms: int
    timestamp: str


class ValidationReporter:
    """
    Generates comprehensive validation reports for Phase 4
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    async def generate_report(self, validation_result: Any, targets: Any) -> str:
        """
        Generate comprehensive validation report

        Args:
            validation_result: Results from Phase4ValidationSuite
            targets: ValidationTargets used for the test

        Returns:
            Path to generated report file
        """
        self.logger.info("Generating Phase 4 validation report...")

        # Create report directory
        report_dir = Path.cwd() / "tests/validation/phase4/reports"
        report_dir.mkdir(parents=True, exist_ok=True)

        # Generate report timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"phase4_validation_report_{timestamp}.md"

        # Generate report content
        report_content = self._generate_markdown_report(validation_result, targets)

        # Write report to file
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_content)

        # Also generate JSON report for programmatic access
        json_report_file = report_dir / f"phase4_validation_report_{timestamp}.json"
        json_content = self._generate_json_report(validation_result, targets)

        with open(json_report_file, "w", encoding="utf-8") as f:
            json.dump(json_content, f, indent=2, default=str)

        self.logger.info(f"Validation reports generated: {report_file}")

        return str(report_file)

    def _generate_markdown_report(self, validation_result: Any, targets: Any) -> str:
        """Generate detailed markdown report"""

        # Calculate summary statistics
        summary = self._calculate_summary(validation_result)

        report = f"""# Phase 4 Architectural Validation Report

**Generated:** {validation_result.timestamp}  
**Status:** {"‚úÖ PASSED" if validation_result.passed else "‚ùå FAILED"}  
**Execution Time:** {validation_result.execution_time_ms}ms

## Executive Summary

Phase 4 architectural improvements validation completed with **{summary.success_rate:.1f}% success rate**.

- **Total Tests:** {summary.total_tests}
- **Passed:** {summary.passed_tests} ‚úÖ
- **Failed:** {summary.failed_tests} ‚ùå
- **Warnings:** {summary.warnings} ‚ö†Ô∏è
- **Critical Failures:** {summary.critical_failures} üö®

## Validation Targets Status

### Coupling Score Improvements

{self._format_coupling_results(validation_result.coupling_results, targets)}

### Performance Benchmarks

{self._format_performance_results(validation_result.performance_results, targets)}

### Code Quality Metrics

{self._format_quality_results(validation_result.quality_results, targets)}

## Detailed Results

### 1. Coupling Analysis

{self._format_detailed_coupling_analysis(validation_result.coupling_results)}

### 2. Performance Testing

{self._format_detailed_performance_analysis(validation_result.performance_results)}

### 3. Backwards Compatibility

{self._format_compatibility_analysis(validation_result.compatibility_results)}

### 4. Integration Testing

{self._format_integration_analysis(validation_result.integration_results)}

## Issues and Recommendations

{self._format_issues_and_recommendations(validation_result)}

## Conclusion

{self._format_conclusion(validation_result, summary)}

---

*Report generated by Phase 4 Validation Framework v2.0.0*
"""

        return report

    def _format_coupling_results(self, coupling_results: Dict[str, Any], targets: Any) -> str:
        """Format coupling improvements section"""
        if not coupling_results:
            return "‚ùå Coupling analysis failed - no results available"

        improvements = coupling_results.get("improvements", {})

        sections = []

        # UnifiedManagement
        if "UnifiedManagement" in improvements:
            um = improvements["UnifiedManagement"]
            target_met = "‚úÖ" if um.get("target_met", False) else "‚ùå"
            sections.append(
                f"- **UnifiedManagement:** {um.get('current', 0):.2f} (target: ‚â§{targets.unified_management_coupling}) {target_met}"
            )

        # SageAgent
        if "SageAgent" in improvements:
            sa = improvements["SageAgent"]
            target_met = "‚úÖ" if sa.get("target_met", False) else "‚ùå"
            sections.append(
                f"- **SageAgent:** {sa.get('current', 0):.2f} (target: ‚â§{targets.sage_agent_coupling}) {target_met}"
            )

        # Task Management Average
        if "task_management_average" in improvements:
            tm = improvements["task_management_average"]
            target_met = "‚úÖ" if tm.get("target_met", False) else "‚ùå"
            sections.append(
                f"- **Task Management Avg:** {tm.get('current', 0):.2f} (target: ‚â§{targets.task_management_avg_coupling}) {target_met}"
            )

        if not sections:
            return "‚ö†Ô∏è No coupling improvement data available"

        return "\\n".join(sections)

    def _format_performance_results(self, performance_results: Dict[str, Any], targets: Any) -> str:
        """Format performance benchmarks section"""
        if not performance_results:
            return "‚ùå Performance testing failed - no results available"

        overall = performance_results.get("overall", {})

        sections = [
            f"- **Memory Increase:** {overall.get('memory_increase_percent', 0):.1f}% (target: ‚â§{targets.max_memory_increase}%) {'‚úÖ' if overall.get('memory_increase_percent', float('inf')) <= targets.max_memory_increase else '‚ùå'}",
            f"- **Throughput Ratio:** {overall.get('throughput_ratio', 0):.2f} (target: ‚â•{targets.min_throughput_ratio}) {'‚úÖ' if overall.get('throughput_ratio', 0) >= targets.min_throughput_ratio else '‚ùå'}",
            f"- **Init Time:** {overall.get('init_time_ms', 0):.0f}ms (target: ‚â§{targets.max_init_time_ms}ms) {'‚úÖ' if overall.get('init_time_ms', float('inf')) <= targets.max_init_time_ms else '‚ùå'}",
            f"- **Performance Degradation:** {overall.get('performance_degradation_percent', 0):.1f}% (target: ‚â§{targets.max_performance_degradation}%) {'‚úÖ' if overall.get('performance_degradation_percent', float('inf')) <= targets.max_performance_degradation else '‚ùå'}",
        ]

        return "\\n".join(sections)

    def _format_quality_results(self, quality_results: Dict[str, Any], targets: Any) -> str:
        """Format code quality metrics section"""
        if not quality_results:
            return "‚ùå Quality analysis failed - no results available"

        sections = [
            f"- **Max Lines per Class:** {quality_results.get('max_lines_per_class', 0)} (target: ‚â§{targets.max_lines_per_class}) {'‚úÖ' if quality_results.get('max_lines_per_class', float('inf')) <= targets.max_lines_per_class else '‚ùå'}",
            f"- **Magic Literals:** {quality_results.get('magic_literals_count', 0)} (target: {targets.magic_literals_target}) {'‚úÖ' if quality_results.get('magic_literals_count', float('inf')) <= targets.magic_literals_target else '‚ùå'}",
            f"- **Test Coverage:** {quality_results.get('test_coverage_percent', 0):.1f}% (target: ‚â•{targets.min_test_coverage}%) {'‚úÖ' if quality_results.get('test_coverage_percent', 0) >= targets.min_test_coverage else '‚ùå'}",
            f"- **Max Cyclomatic Complexity:** {quality_results.get('max_cyclomatic_complexity', 0)} (target: ‚â§{targets.max_cyclomatic_complexity}) {'‚úÖ' if quality_results.get('max_cyclomatic_complexity', float('inf')) <= targets.max_cyclomatic_complexity else '‚ùå'}",
        ]

        return "\\n".join(sections)

    def _format_detailed_coupling_analysis(self, coupling_results: Dict[str, Any]) -> str:
        """Format detailed coupling analysis"""
        if not coupling_results:
            return "No coupling analysis data available."

        sections = []

        coupling_scores = coupling_results.get("coupling_scores", {})
        for component, metrics in coupling_scores.items():
            if isinstance(metrics, dict) and "coupling_score" in metrics:
                sections.append(
                    f"""
**{component}**
- Coupling Score: {metrics['coupling_score']}
- Incoming Dependencies: {metrics.get('incoming_dependencies', 'N/A')}
- Outgoing Dependencies: {metrics.get('outgoing_dependencies', 'N/A')}
- Stability Metric: {metrics.get('stability_metric', 'N/A'):.3f}
- Distance from Main: {metrics.get('distance_from_main', 'N/A'):.3f}
"""
                )

        if not sections:
            return "No detailed coupling data available."

        return "\\n".join(sections)

    def _format_detailed_performance_analysis(self, performance_results: Dict[str, Any]) -> str:
        """Format detailed performance analysis"""
        if not performance_results:
            return "No performance data available."

        sections = []

        # Memory Usage
        if "memory_usage" in performance_results:
            memory = performance_results["memory_usage"]
            sections.append(
                f"""
**Memory Usage Analysis**
- Baseline Memory: {memory.get('baseline_memory_mb', 0):.1f} MB
- Maximum Increase: {memory.get('max_memory_increase_percent', 0):.1f}%
- Average Increase: {memory.get('avg_memory_increase_percent', 0):.1f}%
- Status: {'‚úÖ Passed' if memory.get('passed', False) else '‚ùå Failed'}
"""
            )

        # Task Throughput
        if "task_throughput" in performance_results:
            throughput = performance_results["task_throughput"]
            sections.append(
                f"""
**Task Processing Throughput**
- Current Throughput: {throughput.get('current_throughput', 0):.1f} tasks/sec
- Maximum Throughput: {throughput.get('max_throughput', 0):.1f} tasks/sec
- Status: {'‚úÖ Passed' if throughput.get('passed', False) else '‚ùå Failed'}
"""
            )

        # Service Initialization
        if "service_init" in performance_results:
            init = performance_results["service_init"]
            sections.append(
                f"""
**Service Initialization Times**
- Total Init Time: {init.get('total_init_time_ms', 0):.1f} ms
- Maximum Init Time: {init.get('max_init_time_ms', 0):.1f} ms
- Average Init Time: {init.get('avg_init_time_ms', 0):.1f} ms
- Status: {'‚úÖ Passed' if init.get('passed', False) else '‚ùå Failed'}
"""
            )

        if not sections:
            return "No detailed performance data available."

        return "\\n".join(sections)

    def _format_compatibility_analysis(self, compatibility_results: Dict[str, Any]) -> str:
        """Format compatibility analysis"""
        if not compatibility_results:
            return "No compatibility test data available."

        summary = compatibility_results.get("summary", {})

        sections = [
            f"""
**Compatibility Test Summary**
- Total Tests: {summary.get('total_tests', 0)}
- Passed: {summary.get('passed', 0)}
- Failed: {summary.get('failed', 0)}
- Warnings: {summary.get('warnings', 0)}
- Overall Status: {'‚úÖ All Compatible' if compatibility_results.get('all_tests_passed', False) else '‚ùå Compatibility Issues'}
"""
        ]

        # Critical failures
        critical_failures = compatibility_results.get("critical_failures", [])
        if critical_failures:
            sections.append("\\n**Critical Compatibility Failures:**")
            for failure in critical_failures:
                sections.append(f"- {failure.get('test', 'Unknown')}: {failure.get('error', 'No details')}")

        # Category breakdown
        categories = compatibility_results.get("categories", {})
        if categories:
            sections.append("\\n**Results by Category:**")
            for category, results in categories.items():
                passed = results.get("passed", 0)
                total = passed + results.get("failed", 0)
                sections.append(f"- {category}: {passed}/{total} passed")

        return "\\n".join(sections)

    def _format_integration_analysis(self, integration_results: Dict[str, Any]) -> str:
        """Format integration analysis"""
        if not integration_results:
            return "No integration test data available."

        summary = integration_results.get("summary", {})

        sections = [
            f"""
**Integration Test Summary**
- Total Tests: {summary.get('total_tests', 0)}
- Passed: {summary.get('passed', 0)}
- Failed: {summary.get('failed', 0)}
- Skipped: {summary.get('skipped', 0)}
- Overall Status: {'‚úÖ All Services Integrated' if integration_results.get('all_services_integrated', False) else '‚ùå Integration Issues'}
"""
        ]

        # Service health
        service_health = integration_results.get("service_health", {})
        if service_health:
            healthy_count = service_health.get("healthy_count", 0)
            total_count = service_health.get("total_count", 0)
            sections.append(f"\\n**Service Health:** {healthy_count}/{total_count} services healthy")

            unhealthy = service_health.get("unhealthy_services", [])
            if unhealthy:
                sections.append(f"**Unhealthy Services:** {', '.join(unhealthy)}")

        # Communication tests
        communication = integration_results.get("communication_tests", {})
        if communication:
            success_rate = communication.get("success_rate", 0)
            sections.append(f"\\n**Service Communication:** {success_rate:.1f}% success rate")

        return "\\n".join(sections)

    def _format_issues_and_recommendations(self, validation_result: Any) -> str:
        """Format issues and recommendations section"""
        sections = []

        # Collect all errors
        if validation_result.errors:
            sections.append("### Critical Issues")
            for error in validation_result.errors:
                sections.append(f"- ‚ùå {error}")

        # Collect all warnings
        warnings = []

        # Add warnings from different components
        if hasattr(validation_result, "coupling_results") and validation_result.coupling_results:
            coupling_warnings = validation_result.coupling_results.get("warnings", [])
            warnings.extend(coupling_warnings)

        if hasattr(validation_result, "performance_results") and validation_result.performance_results:
            perf_warnings = validation_result.performance_results.get("warnings", [])
            warnings.extend(perf_warnings)

        if warnings:
            sections.append("\\n### Warnings")
            for warning in warnings:
                sections.append(f"- ‚ö†Ô∏è {warning}")

        # Recommendations based on results
        sections.append("\\n### Recommendations")

        if not validation_result.passed:
            sections.append("- üîß Address critical failures before proceeding with Phase 4 deployment")

        if validation_result.coupling_results and not self._check_all_coupling_targets_met(
            validation_result.coupling_results
        ):
            sections.append("- üîß Further refactoring needed to meet coupling reduction targets")

        if validation_result.performance_results and not self._check_all_performance_targets_met(
            validation_result.performance_results
        ):
            sections.append("- üîß Performance optimization required to meet benchmark targets")

        if validation_result.compatibility_results and not validation_result.compatibility_results.get(
            "all_tests_passed", True
        ):
            sections.append("- üîß Fix backwards compatibility issues to maintain legacy support")

        if not sections:
            sections.append("‚úÖ No issues found - validation passed successfully!")

        return "\\n".join(sections)

    def _format_conclusion(self, validation_result: Any, summary: ValidationSummary) -> str:
        """Format conclusion section"""
        if validation_result.passed:
            return f"""
‚úÖ **Phase 4 architectural improvements have successfully passed all validation criteria.**

The refactoring has achieved:
- Coupling score reductions meeting all targets
- Performance benchmarks maintained within acceptable ranges
- Full backwards compatibility preserved
- Successful service integration

**Recommendation:** Proceed with Phase 4 deployment.

**Success Rate:** {summary.success_rate:.1f}%  
**Total Execution Time:** {summary.execution_time_ms}ms
"""
        else:
            return f"""
‚ùå **Phase 4 architectural improvements have failed validation.**

**Critical Issues:** {summary.critical_failures}  
**Success Rate:** {summary.success_rate:.1f}%

**Recommendation:** Address all critical failures and re-run validation before deployment.

Please review the detailed results above and implement necessary fixes.
"""

    def _calculate_summary(self, validation_result: Any) -> ValidationSummary:
        """Calculate validation summary statistics"""
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        warnings = 0
        critical_failures = 0

        # Count tests from all components
        components = [
            "coupling_results",
            "performance_results",
            "quality_results",
            "compatibility_results",
            "integration_results",
        ]

        for component in components:
            if hasattr(validation_result, component):
                results = getattr(validation_result, component)
                if results:
                    # Add component-specific counting logic here
                    total_tests += 1
                    if self._is_component_passed(component, results):
                        passed_tests += 1
                    else:
                        failed_tests += 1
                        critical_failures += 1

        # Count warnings
        if validation_result.warnings:
            warnings = len(validation_result.warnings)

        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        return ValidationSummary(
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            warnings=warnings,
            success_rate=success_rate,
            critical_failures=critical_failures,
            execution_time_ms=validation_result.execution_time_ms,
            timestamp=validation_result.timestamp,
        )

    def _is_component_passed(self, component: str, results: Dict[str, Any]) -> bool:
        """Check if a component passed its validation"""
        if component == "coupling_results":
            return self._check_all_coupling_targets_met(results)
        elif component == "performance_results":
            return self._check_all_performance_targets_met(results)
        elif component in ["compatibility_results", "integration_results"]:
            return results.get("all_tests_passed", False) or results.get("all_services_integrated", False)
        else:
            return results.get("passed", False)

    def _check_all_coupling_targets_met(self, coupling_results: Dict[str, Any]) -> bool:
        """Check if all coupling targets are met"""
        improvements = coupling_results.get("improvements", {})
        for component, improvement in improvements.items():
            if not improvement.get("target_met", False):
                return False
        return True

    def _check_all_performance_targets_met(self, performance_results: Dict[str, Any]) -> bool:
        """Check if all performance targets are met"""
        overall = performance_results.get("overall", {})
        return overall.get("all_tests_passed", False)

    def _generate_json_report(self, validation_result: Any, targets: Any) -> Dict[str, Any]:
        """Generate JSON report for programmatic access"""
        return {
            "validation_summary": {
                "passed": validation_result.passed,
                "execution_time_ms": validation_result.execution_time_ms,
                "timestamp": validation_result.timestamp,
            },
            "targets": asdict(targets) if hasattr(targets, "__dict__") else targets,
            "results": {
                "coupling": validation_result.coupling_results,
                "performance": validation_result.performance_results,
                "quality": validation_result.quality_results,
                "compatibility": validation_result.compatibility_results,
                "integration": validation_result.integration_results,
            },
            "errors": validation_result.errors,
            "warnings": validation_result.warnings,
            "summary": asdict(self._calculate_summary(validation_result)),
        }

    async def generate_improvement_report(self, validation_result: Any, baseline_metrics: Dict[str, Any]) -> str:
        """Generate improvement comparison report"""
        self.logger.info("Generating improvement comparison report...")

        if not baseline_metrics:
            return "No baseline metrics available for comparison"

        # Compare current results with baseline
        improvements = []

        # Coupling improvements
        if validation_result.coupling_results and baseline_metrics.get("coupling"):
            coupling_improvements = self._calculate_coupling_improvements(
                validation_result.coupling_results, baseline_metrics["coupling"]
            )
            improvements.extend(coupling_improvements)

        # Performance improvements
        if validation_result.performance_results and baseline_metrics.get("performance"):
            performance_improvements = self._calculate_performance_improvements(
                validation_result.performance_results, baseline_metrics["performance"]
            )
            improvements.extend(performance_improvements)

        # Generate improvement report
        report_content = self._format_improvement_report(improvements, validation_result)

        return report_content

    def _calculate_coupling_improvements(
        self, current: Dict[str, Any], baseline: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Calculate coupling improvements"""
        improvements = []

        current_scores = current.get("coupling_scores", {})
        baseline_scores = baseline

        for component in ["UnifiedManagement", "SageAgent", "task_management_average"]:
            if component in current_scores and component in baseline_scores:
                current_score = (
                    current_scores[component].get("coupling_score", 0)
                    if isinstance(current_scores[component], dict)
                    else current_scores[component]
                )
                baseline_score = baseline_scores[component]

                improvement_percent = ((baseline_score - current_score) / baseline_score) * 100

                improvements.append(
                    {
                        "category": "coupling",
                        "component": component,
                        "baseline": baseline_score,
                        "current": current_score,
                        "improvement_percent": improvement_percent,
                        "target_met": improvement_percent >= 0,  # Any improvement is good
                    }
                )

        return improvements

    def _calculate_performance_improvements(
        self, current: Dict[str, Any], baseline: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Calculate performance improvements"""
        improvements = []

        current_overall = current.get("overall", {})

        # Memory usage comparison
        if "memory_usage_mb" in baseline:
            baseline_memory = baseline["memory_usage_mb"]
            current_memory = current_overall.get("memory_increase_percent", 0)

            improvements.append(
                {
                    "category": "performance",
                    "component": "memory_usage",
                    "baseline": baseline_memory,
                    "current": current_memory,
                    "improvement_percent": (
                        0 if current_memory <= 10 else -current_memory
                    ),  # Negative if increased too much
                    "target_met": current_memory <= 10,
                }
            )

        return improvements

    def _format_improvement_report(self, improvements: List[Dict[str, Any]], validation_result: Any) -> str:
        """Format improvement comparison report"""
        report = f"""# Phase 4 Improvement Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Improvement Summary

"""

        if not improvements:
            report += "No improvement data available for comparison."
            return report

        # Group by category
        coupling_improvements = [i for i in improvements if i["category"] == "coupling"]
        performance_improvements = [i for i in improvements if i["category"] == "performance"]

        if coupling_improvements:
            report += "### Coupling Score Improvements\\n\\n"
            for imp in coupling_improvements:
                status = "‚úÖ" if imp["target_met"] else "‚ùå"
                report += f"- **{imp['component']}:** {imp['baseline']:.2f} ‚Üí {imp['current']:.2f} ({imp['improvement_percent']:+.1f}%) {status}\\n"

        if performance_improvements:
            report += "\\n### Performance Improvements\\n\\n"
            for imp in performance_improvements:
                status = "‚úÖ" if imp["target_met"] else "‚ùå"
                report += f"- **{imp['component']}:** {imp['improvement_percent']:+.1f}% change {status}\\n"

        return report
