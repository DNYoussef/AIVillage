#!/usr/bin/env python3
"""
Caching Performance Regression Guard Tests
==========================================

Prevents performance regression in caching infrastructure.
Tests N+1 query prevention, connection pooling, and cache efficiency.

Generated by TDD London School Swarm Agent for Phase 5 guard protection.
"""

import pytest
import asyncio
import time
from unittest.mock import MagicMock, AsyncMock, patch

import sys

sys.path.insert(0, "src")

from src.performance.caching_manager import (
    CacheConfig,
    CacheKey,
    RedisCacheManager,
    MemcachedManager,
    UnifiedCacheManager,
    cache_agent_forge_result,
    get_cached_agent_forge_result,
)


class TestCachingPerformanceRegression:
    """Performance regression tests for caching infrastructure."""

    @pytest.fixture
    def mock_redis_sentinel(self):
        """Mock Redis Sentinel for testing."""
        sentinel_mock = MagicMock()

        # Mock Redis clients
        master_mock = AsyncMock()
        replica_mock = AsyncMock()

        # Setup successful responses
        master_mock.get.return_value = b'{"test": "data"}'
        master_mock.setex.return_value = True
        master_mock.delete.return_value = 1

        replica_mock.get.return_value = b'{"test": "data"}'

        sentinel_mock.master_for.return_value = master_mock
        sentinel_mock.slave_for.return_value = replica_mock

        return sentinel_mock, master_mock, replica_mock

    @pytest.fixture
    def cache_config(self):
        """Test cache configuration."""
        return CacheConfig(
            redis_sentinels=[("localhost", 26379)],
            redis_master_name="test_master",
            redis_password="test_password",
            memcached_servers=["localhost:11211"],
            default_ttl=3600,
            max_retries=3,
            retry_delay=0.01,  # Fast retries for testing
        )

    @pytest.mark.asyncio
    async def test_redis_connection_pooling_efficiency(self, cache_config, mock_redis_sentinel):
        """Test Redis connection pooling prevents connection exhaustion."""
        sentinel_mock, master_mock, replica_mock = mock_redis_sentinel

        with patch("redis.sentinel.Sentinel", return_value=sentinel_mock):
            cache_manager = RedisCacheManager(cache_config)

            # Simulate multiple concurrent operations
            tasks = []
            for i in range(100):  # Many concurrent requests
                task = cache_manager.get(f"test_key_{i}")
                tasks.append(task)

            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            duration = time.time() - start_time

            # Should complete quickly with connection pooling
            assert duration < 1.0, f"Connection pooling inefficient: {duration}s for 100 requests"

            # Should have successful results
            successful_results = [r for r in results if not isinstance(r, Exception)]
            assert len(successful_results) > 0, "No successful cache operations"

            # Should not create excessive connections (Sentinel should reuse)
            assert sentinel_mock.master_for.call_count <= 1, "Too many master connections created"

    @pytest.mark.asyncio
    async def test_cache_key_generation_performance(self):
        """Test cache key generation is efficient and collision-resistant."""
        # Test Agent Forge key generation performance
        start_time = time.time()

        keys = []
        for i in range(10000):
            params = {"model_id": f"model_{i}", "batch_size": 32, "learning_rate": 0.001}
            params_hash = CacheKey.hash_params(params)
            key = CacheKey.agent_forge_key("training", f"model_{i}", params_hash)
            keys.append(key)

        generation_time = time.time() - start_time

        # Should generate keys quickly
        assert generation_time < 0.5, f"Key generation too slow: {generation_time}s for 10k keys"

        # Should have no duplicates (collision resistance)
        unique_keys = set(keys)
        assert len(unique_keys) == len(keys), f"Hash collisions detected: {len(keys)} -> {len(unique_keys)}"

        # Keys should be reasonable length (not too long for memory efficiency)
        avg_key_length = sum(len(k) for k in keys) / len(keys)
        assert avg_key_length < 200, f"Keys too long: average {avg_key_length} chars"

    @pytest.mark.asyncio
    async def test_n_plus_one_query_prevention(self, cache_config, mock_redis_sentinel):
        """Test caching prevents N+1 query patterns."""
        sentinel_mock, master_mock, replica_mock = mock_redis_sentinel

        # Setup cache hits after first miss
        cache_hits = {}

        def mock_get(key):
            if key in cache_hits:
                return b'{"cached": "result"}'
            cache_hits[key] = True
            return None  # First call is miss

        replica_mock.get.side_effect = mock_get

        with patch("redis.sentinel.Sentinel", return_value=sentinel_mock):
            cache_manager = RedisCacheManager(cache_config)

            # Simulate N+1 query pattern
            base_params = {"model": "test", "version": "1.0"}

            # First pass - cache misses (simulates database queries)
            first_pass_start = time.time()
            for i in range(50):
                params = {**base_params, "item_id": i}
                params_hash = CacheKey.hash_params(params)
                key = CacheKey.agent_forge_key("inference", "test_model", params_hash)

                result = await cache_manager.get(key)
                if result is None:
                    # Simulate expensive operation (database query)
                    await asyncio.sleep(0.01)  # 10ms simulated DB query
                    await cache_manager.set(key, {"result": f"data_{i}"}, service="test")

            first_pass_time = time.time() - first_pass_start

            # Second pass - cache hits (should be much faster)
            second_pass_start = time.time()
            for i in range(50):
                params = {**base_params, "item_id": i}
                params_hash = CacheKey.hash_params(params)
                key = CacheKey.agent_forge_key("inference", "test_model", params_hash)

                result = await cache_manager.get(key)
                assert result is not None, f"Cache miss on second pass for key {key}"

            second_pass_time = time.time() - second_pass_start

            # Cache hits should be significantly faster
            speedup_ratio = first_pass_time / second_pass_time if second_pass_time > 0 else float("inf")
            assert speedup_ratio > 5, f"Insufficient cache speedup: {speedup_ratio}x"

    @pytest.mark.asyncio
    async def test_cache_memory_efficiency(self, cache_config):
        """Test cache uses memory efficiently and respects size limits."""
        # Test with mock memcached
        mock_memcached = MagicMock()

        stored_data = {}

        def mock_set(key, value, time=None):
            # Simulate memory limit
            if len(stored_data) >= 1000:  # Simulate cache eviction
                # Remove oldest entries
                keys_to_remove = list(stored_data.keys())[:100]
                for k in keys_to_remove:
                    del stored_data[k]

            stored_data[key] = value
            return True

        def mock_get(key):
            return stored_data.get(key)

        mock_memcached.set.side_effect = mock_set
        mock_memcached.get.side_effect = mock_get
        mock_memcached.get_stats.return_value = []

        with patch("memcache.Client", return_value=mock_memcached):
            cache_manager = MemcachedManager(cache_config)

            # Store many items to test memory efficiency
            large_data = "x" * 1000  # 1KB per item

            for i in range(1500):  # More than limit
                await cache_manager.set(f"key_{i}", large_data, ttl=3600)

            # Should have evicted old entries
            assert len(stored_data) <= 1000, "Cache not respecting memory limits"

            # Recent entries should still be available
            recent_key = "key_1499"
            result = await cache_manager.get(recent_key)
            assert result == large_data, "Recent cache entry was incorrectly evicted"

    @pytest.mark.asyncio
    async def test_cache_failover_performance(self, cache_config, mock_redis_sentinel):
        """Test cache failover doesn't significantly impact performance."""
        sentinel_mock, master_mock, replica_mock = mock_redis_sentinel

        # Simulate master failure
        failure_count = 0

        def failing_master_get(key):
            nonlocal failure_count
            failure_count += 1
            if failure_count <= 3:  # First 3 calls fail
                raise Exception("Redis master down")
            return b'{"recovered": "data"}'

        master_mock.get.side_effect = failing_master_get

        with patch("redis.sentinel.Sentinel", return_value=sentinel_mock):
            cache_manager = RedisCacheManager(cache_config)

            start_time = time.time()

            # Should retry and eventually succeed
            result = await cache_manager.get("test_key")

            failover_time = time.time() - start_time

            # Should recover within reasonable time
            assert failover_time < 1.0, f"Failover took too long: {failover_time}s"
            assert result is not None, "Failed to recover from master failure"

    @pytest.mark.asyncio
    async def test_unified_cache_service_routing_performance(self, cache_config):
        """Test unified cache routes to appropriate backends efficiently."""
        with patch("redis.sentinel.Sentinel"), patch("memcache.Client"):
            unified_cache = UnifiedCacheManager(cache_config)

            # Mock the underlying cache managers
            unified_cache.redis = AsyncMock()
            unified_cache.memcached = AsyncMock()

            unified_cache.redis.get.return_value = {"redis": "result"}
            unified_cache.memcached.get.return_value = {"memcached": "result"}

            # Test service routing performance
            services = ["agent_forge", "hyperrag", "p2p-mesh", "api-gateway", "edge-computing"]

            start_time = time.time()

            for service in services * 100:  # 500 operations
                await unified_cache.get(f"key_{service}", service)

            routing_time = time.time() - start_time

            # Service routing should be fast
            assert routing_time < 0.5, f"Service routing too slow: {routing_time}s for 500 ops"

            # Should route to correct backends
            assert unified_cache.redis.get.call_count > 0, "Redis not used for appropriate services"
            assert unified_cache.memcached.get.call_count > 0, "Memcached not used for appropriate services"

    def test_cache_key_deterministic_hashing(self):
        """Test cache key hashing is deterministic and collision-resistant."""
        # Same parameters should always generate same hash
        params1 = {"model": "test", "batch_size": 32, "lr": 0.001}
        params2 = {"model": "test", "batch_size": 32, "lr": 0.001}

        hash1 = CacheKey.hash_params(params1)
        hash2 = CacheKey.hash_params(params2)

        assert hash1 == hash2, "Same parameters produced different hashes"

        # Different parameters should generate different hashes
        params3 = {"model": "test", "batch_size": 64, "lr": 0.001}
        hash3 = CacheKey.hash_params(params3)

        assert hash1 != hash3, "Different parameters produced same hash"

        # Hash should be stable across multiple calls
        hashes = [CacheKey.hash_params(params1) for _ in range(10)]
        assert all(h == hashes[0] for h in hashes), "Hash not deterministic"

        # Hash should be reasonably short but unique
        assert len(hash1) == 16, f"Hash length not optimal: {len(hash1)} chars"

    @pytest.mark.asyncio
    async def test_convenience_functions_performance(self):
        """Test convenience functions don't add significant overhead."""
        # Mock the cache manager
        with patch("src.performance.caching_manager.cache_manager") as mock_cache:
            mock_cache.get.return_value = None
            mock_cache.set.return_value = True

            # Test Agent Forge convenience functions
            params = {"model": "test", "epochs": 10}

            start_time = time.time()

            for i in range(1000):
                # Test caching
                await cache_agent_forge_result("training", "model_v1", params, {"result": i})

                # Test retrieval
                await get_cached_agent_forge_result("training", "model_v1", params)

            convenience_time = time.time() - start_time

            # Convenience functions should be fast
            assert convenience_time < 0.5, f"Convenience functions too slow: {convenience_time}s"

            # Should have made appropriate calls to underlying cache
            assert mock_cache.set.call_count == 1000, "Incorrect number of cache set calls"
            assert mock_cache.get.call_count == 1000, "Incorrect number of cache get calls"


class TestCachingBehavioralContracts:
    """Behavioral contract tests for caching system."""

    @pytest.fixture
    def cache_config(self):
        return CacheConfig(max_retries=2, retry_delay=0.01)

    @pytest.mark.asyncio
    async def test_cache_get_contract(self, cache_config):
        """Contract: Cache get always returns value or None, never raises."""
        with patch("redis.sentinel.Sentinel") as mock_sentinel_class:
            # Setup failing Redis
            mock_sentinel = MagicMock()
            mock_master = MagicMock()
            mock_master.get.side_effect = Exception("Connection failed")
            mock_sentinel.master_for.return_value = mock_master
            mock_sentinel_class.return_value = mock_sentinel

            cache_manager = RedisCacheManager(cache_config)

            # Should not raise exception, should return None
            result = await cache_manager.get("test_key")
            assert result is None, "Failed get should return None"

    @pytest.mark.asyncio
    async def test_cache_set_contract(self, cache_config):
        """Contract: Cache set always returns bool, never raises."""
        with patch("redis.sentinel.Sentinel") as mock_sentinel_class:
            # Setup failing Redis
            mock_sentinel = MagicMock()
            mock_master = MagicMock()
            mock_master.setex.side_effect = Exception("Connection failed")
            mock_sentinel.master_for.return_value = mock_master
            mock_sentinel_class.return_value = mock_sentinel

            cache_manager = RedisCacheManager(cache_config)

            # Should not raise exception, should return False
            result = await cache_manager.set("test_key", "test_value")
            assert result is False, "Failed set should return False"

    @pytest.mark.asyncio
    async def test_cache_key_consistency_contract(self):
        """Contract: Same inputs always produce same cache keys."""
        # Test multiple times to ensure consistency
        for _ in range(10):
            key1 = CacheKey.agent_forge_key("phase1", "model1", "hash123")
            key2 = CacheKey.agent_forge_key("phase1", "model1", "hash123")
            assert key1 == key2, "Same inputs produced different keys"

            # Different inputs should produce different keys
            key3 = CacheKey.agent_forge_key("phase2", "model1", "hash123")
            assert key1 != key3, "Different inputs produced same key"

    @pytest.mark.asyncio
    async def test_unified_cache_service_mapping_contract(self, cache_config):
        """Contract: Services always map to consistent cache backends."""
        with patch("redis.sentinel.Sentinel"), patch("memcache.Client"):
            cache = UnifiedCacheManager(cache_config)

            # Test service mapping consistency
            service_mappings = {}
            test_services = ["agent_forge", "hyperrag", "p2p-mesh", "unknown-service"]

            for service in test_services:
                cache_backend = cache.service_cache_map.get(service, cache.redis)
                service_mappings[service] = type(cache_backend).__name__

            # Same service should always map to same backend
            for _ in range(5):
                for service in test_services:
                    current_backend = cache.service_cache_map.get(service, cache.redis)
                    assert (
                        type(current_backend).__name__ == service_mappings[service]
                    ), f"Service {service} mapping changed"

    def test_cache_stats_contract(self, cache_config):
        """Contract: Cache stats always return valid dict with required fields."""
        with patch("redis.sentinel.Sentinel"):
            cache_manager = RedisCacheManager(cache_config)
            stats = cache_manager.get_stats()

            required_fields = ["hits", "misses", "errors", "hit_rate", "total_operations"]
            for field in required_fields:
                assert field in stats, f"Missing required stat field: {field}"
                assert isinstance(stats[field], (int, float)), f"Invalid stat type for {field}"

            # Hit rate should be between 0 and 1
            assert 0 <= stats["hit_rate"] <= 1, f"Invalid hit rate: {stats['hit_rate']}"

            # Total should equal hits + misses
            assert stats["total_operations"] == stats["hits"] + stats["misses"], "Total operations inconsistent"
