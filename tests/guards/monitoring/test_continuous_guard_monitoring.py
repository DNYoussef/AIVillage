#!/usr/bin/env python3
"""
Continuous Guard Test Monitoring
================================

Implements continuous monitoring hooks for guard test execution.
Provides real-time validation and automated regression detection.

Generated by TDD London School Swarm Agent for Phase 5 guard protection.
"""

import pytest
import asyncio
import json
import time
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
from unittest.mock import MagicMock, patch
import logging

# Configure monitoring logger
monitoring_logger = logging.getLogger("guard_monitoring")
monitoring_logger.setLevel(logging.INFO)


class GuardTestStatus(Enum):
    """Guard test execution status."""
    PENDING = "pending"
    RUNNING = "running" 
    PASSED = "passed"
    FAILED = "failed"
    ERROR = "error"
    SKIPPED = "skipped"


class AlertLevel(Enum):
    """Alert severity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class GuardTestResult:
    """Individual guard test execution result."""
    test_name: str
    status: GuardTestStatus
    execution_time: float
    timestamp: datetime = field(default_factory=datetime.now)
    error_message: Optional[str] = None
    assertions_count: int = 0
    coverage_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "test_name": self.test_name,
            "status": self.status.value,
            "execution_time": self.execution_time,
            "timestamp": self.timestamp.isoformat(),
            "error_message": self.error_message,
            "assertions_count": self.assertions_count,
            "coverage_data": self.coverage_data
        }


@dataclass
class GuardAlert:
    """Guard monitoring alert."""
    alert_id: str
    level: AlertLevel
    category: str
    message: str
    details: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)
    acknowledged: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "alert_id": self.alert_id,
            "level": self.level.value,
            "category": self.category,
            "message": self.message,
            "details": self.details,
            "timestamp": self.timestamp.isoformat(),
            "acknowledged": self.acknowledged
        }


class GuardTestMonitor:
    """
    Continuous monitoring system for guard tests.
    
    Provides real-time test execution monitoring, automated regression
    detection, and alert generation for guard test failures.
    """
    
    def __init__(self, alert_threshold: int = 3, history_size: int = 1000):
        """Initialize guard test monitor."""
        self.alert_threshold = alert_threshold
        self.history_size = history_size
        self.test_results: List[GuardTestResult] = []
        self.alerts: List[GuardAlert] = []
        self.baseline_metrics: Dict[str, float] = {}
        self.monitoring_active = False
        self.alert_callbacks: List[Callable[[GuardAlert], None]] = []
        
        # Performance tracking
        self.performance_baselines = {
            "max_execution_time": 30.0,  # 30 seconds per test
            "max_failure_rate": 0.05,    # 5% failure rate
            "max_regression_count": 2     # 2 regressions per hour
        }
        
        monitoring_logger.info("Initialized GuardTestMonitor")
    
    def register_alert_callback(self, callback: Callable[[GuardAlert], None]):
        """Register callback for alert notifications."""
        self.alert_callbacks.append(callback)
    
    def start_monitoring(self):
        """Start continuous monitoring."""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        monitoring_logger.info("Started continuous guard test monitoring")
        
        # Start background monitoring thread
        monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        monitor_thread.start()
    
    def stop_monitoring(self):
        """Stop continuous monitoring."""
        self.monitoring_active = False
        monitoring_logger.info("Stopped continuous guard test monitoring")
    
    def record_test_result(self, result: GuardTestResult):
        """Record a guard test execution result."""
        self.test_results.append(result)
        
        # Maintain history size limit
        if len(self.test_results) > self.history_size:
            self.test_results = self.test_results[-self.history_size:]
        
        # Check for regressions and performance issues
        self._check_for_regressions(result)
        self._check_performance_thresholds(result)
        
        monitoring_logger.debug(f"Recorded test result: {result.test_name} - {result.status.value}")
    
    def _monitoring_loop(self):
        """Background monitoring loop."""
        while self.monitoring_active:
            try:
                self._perform_periodic_checks()
                time.sleep(60)  # Check every minute
                
            except Exception as e:
                monitoring_logger.error(f"Error in monitoring loop: {e}")
                time.sleep(10)  # Brief delay on error
    
    def _perform_periodic_checks(self):
        """Perform periodic monitoring checks."""
        # Check for sustained failures
        self._check_sustained_failures()
        
        # Check for performance degradation
        self._check_performance_trends()
        
        # Check for coverage regressions
        self._check_coverage_regressions()
        
        # Clean up old alerts
        self._cleanup_old_alerts()
    
    def _check_for_regressions(self, result: GuardTestResult):
        """Check if test result indicates a regression."""
        if result.status == GuardTestStatus.FAILED:
            # Count recent failures for this test
            recent_failures = self._count_recent_failures(
                result.test_name, 
                timedelta(hours=1)
            )
            
            if recent_failures >= self.alert_threshold:
                alert = GuardAlert(
                    alert_id=f"regression_{result.test_name}_{int(time.time())}",
                    level=AlertLevel.HIGH,
                    category="regression",
                    message=f"Guard test regression detected: {result.test_name}",
                    details={
                        "test_name": result.test_name,
                        "failure_count": recent_failures,
                        "last_error": result.error_message,
                        "execution_time": result.execution_time
                    }
                )
                self._raise_alert(alert)
    
    def _check_performance_thresholds(self, result: GuardTestResult):
        """Check if test performance exceeds thresholds."""
        if result.execution_time > self.performance_baselines["max_execution_time"]:
            alert = GuardAlert(
                alert_id=f"performance_{result.test_name}_{int(time.time())}",
                level=AlertLevel.MEDIUM,
                category="performance",
                message=f"Guard test performance degradation: {result.test_name}",
                details={
                    "test_name": result.test_name,
                    "execution_time": result.execution_time,
                    "threshold": self.performance_baselines["max_execution_time"],
                    "slowdown_factor": result.execution_time / self.performance_baselines["max_execution_time"]
                }
            )
            self._raise_alert(alert)
    
    def _check_sustained_failures(self):
        """Check for sustained failure patterns."""
        if len(self.test_results) < 10:
            return
        
        recent_results = [r for r in self.test_results 
                         if r.timestamp > datetime.now() - timedelta(hours=24)]
        
        if not recent_results:
            return
        
        failure_rate = len([r for r in recent_results 
                           if r.status == GuardTestStatus.FAILED]) / len(recent_results)
        
        if failure_rate > self.performance_baselines["max_failure_rate"]:
            alert = GuardAlert(
                alert_id=f"sustained_failures_{int(time.time())}",
                level=AlertLevel.CRITICAL,
                category="reliability",
                message="Sustained high failure rate in guard tests",
                details={
                    "failure_rate": failure_rate,
                    "threshold": self.performance_baselines["max_failure_rate"],
                    "total_tests": len(recent_results),
                    "failed_tests": len([r for r in recent_results 
                                       if r.status == GuardTestStatus.FAILED])
                }
            )
            self._raise_alert(alert)
    
    def _check_performance_trends(self):
        """Check for performance degradation trends."""
        if len(self.test_results) < 20:
            return
        
        # Group results by test name
        test_groups = {}
        for result in self.test_results[-100:]:  # Last 100 results
            if result.test_name not in test_groups:
                test_groups[result.test_name] = []
            test_groups[result.test_name].append(result)
        
        for test_name, results in test_groups.items():
            if len(results) < 10:
                continue
            
            # Check for increasing execution times
            recent_results = sorted(results, key=lambda x: x.timestamp)[-10:]
            older_results = sorted(results, key=lambda x: x.timestamp)[-20:-10]
            
            if not older_results:
                continue
            
            recent_avg = sum(r.execution_time for r in recent_results) / len(recent_results)
            older_avg = sum(r.execution_time for r in older_results) / len(older_results)
            
            if recent_avg > older_avg * 1.5:  # 50% increase
                alert = GuardAlert(
                    alert_id=f"performance_trend_{test_name}_{int(time.time())}",
                    level=AlertLevel.MEDIUM,
                    category="performance_trend",
                    message=f"Performance degradation trend detected: {test_name}",
                    details={
                        "test_name": test_name,
                        "recent_avg_time": recent_avg,
                        "older_avg_time": older_avg,
                        "degradation_factor": recent_avg / older_avg
                    }
                )
                self._raise_alert(alert)
    
    def _check_coverage_regressions(self):
        """Check for test coverage regressions."""
        recent_results = [r for r in self.test_results 
                         if r.timestamp > datetime.now() - timedelta(hours=24)
                         and r.coverage_data]
        
        if not recent_results:
            return
        
        # Calculate current coverage
        current_coverage = self._calculate_average_coverage(recent_results)
        
        # Compare with baseline
        for metric, value in current_coverage.items():
            baseline = self.baseline_metrics.get(f"coverage_{metric}")
            if baseline and value < baseline * 0.95:  # 5% regression threshold
                alert = GuardAlert(
                    alert_id=f"coverage_regression_{metric}_{int(time.time())}",
                    level=AlertLevel.HIGH,
                    category="coverage",
                    message=f"Test coverage regression detected: {metric}",
                    details={
                        "metric": metric,
                        "current_coverage": value,
                        "baseline_coverage": baseline,
                        "regression_percentage": ((baseline - value) / baseline) * 100
                    }
                )
                self._raise_alert(alert)
    
    def _count_recent_failures(self, test_name: str, time_window: timedelta) -> int:
        """Count recent failures for a specific test."""
        cutoff_time = datetime.now() - time_window
        return len([r for r in self.test_results 
                   if r.test_name == test_name 
                   and r.timestamp > cutoff_time 
                   and r.status == GuardTestStatus.FAILED])
    
    def _calculate_average_coverage(self, results: List[GuardTestResult]) -> Dict[str, float]:
        """Calculate average coverage from test results."""
        coverage_sums = {}
        coverage_counts = {}
        
        for result in results:
            for metric, value in result.coverage_data.items():
                if isinstance(value, (int, float)):
                    coverage_sums[metric] = coverage_sums.get(metric, 0) + value
                    coverage_counts[metric] = coverage_counts.get(metric, 0) + 1
        
        return {metric: coverage_sums[metric] / coverage_counts[metric] 
                for metric in coverage_sums 
                if coverage_counts[metric] > 0}
    
    def _raise_alert(self, alert: GuardAlert):
        """Raise an alert and notify callbacks."""
        # Check for duplicate alerts
        recent_alerts = [a for a in self.alerts 
                        if a.timestamp > datetime.now() - timedelta(minutes=15)
                        and a.category == alert.category
                        and a.details.get("test_name") == alert.details.get("test_name")]
        
        if recent_alerts:
            monitoring_logger.debug(f"Suppressing duplicate alert: {alert.message}")
            return
        
        self.alerts.append(alert)
        monitoring_logger.warning(f"ALERT [{alert.level.value.upper()}] {alert.message}")
        
        # Notify callbacks
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                monitoring_logger.error(f"Error in alert callback: {e}")
    
    def _cleanup_old_alerts(self):
        """Clean up old alerts."""
        cutoff_time = datetime.now() - timedelta(hours=24)
        self.alerts = [a for a in self.alerts if a.timestamp > cutoff_time]
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """Get current monitoring status."""
        recent_results = [r for r in self.test_results 
                         if r.timestamp > datetime.now() - timedelta(hours=24)]
        
        active_alerts = [a for a in self.alerts if not a.acknowledged]
        
        status_counts = {}
        for status in GuardTestStatus:
            status_counts[status.value] = len([r for r in recent_results 
                                             if r.status == status])
        
        return {
            "monitoring_active": self.monitoring_active,
            "total_tests_24h": len(recent_results),
            "status_distribution": status_counts,
            "active_alerts": len(active_alerts),
            "alert_by_level": {
                level.value: len([a for a in active_alerts if a.level == level])
                for level in AlertLevel
            },
            "average_execution_time": (
                sum(r.execution_time for r in recent_results) / len(recent_results)
                if recent_results else 0
            ),
            "timestamp": datetime.now().isoformat()
        }
    
    def acknowledge_alert(self, alert_id: str) -> bool:
        """Acknowledge an alert."""
        for alert in self.alerts:
            if alert.alert_id == alert_id:
                alert.acknowledged = True
                monitoring_logger.info(f"Acknowledged alert: {alert_id}")
                return True
        return False
    
    def export_metrics(self, output_path: Path):
        """Export monitoring metrics to file."""
        metrics = {
            "monitoring_status": self.get_monitoring_status(),
            "recent_results": [r.to_dict() for r in self.test_results[-100:]],
            "active_alerts": [a.to_dict() for a in self.alerts if not a.acknowledged],
            "baseline_metrics": self.baseline_metrics,
            "export_timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        monitoring_logger.info(f"Exported monitoring metrics to {output_path}")


# Pytest integration hooks
class GuardTestPlugin:
    """Pytest plugin for guard test monitoring."""
    
    def __init__(self):
        self.monitor = GuardTestMonitor()
        self.monitor.start_monitoring()
    
    def pytest_runtest_setup(self, item):
        """Called before each test runs."""
        test_name = f"{item.module.__name__}::{item.name}"
        monitoring_logger.debug(f"Starting test: {test_name}")
    
    def pytest_runtest_call(self, item):
        """Called during test execution."""
        pass
    
    def pytest_runtest_teardown(self, item):
        """Called after each test completes."""
        test_name = f"{item.module.__name__}::{item.name}"
        
        # Get test result from pytest's internal state
        result = GuardTestResult(
            test_name=test_name,
            status=GuardTestStatus.PASSED,  # Default, overridden in hook
            execution_time=0.0,  # Would be measured in real implementation
            assertions_count=0,   # Would be counted in real implementation
        )
        
        self.monitor.record_test_result(result)


class TestContinuousGuardMonitoring:
    """Tests for continuous guard monitoring system."""
    
    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        return GuardTestMonitor(alert_threshold=2, history_size=100)
    
    def test_monitor_initialization(self, monitor):
        """Test monitor initializes correctly."""
        assert monitor.alert_threshold == 2
        assert monitor.history_size == 100
        assert len(monitor.test_results) == 0
        assert len(monitor.alerts) == 0
        assert not monitor.monitoring_active
    
    def test_test_result_recording(self, monitor):
        """Test recording of test results."""
        result = GuardTestResult(
            test_name="test_example",
            status=GuardTestStatus.PASSED,
            execution_time=1.5
        )
        
        monitor.record_test_result(result)
        
        assert len(monitor.test_results) == 1
        assert monitor.test_results[0].test_name == "test_example"
        assert monitor.test_results[0].status == GuardTestStatus.PASSED
    
    def test_regression_detection(self, monitor):
        """Test regression detection triggers alerts."""
        # Record multiple failures for same test
        for i in range(3):
            result = GuardTestResult(
                test_name="test_failing",
                status=GuardTestStatus.FAILED,
                execution_time=1.0,
                error_message=f"Error {i}"
            )
            monitor.record_test_result(result)
        
        # Should trigger regression alert
        regression_alerts = [a for a in monitor.alerts if a.category == "regression"]
        assert len(regression_alerts) >= 1
        assert regression_alerts[0].level == AlertLevel.HIGH
    
    def test_performance_threshold_detection(self, monitor):
        """Test performance threshold detection."""
        # Record slow test execution
        result = GuardTestResult(
            test_name="test_slow",
            status=GuardTestStatus.PASSED,
            execution_time=35.0  # Exceeds 30s threshold
        )
        
        monitor.record_test_result(result)
        
        # Should trigger performance alert
        perf_alerts = [a for a in monitor.alerts if a.category == "performance"]
        assert len(perf_alerts) >= 1
        assert perf_alerts[0].level == AlertLevel.MEDIUM
    
    def test_alert_callback_system(self, monitor):
        """Test alert callback notification system."""
        alert_received = []
        
        def test_callback(alert):
            alert_received.append(alert)
        
        monitor.register_alert_callback(test_callback)
        
        # Trigger an alert
        result = GuardTestResult(
            test_name="test_callback",
            status=GuardTestStatus.FAILED,
            execution_time=1.0
        )
        
        for _ in range(3):  # Trigger threshold
            monitor.record_test_result(result)
        
        # Callback should have been called
        assert len(alert_received) >= 1
        assert alert_received[0].category == "regression"
    
    def test_monitoring_status_report(self, monitor):
        """Test monitoring status report generation."""
        # Add some test results
        for i in range(5):
            status = GuardTestStatus.PASSED if i < 4 else GuardTestStatus.FAILED
            result = GuardTestResult(
                test_name=f"test_{i}",
                status=status,
                execution_time=1.0 + i * 0.5
            )
            monitor.record_test_result(result)
        
        status = monitor.get_monitoring_status()
        
        assert "monitoring_active" in status
        assert "total_tests_24h" in status
        assert "status_distribution" in status
        assert "active_alerts" in status
        assert status["total_tests_24h"] == 5
        assert status["status_distribution"]["passed"] == 4
        assert status["status_distribution"]["failed"] == 1
    
    def test_alert_acknowledgment(self, monitor):
        """Test alert acknowledgment system."""
        # Create an alert
        alert = GuardAlert(
            alert_id="test_alert_123",
            level=AlertLevel.HIGH,
            category="test",
            message="Test alert",
            details={}
        )
        monitor.alerts.append(alert)
        
        # Acknowledge the alert
        result = monitor.acknowledge_alert("test_alert_123")
        assert result is True
        assert alert.acknowledged is True
        
        # Try to acknowledge non-existent alert
        result = monitor.acknowledge_alert("nonexistent")
        assert result is False
    
    def test_metrics_export(self, monitor):
        """Test metrics export functionality."""
        import tempfile
        
        # Add some data
        result = GuardTestResult(
            test_name="test_export",
            status=GuardTestStatus.PASSED,
            execution_time=2.0
        )
        monitor.record_test_result(result)
        
        # Export metrics
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:
            temp_path = Path(f.name)
        
        try:
            monitor.export_metrics(temp_path)
            
            # Verify export
            assert temp_path.exists()
            with open(temp_path, 'r') as f:
                metrics = json.load(f)
            
            assert "monitoring_status" in metrics
            assert "recent_results" in metrics
            assert "export_timestamp" in metrics
            assert len(metrics["recent_results"]) == 1
            
        finally:
            if temp_path.exists():
                temp_path.unlink()


def sample_alert_handler(alert: GuardAlert):
    """Sample alert handler for demonstration."""
    print(f"GUARD ALERT: [{alert.level.value}] {alert.message}")
    
    if alert.level in [AlertLevel.HIGH, AlertLevel.CRITICAL]:
        # In real implementation, could send notifications via:
        # - Slack/Discord webhooks
        # - Email notifications  
        # - PagerDuty/OpsGenie
        # - Custom monitoring systems
        pass


# Global monitor instance
_global_monitor = None

def get_global_monitor() -> GuardTestMonitor:
    """Get or create global monitor instance."""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = GuardTestMonitor()
        _global_monitor.register_alert_callback(sample_alert_handler)
        _global_monitor.start_monitoring()
    return _global_monitor