# Memory SFT configuration for retrieval tasks
vocab_size: 32000
d_model: 512
n_layers: 12
n_head: 8
max_seq_len: 2048
rope_base: 10000

# Memory parameters
mem_dim: 256
mem_tokens: 8
mem_slots: 1024

# Titans parameters (more aggressive for SFT)
alpha: 6.0      # Higher surprise gating
beta: 0.95      # Higher momentum
eta: 2e-2       # Higher learning rate
eta_decay: 5e-5 # Lower forgetting rate

# Training
dropout: 0.1
tie_embeddings: true

# Training hyperparameters
learning_rate: 1e-4  # Lower for SFT
weight_decay: 0.1
warmup_steps: 500
max_steps: 10000
batch_size: 16
gradient_accumulation_steps: 2

# Scheduler
scheduler_type: "linear"
min_lr_ratio: 0.0

# Logging
log_interval: 25
checkpoint_interval: 1000
